<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>服务器开荒日记（CUDA、cuDNN、nvidia-fabricmaneger安装)—–解决nvcc、驱动正常，但GPU无法正常使用问题</title>
      <link href="/2025/02/19/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BC%80%E8%8D%92%E6%97%A5%E8%AE%B0%EF%BC%88CUDA%E3%80%81cuDNN%E3%80%81nvidia-fabricmaneger%E5%AE%89%E8%A3%85)%E2%80%94%E2%80%93%E8%A7%A3%E5%86%B3nvcc%E3%80%81%E9%A9%B1%E5%8A%A8%E6%AD%A3%E5%B8%B8%EF%BC%8C%E4%BD%86GPU%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98/"/>
      <url>/2025/02/19/%E6%9C%8D%E5%8A%A1%E5%99%A8%E5%BC%80%E8%8D%92%E6%97%A5%E8%AE%B0%EF%BC%88CUDA%E3%80%81cuDNN%E3%80%81nvidia-fabricmaneger%E5%AE%89%E8%A3%85)%E2%80%94%E2%80%93%E8%A7%A3%E5%86%B3nvcc%E3%80%81%E9%A9%B1%E5%8A%A8%E6%AD%A3%E5%B8%B8%EF%BC%8C%E4%BD%86GPU%E6%97%A0%E6%B3%95%E6%AD%A3%E5%B8%B8%E4%BD%BF%E7%94%A8%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h2 id="1-安装显卡驱动"><a href="#1-安装显卡驱动" class="headerlink" title="1 安装显卡驱动"></a>1 安装显卡驱动</h2><h3 id="1-1-禁用nouveau"><a href="#1-1-禁用nouveau" class="headerlink" title="1.1 禁用nouveau"></a>1.1 禁用nouveau</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/modprobe.d/blacklist-nouveau.conf</span><br></pre></td></tr></table></figure><p>在文件中插入以下内容，将nouveau加入黑名单，默认不开启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure><p>输入以下命令使禁用生效然后重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo update-initramfs -u</span><br><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p>重启后验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure><p>如果回车后无反应，则禁用成功</p><h3 id="1-2-安装显卡驱动"><a href="#1-2-安装显卡驱动" class="headerlink" title="1.2 安装显卡驱动"></a>1.2 安装显卡驱动</h3><p>查询电脑最适合的显卡驱动版本</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ubuntu-drivers devices</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/image-20250219190939154.png" alt="image-20250219190939154"></p><p>安装推荐的显卡驱动，后面标recommended</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install nvidia-driver-570 <span class="comment">#此处数字要对应上面查询到的版本号</span></span><br></pre></td></tr></table></figure><p>安装完成后重启</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo reboot</span><br></pre></td></tr></table></figure><p>重启后在终端验证</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvidia-smi</span><br></pre></td></tr></table></figure><p>若出现GPU列表，则安装成功</p><h2 id="2-安装CUDA"><a href="#2-安装CUDA" class="headerlink" title="2 安装CUDA"></a>2 安装CUDA</h2><p>下载<a href="[CUDA Toolkit 12.4 Update 1 Downloads | NVIDIA 开发者](https://developer.nvidia.cn/cuda-downloads">CUDA</a>)，并按照官方提示安装。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/12.4.1/local_installers/cuda_12.4.1_550.54.15_linux.run</span><br><span class="line">sudo sh cuda_12.4.1_550.54.15_linux.run</span><br></pre></td></tr></table></figure><p>等待一会，会出现安装界面，取消第一个Driver选项里的X，[X]改为[ ]，选择Install。</p><p>安装完后需要配置环境变量，将以下内容添加到<code>~/.bashrc</code>文件中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo vim ~/.bashrc</span><br></pre></td></tr></table></figure><p>在文件末尾添加如下两行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=/usr/<span class="built_in">local</span>/cuda/lib</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:/usr/<span class="built_in">local</span>/cuda/bin</span><br></pre></td></tr></table></figure><p>然后刷新<code>~/.bashrc</code>配置文件，使得配置生效。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure><p>测试、查询nvcc版本检查是否安装成功</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc -V</span><br></pre></td></tr></table></figure><h2 id="3-安装cuDNN"><a href="#3-安装cuDNN" class="headerlink" title="3 安装cuDNN"></a>3 安装cuDNN</h2><p><a href="https://developer.nvidia.com/cudnn-downloads">cuDNN下载链接</a>, 下载对应版本的cuDNN</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.1-1_all.deb</span><br><span class="line">sudo dpkg -i cuda-keyring_1.1-1_all.deb</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get -y install cudnn</span><br></pre></td></tr></table></figure><h2 id="4-安装Fabric-manager（天坑）"><a href="#4-安装Fabric-manager（天坑）" class="headerlink" title="4 安装Fabric-manager（天坑）"></a>4 安装Fabric-manager（天坑）</h2><p>之前装好了显卡驱动、CUDA、cuDNN后，使用Pytorch测试一直显示GPU不可用，报错代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unexpected error from cudaGetDeviceCount(). Did you run some cuda <span class="built_in">functions</span> before calling NumCudaDevices() that might have already <span class="built_in">set</span> an error? Error 802: system not yet initialized</span><br></pre></td></tr></table></figure><p>只能尝试重装驱动和CUDA等，试了无数个版本都未果。丢给DeepSeek、文心一言、豆包等都是得到教科书式的标准答案：”建议检查驱动兼容性”、”尝试清理残留文件”，最后天无绝人之路，尝试丢给ChatGPT，答案输出的同时，瞬间看出具体问题所在，激动的心，颤抖的手。。。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/image-20250219193707140.png" alt="image-20250219193707140"></p><p>在使用NVIDIA显卡(V100/A100/A30等)时，需要安装对应的驱动，还要安装与驱动版本对应的 nvidia-fabricmanager 服务，使GPU卡间能够通过NVSwitch互联。</p><p><strong>安装nvidia-fabricmanager</strong>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">version=570.xxx.xx  <span class="comment">#已经安装的驱动版本</span></span><br><span class="line">main_version=$(<span class="built_in">echo</span> <span class="variable">$version</span> | awk -F <span class="string">&#x27;.&#x27;</span> <span class="string">&#x27;&#123;print $1&#125;&#x27;</span>)</span><br><span class="line">apt-get update</span><br><span class="line">apt-get -y install nvidia-fabricmanager-<span class="variable">$&#123;main_version&#125;</span>=<span class="variable">$&#123;version&#125;</span>-*</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>启动服务</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl start nvidia-fabricmanager</span><br></pre></td></tr></table></figure><p><strong>查看状态</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl status nvidia-fabricmanager</span><br></pre></td></tr></table></figure><p><strong>设置开机自启动</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl <span class="built_in">enable</span> nvidia-fabricmanager</span><br></pre></td></tr></table></figure><h2 id="5-使用Pytorch验证"><a href="#5-使用Pytorch验证" class="headerlink" title="5 使用Pytorch验证"></a>5 使用Pytorch验证</h2><p>使用miniconda安装好对应版本的Pytorch，运行下面的程序进行验证。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查是否有可用的GPU</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    device_count = torch.cuda.device_count()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;当前系统中可用的GPU数量为：<span class="subst">&#123;device_count&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;未检测到可用的GPU&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用GPU进行计算任务</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    y = y.cuda()</span><br><span class="line">    z = x + y</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;在GPU上进行计算：<span class="subst">&#123;z&#125;</span>&quot;</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;无法使用GPU进行计算&quot;</span>)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> -[运维] -[人工智能] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[服务器部署] -[CUDA] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BRIDGE阅读笔记</title>
      <link href="/2022/05/06/BRIDGE%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/05/06/BRIDGE%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>论文地址：Findings of EMNLP 2020 <a href="https://arxiv.org/abs/2012.12627">https://arxiv.org/abs/2012.12627</a><br>代码：<a href="https://github.com/salesforce/TabularSemanticParsing">BRIDGE</a></p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>早期的text2sql任务是在处理单表的问题，然而实际上的数据库是多表、多领域的，早期的方案不能很好的扩展。</p></blockquote><p>针对不同的数据库（DB），近似的自然语言表达生成的 SQL 可能十分不同。因此，跨数据库 text-to-SQL 语义解析器不能仅简单地记住所看到的 SQL 模式，而是必须准确地<strong>建模自然语言问题、目标数据库结构以及两者的上下文</strong>。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061532680.bmp" alt="202256105655"></p><p>如图中的例子，两个问题问的内容类似，但是生成的sql语句却很不一样。</p><p>最先进的跨数据库 text-to-SQL 语义解析器采用以下三个设计原则：</p><ol><li>question和schema 的表示是互相关联的；</li><li>BERT等预训练模型可以通过增强对自然语言变化的泛化和捕捉长期依赖关系，显著提高解析准确性；</li><li>在数据隐私允许的范围内，利用DB content来帮助理解schema，如上图的第二个例子中，“PLVDB”是name字段的值，但是name这个字段在问题中却没有提到，我们需要设计一个方法来解决这之前的隐藏的依赖关系</li></ol><p>本文提出的BRIDGE，它整合了上述三个设计原则。</p><ol><li>用 tagged sequence 标记序列 来表示关系型数据库的schema，并把这个序列连接在question的后面。不像之前的工作，提出具体的layers来对db schema联合text-db linking建模，BRIDGE只是对标记序列（不包括question）用bert来表示embedding，然后是两个单层的BILSTM来表示schema。每个模式组件（表或字段）仅<strong>使用混合序列中其特殊标记对应的隐藏状态来表示</strong>。</li><li>为了更好的对齐schema 和question，作者提出了<strong>anchor text</strong>，这个anchor text是 question中的词和DB单元值做相似度计算（编辑距离），大于某个阈值就认为是相同的词提取出来的，隐式地实现 text-DB 对齐。</li></ol><p>通过结合 pointer-generator 解码器和 schema-consistency driven search space pruning（模式一致性驱动的搜索空间修剪），BRIDGE实现了SOTA的表现。<br>通过深层次的模型比较和错误分析，证明了本文提出的模型对于记住结构化的模型、变化的查询语言泛化能力是<strong>有效的</strong>，但在组合概括方面存在困难，并且缺乏可解释性。<br>跨领域的文本到sql仍然提出了许多未解决的挑战，需要模型在训练数据经常稀少的情况下，对自然语言变化和结构组合进行泛化。</p><h1 id="2-Model"><a href="#2-Model" class="headerlink" title="2 Model"></a>2 Model</h1><p>BRIDGE模型——结合了<code>基于BERT的编码器</code>和<code>顺序 pointer-generator</code>，以执行端到端 cross-DB text-to-SQL 语义解析。</p><h2 id="2-1-Problem-Definition"><a href="#2-1-Problem-Definition" class="headerlink" title="2.1 Problem Definition"></a>2.1 Problem Definition</h2><p>给定自然语言问题 Q 和关系型数据库的模式（schema）$\mathcal{S}=\langle\mathcal{T}, C\rangle$，解析器需要生成相应的 SQL 查询 $Y$。一个</p><p>数据库中可能包含很多张<strong>表</strong>（tables），一张表又包含多个<strong>字段</strong>（fields），所以 $\mathcal{T}=\left\{t_{1}, \ldots, t_{N}\right\}$ ， $C=\left\{c_{11}, \ldots, c_{1\left|T_{1}\right|}, \ldots, c_{n 1}, \ldots, c_{N\left|T_{N}\right|}\right\}$ 。每张表的表名和字段名都是文本字符。表中的字段可能有主键、外键，同时字段有不同的<strong>数据类型</strong>。</p><p>最新方法表明访问数据库内容可以显着提高系统性能。但为保护隐私，模型仅可以访问每个字段的<strong>值集</strong>（value set），而不是整个数据库的内容。 把这些 value sets 叫做 picklists。</p><h2 id="2-2-Question-Schema-Serialization-and-Encoding"><a href="#2-2-Question-Schema-Serialization-and-Encoding" class="headerlink" title="2.2 Question-Schema Serialization and Encoding"></a>2.2 Question-Schema Serialization and Encoding</h2><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061533235.bmp" alt="202256112023"></p><p>如图，将Q和S拼接为一个混合的问题-模式序列，作为编码器的输入：</p><script type="math/tex; mode=display"> X=[CLS],Q,[SEP],[T],t1,[C],c11…,c1∣T1∣[T],t2,[C],c21,…,[C],cN|TN|,[SEP]X=[CLS],Q,[SEP],[T],t1,[C],c11…,c1∣T1∣[T],t2,[C],c21,…,[C],cN|TN|,[SEP]</script><p> 每个表名前面都有特殊标记[T]，每个字段名前面都有[C]。</p><ul><li>$X$首先输入BERT，随后经过一层Bi-LSTM获得序列的初始编码表示 $h_X$ 。</li><li>$h_X$ 中的问题片段继续通过一层Bi-LSTM获得Q的最终编码 $h_Q$ </li><li>每个表/字段使用对应于其特殊标记 [T] / [C] 的 $h_X$ 切片表示。</li></ul><p><strong>Meta-data Features</strong>:相比于表名，字段名多了<strong>主键</strong>、<strong>外键</strong>等属性。为了利用这些特征（meta-data），论文中用了一层前馈网络（ $g\left(\mathbb{R}^{4 n} \rightarrow \mathbb{R}^{n}\right)$) 对表名、字段名进一步编码。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061533577.bmp" alt="20225611287"></p><ul><li>表名：没有额外特征，后三个维度均用零向量替代。</li><li>字段名：这里的 $f_{pri},f_{for},f_{type}$ 分别表示各个字段的<strong>主键、外键、类型特征</strong>， $\boldsymbol{h}_{\mathrm{X}}^{q}$ 表示字段特征。将4个向量<strong>横向</strong>顺序拼接。</li><li>各个表名、字段名都进行 g 函数转化，纵向拼接得到模式（schema）的最终编码 。<h2 id="2-3-Bridging"><a href="#2-3-Bridging" class="headerlink" title="2.3 Bridging"></a>2.3 Bridging</h2>仅对表名/字段名及其关系进行建模并不足以捕捉模式（schema）的语义及其与Question的依赖关系，即缺少 Q 和 S 的交互。</li><li><p>解决方法：使用锚文本（<strong>anchor text</strong>）将问题 Q 中提及的值（value mentions）与数据库字段（DB fields）链接起来。<strong>锚文本为BERT提供了其他词汇线索，来标识Q中的相应提及（mentions）。</strong></p></li><li><p>具体实现：将问题 Q 中的每一个 token，与数据库表中每一列的所有 value 值进行<strong>字符串模糊匹配</strong>，匹配上的 value 值将被插入到序列 X 中（在相应的字段名称之后，并由特殊标记[V]分隔）。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061533899.bmp" alt="202256113426"></p></li><li><p>例如：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061534116.bmp" alt="202256113620"></p><p>如图，问题 Q 和表格“Properties”、“Reference Property Types”相关联。其中 Q 包含的两个单词“houses”和“apartments”与两张表中的同名字段“Property type code”有重合单元值。字段名“Property type code”本身没有在问题Q中出现，若让模型直接推理出“houses”、“apartments”和“Property type code”相关，难度很大。</p><h2 id="2-4-Decoder"><a href="#2-4-Decoder" class="headerlink" title="2.4 Decoder"></a>2.4 Decoder</h2><p>解码器的目的是从编码特征中还原出相应SQL。</p></li></ul><p>相比于前人的工作（RAT-SQL、IRNet等），BRIDGE解码器设计非常简洁，仅<strong>使用了一层带多头注意力机制的 LSTM pointer-generator 网络</strong>。</p><p>解码器使用由 70 个 SQL 关键字和保留token以及 10 位数字组成的<strong>生成词汇表</strong>来生成问题中未明确提及的数字（例如“第一”、“第二”、“最年轻”等）。</p><p>在每一个step中，解码器从如下动作中选择1种：</p><ol><li>从词汇表 V 中选择一个token（SQL关键字）</li><li>从问题 Q 中复制一个token</li><li>从模式 S 中复制一个组件（字段名、表名、单元值）</li></ol><p>从数学定义上分析，在每一个时刻$t$，给定解码状态 $s_t$ 和编码表示<br>$\left[\boldsymbol{h}_{Q} ; \boldsymbol{h}_{S}\right]∈ \mathbb{R}^{(|Q|+|S|) \times n}$ ，按照以下公式计算多头注意力：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061534347.bmp" alt="202256115544"></p><p>$\alpha_{t j}^{(h)}$ 表示从 Q 或 S 中复制相应 token 加入当前解码结果的<strong>权重</strong>。<br>解码由 $V$ 产生（即上述解码器动作1）的概率和总的输出分布为：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061535517.bmp" alt="202256115841"></p><p>其中， $P_{\mathcal{V}}\left(y_{t}\right)$ 是 softmax LSTM 的输出分布， $\tilde{X}_{j}$ 是长度为 ($|Q|+|\mathcal{S}|$) 的序列，其只包含 $X$ 中的 question words 和特殊标记[T]、[C]。</p><h2 id="2-5-Schema-Consistency-Guided-Decoding"><a href="#2-5-Schema-Consistency-Guided-Decoding" class="headerlink" title="2.5 Schema-Consistency Guided Decoding"></a>2.5 Schema-Consistency Guided Decoding</h2><p>基于每个 SQL 子句中出现的 DB 字段必须仅来自 FROM 子句中的表，提出了一种简单的<strong>序列解码器剪枝策略</strong>。</p><p><strong>按执行顺序生成SQL子句（Generating SQL Clauses in Execution Order）</strong>：将训练集中每个 SQL 查询的子句重新排列为表 1 所示的标准 DB 执行顺序。例如，<code>`SELECT COUNT(*) FROM Properties</code> 转换为 <code>FROM Properties SELECT COUNT(*)</code>。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205061541729.png" alt="image-20220506154110685"></p><p>在执行顺序中包含子句的所有SQL查询都满足以下引理:</p><blockquote><p>引理1：假设$Y_{exec}$是一个SQL查询，其中的子句按照执行顺序排列，那么$Y_{exec}$中的任何表字段都必须出现在表之后。</p></blockquote><p>采用二元注意力掩码：$\tilde{\alpha}_{t}^{(H)}=\alpha_{t}^{(H)}\cdot \xi$ ，$\xi$ 初始值设为维度和字段数目相同的0向量，一旦表$t_i$被解码，$\xi$ 中对应于 ${c_{i1},\dots,c_{i|T_i|}}$ 的元素置为1。这允许解码器仅在由引理1中的条件指定的空间中搜索，而解码速度的开销很小。</p><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3 实验"></a>3 实验</h1><h2 id="3-1-评估指标"><a href="#3-1-评估指标" class="headerlink" title="3.1 评估指标"></a>3.1 评估指标</h2><p><strong>精确集合匹配（Exact Set Match (E-SM) ）</strong>：通过检查预测查询中每个SQL子句的<strong>无序</strong>集合匹配情况来评估预测SQL的结构正确性。它忽略了预测值中的误差。</p><p><strong>执行准确度（Execution Accuracy (EA) ）</strong>：检查预测的SQL是否可在目标数据库上执行，以及执行结果是否与实际结果相匹配。这是一个性能上限，因为具有不同语义的两个SQL查询可以在一个数据库上执行相同的结果。</p><h2 id="3-2-Implementation-Details"><a href="#3-2-Implementation-Details" class="headerlink" title="3.2 Implementation Details"></a>3.2 Implementation Details</h2><h3 id="锚文本选择（Anchor-Text-Selection）"><a href="#锚文本选择（Anchor-Text-Selection）" class="headerlink" title="锚文本选择（Anchor Text Selection）"></a>锚文本选择（Anchor Text Selection）</h3><p>给定一个数据库DB，使用官方数据库文件计算每个字段的值集pickist。设计了一种模糊匹配算法来将问题与数据库中可能提及的值进行匹配。</p><p>将问题和字段值转换为小写字符序列，并使用启发式确定的匹配边界计算最长的子序列匹配。</p><blockquote><p>例如，问句“how many students keep cats as pets?”与单元值“cat”($s_c$​)匹配，匹配的子字符串为“cat”($s_m$​)。从question中$s_m$​的开始和结束字符索引 i、j 开始进一步搜索，以确保可以在 i-2 到 j + 2 内检测到单词边界，否则匹配无效。这不包括作为问题词子字符串的匹配项，例如“cat” 与“category”。将问题中匹配的全词短语表示为$s_q$，将问题匹配分数和单元格匹配分数定义为：</p><p>$\beta_{q}=|s_{m}|/|s_{q}|$，$\beta_{c}=|s_{c}|/|s_{q}|$。</p></blockquote><p>每个字段最多包含 k 个匹配项，并通过更长的匹配项来打破平局。我们排除所有数字匹配，因为问题中提到的数字通常不对应于 DB 单元（例如“低于 50  美元的鞋子”）或无法有效区分不同字段。</p><h1 id="4-Discussion"><a href="#4-Discussion" class="headerlink" title="4 Discussion"></a>4 Discussion</h1><h2 id="锚文本选择（Anchor-Selection）"><a href="#锚文本选择（Anchor-Selection）" class="headerlink" title="锚文本选择（Anchor Selection）"></a>锚文本选择（Anchor Selection）</h2><p>BRIDGE采用简单的字符串匹配来选择锚文本，提高锚文本选择的准确性可以显著提高端到端准确性。</p><p>将锚文本匹配扩展到简单字符串匹配之外的情况(例如” LA “→” Los Angeles “)是未来的方向。</p><p>目前BRIDGE忽略提到的数字。可以引入一些特征，表明问题中属于特定列的值范围内的特定数字。</p><h2 id="输入大小"><a href="#输入大小" class="headerlink" title="输入大小"></a>输入大小</h2><p>由于BRIDGE使用特殊的标记将所有输入序列化为一个序列，对于大型关系数据库来说，输入太长了。可以通过transformers最新改进框架解决，这些改进已经扩大了注意力机制来建模非常长的序列。</p><h2 id="关系编码"><a href="#关系编码" class="headerlink" title="关系编码"></a>关系编码</h2><p>BRIDGE将DB schema元数据特性融合到每个表字段表示中。这种机制不如直接建模原始图结构强大，规范化特定的注意头来捕获DB连接是一种很有前途的方法，可以在BRIDGE框架内对关系数据库的图结构建模，而无需引入(大量)额外参数。</p><h1 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h1><p>提出了 BRIDGE，这是一种强大的顺序架构，用于在跨数据库语义解析中对自然语言问题和关系数据库之间的依赖关系进行建模。</p><ol><li>BRIDGE 将问题和 DB 模式序列化为标记序列，并最大限度地<strong>利用预训练的 LM</strong>（例如 BERT）来捕获提及的文本和 DB 模式组件之间的链接；</li><li>使用<strong>锚文本</strong>来进一步改善两个跨数据库输入之间的<strong>对齐</strong>；</li><li>结合简单的<strong>顺序指针生成器解码器</strong>和<strong>模式一致性驱动</strong>的<strong>搜索空间修剪</strong>。</li></ol><p>后续计划：</p><ol><li>进一步改进模型的组合泛化（compositional generalization）和可解释性。</li><li>研究BRIDGE的扩展应用，这些任务需要结合文本和表格的理解，如弱监督的语义分析和事实检查。</li></ol>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>S2SQL阅读笔记</title>
      <link href="/2022/05/05/S2SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/05/05/S2SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>Text-to-SQL任务是Semantic Parsing任务中的一个重要分支。目前最先进的基于图编码器的模型已经被很好的应用于该任务，但是它们并没有对问题的句法进行很好的建模。本文提出了S$^2$SQL，将<strong>语法注入</strong>到Text-to-SQL解析器的Question-Schema图编码器中，有效地利用了Text-to-SQL解析器中问题的语法依赖信息来提高性能。还利用<strong>解耦约束</strong>引入不同的关系边缘嵌入，进一步提高了网络的性能。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h1><p>现阶段，Spider数据集上最有效和最流行的编码器架构是question-schema interaction graph。在此基础上，许多最先进的模型得到了进一步发展，它联合建模自然语言问题和结构化数据库schema信息，并使用一些预定义的关系来划分它们之间的交互关系。然而，我们观察到当前基于图的模型还存在两个主要限制。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051606159.png" alt="image-20220505160601059"></p><p><strong>句法建模（Syntactic Modelling）</strong>：句法和语义联合建模是自然语言处理的核心问题。在深度学习范式中，对于以句法为中心特征的任务，如Text-to-SQL任务，应该更好地理解句法的作用。例如，图1显示了基线模型可以学习question和schema之间的 date，id 和 transcript之间的正确链接，但是没有识别出id也应该包含在SELECT子句中。另一方面，在依存句法树的帮助下，date和id彼此接近，因此应该同时出现在SELECT子句中。然而，几乎所有可用的方法都将语言问题视为一个序列，而在基于神经网络的Text-to-SQL模型中忽略了句法信息。</p><p><strong>纠缠的边嵌入（Entangled Edge Embedding）</strong>：Question-Schema interaction graph预先定义了一系列的边，并将它们建模为可学习的嵌入。这些嵌入本质上应该是不同的，因为它们中的每一个都代表不同类型的关系，并且具有不同的含义。之前的研究已经证明了可学习嵌入算法容易被纠缠，不能满足多样性目标。</p><p>本文提出了S$^2$SQL，将语法注入到Text-to-SQL解析器的Question-Schema图编码器中，S$^2$SQL将来自句法依存树的句法标签建模为额外的边嵌入。在本文中，我们研究和证明了适当地将句法信息引入到Text-to-SQL中可以进一步提高性能，并且我们提供了为什么以及如何工作的详细分析。在此基础上，我们提出了一个解耦约束来鼓励模型学习不同的关系嵌入集，从而进一步提高了网络的性能。我们在具有挑战性的Text-to-SQL基准数据集Spider和Spider-Syn上评估了我们提出的模型，并证明当使用不同的预模型进行增强时，S$^2$SQL的性能一致优于其他基于图的模型。简言之，我们工作的贡献有三方面：</p><ul><li>研究了句法在Text-to-SQL任务中的重要性，并提出了一种新颖的、强大的跨领域的Text-to-SQL编码器，即S$^2$SQL。</li><li>为了包含不同的边嵌入学习，引入解耦约束，进一步提高了算法的性能。</li><li>实验结果表明，在具有挑战性的Spider和Spider-Syn数据上，我们的方法优于现有的所有模型。</li></ul><h1 id="2-Question-Schema-Interaction-Graph"><a href="#2-Question-Schema-Interaction-Graph" class="headerlink" title="2 Question-Schema Interaction Graph"></a>2 Question-Schema Interaction Graph</h1><p>输入的Question和Schema可以被一同视为一个图$G={V,E}$，其中$V=Q\cup T\cup C$，最初的节点嵌入矩阵$\mathbf{X} \in \mathbb{R}^{|V| \mathcal{Q}|+| \mathcal{T}|+| \mathcal{C}|| \times d}$的初始值（即每个节点的初始embedding）是将输入展平成如下序列形式：形式 $[\mathrm{CLS}] q_{1} q_{2} \cdots q_{|Q|}[\mathrm{SEP}] t_{10} t_{1} c_{10}^{t_{1}} c_{1}^{t_{1}} c_{20}^{t_{1}} c_{2}^{t_{1}} \cdots t_{20} t_{2} c_{10}^{t_{2}} c_{1}^{t_{2}} c_{20}^{t_{2}} c_{2}^{t_{2}} \cdots[\mathrm{SEP}]$. （之后输入到BERT或其他预训练模型中得到）。其中，$t_{i0}$和$c_{j0}^{t_i}$表示这个table或column的type信息，被放在每个table或column输入项之前。他们之间的连边 $\mathcal{R}=\{R\}_{i=1, j=1}^{|X|,|X|}$表示所有输入节点中任意两个节点之间已知的关系。</p><p><strong>RGAT</strong>（relational graph attention transformers）模型对整个图进行建模，并得到输出表示为：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051626504.png" alt="image-20220505162615459"></p><p>其中，$W_q,W_k,W_v$是可以通过训练学习。$ N_i^n$ 代表节点 $v_i^n$ 的receptive field。</p><h2 id="2-1-注入语法（Injecting-Syntax）"><a href="#2-1-注入语法（Injecting-Syntax）" class="headerlink" title="2.1 注入语法（Injecting Syntax）"></a>2.1 注入语法（Injecting Syntax）</h2><p>以前的工作主要集中在编码器中使用schema与question之间以及schema内部的连接，而忽略了问题的结构。我们提出了一种将句法依赖信息集成到图中的有效方法。一个简单的想法是将所有句法依存类型直接视为新边类型。然而，依存句法解析器将返回55种不同的依存句法类型。如此大量的边类型会显著增加S$^2$SQL中关系嵌入参数量，导致过拟合。为解决这个问题，我们将依赖类型引入三个抽象关系:<strong>Forward</strong>、<strong>Backward</strong>和<strong>NONE</strong>。另外，为了保证边嵌入的简单性，我们只考虑<strong>一阶关系</strong>。通过多层transformer的叠加，该模型无需刻意构造就能隐式捕获多阶关系。具体来说，我们计算在问题中的任意两个token $v_i$ 和 $v_j$ 之间的距离 $D(v_i,v_j)$ 。如果 $v_i$ 和 $v_j$ 具有上述依存关系类型，则该距离设置为 $v_i$ 和 $v_j$ 之间的一阶距离，否则为0。基于这个一阶距离D，我们通过前面定义的三种抽象类型之一，对token $v_i$ 和 $v_j$ 之间的句法关系 $R_{ij}$ 问题进行建模。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051633032.png" alt="image-20220505163302991"></p><p>总的来说，如图2所示，S$^2$SQL在图G中建模了三种结构：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051634370.png" alt="image-20220505163403300"></p><ul><li>问题结构 $R^{question}$：表示两个问句token之间的语法依存关系；</li><li>连接结构 $R^{linking}$ ：将相关实体与相应的schema中的列或表对齐的关系；</li><li>schema结构 $R^{schema}$ ：schema内部的关系，如主键-外键关系。</li></ul><p>如果该对满足下表中列出的描述之一并带有相应的标签，则从源节点 x ∈ S 到目标节点 y ∈ S 存在结构（边）。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051636675.png" alt="image-20220505163651611"></p><h2 id="2-2-解耦约束（Decoupling-Constraint）"><a href="#2-2-解耦约束（Decoupling-Constraint）" class="headerlink" title="2.2 解耦约束（Decoupling Constraint）"></a>2.2 解耦约束（Decoupling Constraint）</h2><p>R中有k条已知边，每条边都表示为一个关系嵌入。直观地说，这些边缘嵌入 $r = [r_1;r_2;:::;R_k]$ 应该是不同的，因为它们有不同的语义含义。为了避免优化过程中存在耦合嵌入边r的潜在风险，我们引入了正交性条件：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051640551.png" alt="image-20220505164025510"></p><p>这里1代表元素全部为1的矩阵，I代表单位矩阵。F即矩阵的Frobenius 范数。（注：这个操作可以直接在Pytorch中通过<code>torch.norm(matrix)</code>实现。）</p><h1 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3 Experiments"></a>3 Experiments</h1><h2 id="3-1-数据集和评价指标"><a href="#3-1-数据集和评价指标" class="headerlink" title="3.1 数据集和评价指标"></a>3.1 数据集和评价指标</h2><p>本文在Spider和Spider-Syn数据集上进行实验。Spider是一个大规模的、复杂的、跨域的Text-to-SQL的基准测试。Spider-Syn源自Spider，通过手动选择反映现实世界问题意译的同义词来替换与模式相关的单词。对于评估，我们遵循官方评估报告准确的匹配准确性（Exact Match，即EM）。</p><h2 id="3-2-Implementation-Details"><a href="#3-2-Implementation-Details" class="headerlink" title="3.2 Implementation Details"></a>3.2 Implementation Details</h2><p>利用PyTorch来实现模型。在预处理过程中，问题、列名和表名的输入将使用Standford Nature Language Processing工具箱进行标记化和语义化。为了与基线进行比较，我们使用相同的一组超参数来配置它，例如，堆叠8个自我注意层，将dropout设置为0.1。位置前馈网络的内层尺寸为1024。在解码器内部，我们使用大小为128的规则嵌入，大小为64的节点类型嵌入，以及在LSTM内部的隐藏层size为512,dropout为0.21。</p><h2 id="3-3-Baseline模型"><a href="#3-3-Baseline模型" class="headerlink" title="3.3 Baseline模型"></a>3.3 Baseline模型</h2><p>我们在Spider和Spider-Syn上进行了实验，并将我们的方法与几个基线进行了比较。</p><ul><li>RYANSQL：一种基于sketch的slot filling方法，它被提出来为其对应的位置合成每个SELECT语句。</li><li>RATSQL：是一个关系感知的模式编码模型，其中的question-schema interaction graph由n-gram模式构建的。</li><li>ShadowGNN：使用与领域无关的表示在抽象和语义级别处理模式。</li><li>BRIDGE：表示一个token序列中的问题和模式，其中将某个问题中提到的value所属的column跟在后面。</li><li>LGESQL：使用Line graph增强的Text-to-SQL模型，无需构造meta path就可以挖掘底层的关系特性。Spider排行榜之前的SOTA。</li></ul><h2 id="3-4-结果与分析"><a href="#3-4-结果与分析" class="headerlink" title="3.4 结果与分析"></a>3.4 结果与分析</h2><p><strong>整体性能表现</strong></p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051645314.png" alt="image-20220505164552260"></p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051646953.png" alt="image-20220505164625895"></p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051647985.png" alt="image-20220505164723935"></p><p>我们首先将S$^2$SQL与Spider上的其他最先进的模型进行比较。如表1所示，我们可以看到S$^2$SQL优于所有现有模型。值得注意的是，S$^2$SQL + RoBERTa在隐藏测试集中的准确率为67.1%，比强基线RAT + RoBERTa高2.8%。同样的，SOTA模型LGESQL + ELECTRA在隐藏测试集上的准确率为72.0%，在开发集上的准确率为75.1%，而S$^2$SQL + ELECTRA可以达到72.1%的测试精度和76.4%的开发精度。表2显示了RAT和S$^2$SQL开发集上基于Table的前训练模型的结果。我们可以看到，当使用不同的训练前模型（包括RoBERTa、GraPPa和GAP）增强时，S$^2$SQL的表现始终优于RAT。此外，如表3所示，S$^2$SQL展示了Spider-Syn数据集上的提升。</p><p><strong>消融实验</strong></p><p>表1的最后一行显示，移除解耦约束会导致开发集性能下降0.5%。这说明解耦关系嵌入有助于提高性能。为了检验解耦约束的影响，我们将任意两个关系嵌入之间的余弦相似度可视化。如图3所示，我们观察到解耦约束消除了纠缠现象(颜色较深)，并产生了更多样化的嵌入集。</p><h2 id="3-5-定性分析"><a href="#3-5-定性分析" class="headerlink" title="3.5 定性分析"></a>3.5 定性分析</h2><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202205051651565.png" alt="image-20220505165104494"></p><p>在表4中，我们将S$^2$SQL模型生成的SQL查询与基线模型LGESQL创建的SQL查询进行了比较。我们注意到，S$^2$SQL比基准系统执行得更好，特别是在问题理解依赖于语法结构的情况下。例如，在第一种情况下，order 和name有NMOD关系，baseline模型预测出错，在第一个示例中，name和 tonnage可以链接正确，但baseline未能捕获name和order的结构，导致生成错误,而S$^2$SQL预测结果正确。</p><h2 id="3-6-关于Syntactic-Parser"><a href="#3-6-关于Syntactic-Parser" class="headerlink" title="3.6 关于Syntactic Parser"></a>3.6 关于Syntactic Parser</h2><p>在我们的实验中，我们使用SpaCy工具作为句法分析器。需要强调的是，SpaCy句法分析的质量对S2SQL的性能影响很小。给出了以下三个主要原因。</p><ul><li>SpaCy是当前的SOTA解析器工具(OntoNotes 5.0语料库上的准确率为95%以上)，在各种介绍语法的论文中得到了广泛的应用，这证明了它的可靠性。</li><li>Spider中的问题并不复杂，可以很好地处理。</li><li>尽管语法分析器错误可能会给S$^2$SQL带来干扰，但我们提出的归纳句法注入方法(而不是独立句法注入)可以减轻句法类型错误的影响。</li></ul><h1 id="4-Conclusion"><a href="#4-Conclusion" class="headerlink" title="4 Conclusion"></a>4 Conclusion</h1><p>本文提出了一种语法增强的问题模式图编码器(S$^2$SQL)，它可以有效地对文本到sql的语法信息进行建模，并引入解耦约束来引入不同的关系嵌入。该模型在广泛使用的基准测试——Spider和Spider syn上取得了最新的性能。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/abs/2203.06958"> S$^2$SQL: Injecting Syntax to Question-Schema Interaction Graph Encoder for Text-to-SQL Parsers</a></p>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LGESQL阅读笔记</title>
      <link href="/2022/04/15/LGESQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/04/15/LGESQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><strong>论文地址</strong>：ACL2021 <a href="https://arxiv.org/abs/2106.01093v3">LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations</a></p><p><strong>代码地址</strong>：<a href="https://github.com/rhythmcao/text2sql-lgesql">LGESQL</a></p><h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>这项工作旨在解决 <strong>Text-to-SQL</strong> 任务中具有挑战性的<strong>异构图</strong>（由不同成分组成的图）编码问题。以前的方法通常是以节点为中心，仅仅利用不同的权重矩阵来对边类型进行参数化，这些方法</p><ul><li>忽略了嵌入在边的拓扑结构中的丰富语义信息</li><li>无法区分每个节点的局部和非局部关系</li></ul><p>为此，我们提出了一个 <strong>Line Graph Enhanced Text-SQL（LGESQL）</strong> 模型来挖掘底层的关系特征，而无需构建元路径。由于<strong>line graph</strong>的存在，信息不仅通过节点之间的连接，而且通过有向边的拓扑结构更有效地传播。此外，在图的迭代过程中，局部和非局部的关系都被不同程度地整合。我们还设计了一个辅助任务，叫做<strong>图修剪（graph pruning）</strong>，以提高<strong>Encoder</strong>的辨别能力。在撰写本报告时，我们的框架在跨领域文本到SQL基准的Spider上取得了 <strong>state-of-the-art</strong>（GLOVE为62.8%，ELECTRA为72.0%）。</p><h1 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1. Introduction"></a>1. Introduction</h1><p><strong>Text-to-SQL</strong>任务旨在给出相应的数据库模式，将自然语言问题转换为SQL查询。它已经在学术界和工业界被广泛研究，以建立数据库的自然语言接口。</p><p>一个艰巨的问题是如何联合编码<strong>问题</strong>和<strong>数据库模式</strong>（包括表和列），以及这些异质输入之间的<strong>各种关系</strong>。通常情况下，以前的文献利用以节点为中心的图形神经网络（GNN）来汇总相邻节点的信息，GNNSQL采用了关系图卷积网络（RGCN）来考虑模式项之间的不同边类型。</p><p>然而，这些边特征是直接从一个固定大小的参数矩阵中获取的，可能会有一个缺点：不了解上下文信息，特别是边的结构拓扑。<strong>元路径（Meta-path ）</strong> 被定义为连接两个对象的组合关系，它可以用来捕捉多跳语义，举例：在图1(a)中，Q-exact match-C 和 C-belong to-T 可以形成一个2跳元路径，表示某个表T 有一个列C正好在问题Q中被提到。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204161630789.png" alt="image-20220416162953634"></p><p>尽管<strong>RATSQL</strong>引入了一些有用的元路径，如 C-same table-C，但它处理所有的关系，无论是1跳还是多跳，都是以同样的方式（相对位置嵌入）表示在一个完整的图中。如果不区分<strong>局部</strong>（1跳邻居）和<strong>非局部</strong>邻居，见 图1(b)，每个节点将平等地关注所有其他节点，这可能导致过度平滑问题 (局部和非局部邻居应该被区别对待)。</p><p>此外，元路径目前是由领域专家构建的，或者通过广度优先搜索进行探索。不幸的是，可能的元路径的数量随着路径长度的增加而呈指数级增长，在其中选择最重要的子集是一个<strong>NP-complete问题</strong>。</p><p>为了解决上述限制，我们提出了一个 <strong>Line Graph Enhanced Text-SQL（LGESQL）</strong> 模型，它明确考虑了边的拓扑结构。根据 <strong>line graph</strong>（线型图） 的定义，我们首先从原来的<strong>以节点为中心</strong>的图中构建一个<strong>以边为中心</strong>的图，这两个图分别捕获节点和边的结构拓扑。</p><p>两个图中的每个节点都从其邻域收集信息，并从<strong>双图</strong>中提取边缘特征来更新其表示。对于以节点为中心的图，我们将局部和非局部的边特征都纳入计算。局部边特征表示 <strong>1跳</strong> 关系，由 <strong>line graph</strong> 中的 <strong>node embeddings</strong> 动态 提供，而非局部边特征则直接从参数矩阵中提取，这种区别鼓励模型要更多地关注局部的边特征。</p><h1 id="2-Preliminaries"><a href="#2-Preliminaries" class="headerlink" title="2. Preliminaries"></a>2. Preliminaries</h1><h2 id="2-1-Problem-definition"><a href="#2-1-Problem-definition" class="headerlink" title="2.1 Problem definition"></a>2.1 Problem definition</h2><p>整个输入节点为中心的异构图$G^n=(V^n,R^n)$由三种类型的节点组成，即$V^n=Q\cup T\cup U $</p><ul><li>其中，Q为问题，T为对应的数据库 <strong>schema</strong> 的Table，C 为 column，R 是边的类型</li><li>$|V^n|$是节点总数，$|V^n|=|Q|+|T|+|C|$</li></ul><h2 id="2-2-Meta-path"><a href="#2-2-Meta-path" class="headerlink" title="2.2 Meta-path"></a>2.2 Meta-path</h2><p>如图所示，一个 meta-path 代表一个路径</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204181532969.png" alt="Meta-path"></p><p>它在两个类型$\tau_1$和$\tau_2$ 的节点之间描述了一组复合关系 $r=r_1\circ r_2\cdots \circ r_l$ (即描述了两个节点之间存在的边的类型信息，边的类型包括 QUESTION，TABLE，COLUMN)。</p><p>在本文的讨论中，我们使用 local 来表示长度为 <strong>1</strong> 的路径（局部路径），而长度 <strong>大于1</strong> 的路径为非局部路径。关系邻接矩阵$R^{n}$中包含局部关系和非局部关系。</p><h2 id="2-3-Line-Graph"><a href="#2-3-Line-Graph" class="headerlink" title="2.3 Line Graph"></a>2.3 Line Graph</h2><p>离散数学里面的对偶图，把原图的边转换为节点，把节点转化为边，以边为中心的异构图。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192206102.png" alt="image-20220419220620033"></p><p>如图所示，原始图中有1-5共5个节点和 ${(1, 2), (1, 3), \cdots\}$ 等多条边。现在我们构造<strong>Line Graph</strong>。首先将图中所有的边全部变成<strong>Line Graph</strong>中的一个节点，然后如果两条边同时接到了一个点上，比如 ( 1 , 2 ) 和 ( 1 , 3 ) 那么这两条边对应到<strong>Line Graph</strong>中的节点就连一条边。</p><ul><li><p>在 <strong>line graph</strong>$G^e=(V^e,R^e)$中的每一个节点$v_i^e,i=1,2,\dots,|V^e|$可以唯一映射到原始节点中心图$G^n=(V^n,R^n)$的一条有向边$r^n_{st}$，或者$v_{s}^{n}\to v_{t}^{n}$</p></li><li><p><strong>Function</strong> f 将源节点和目标节点的索引元组（s,t）映射到 line graph$G^e$中点的索引（相当于把$G^n$中两个节点之间的边映射到$G^{e}$中的节点），表示为 i = f(s,t)，反向映射表示为$f^{-1}$（相当于把$G^{e}$中的节点映射到$G^{n}$中的边）</p></li><li><p>在 <strong>line graph</strong> $G^{e}$中，如果$r^{n}_{f^{-1}(i)}$的 <strong>target node（目标节点）</strong>$v_{i}^{e}$的和 $r^{n}_{f^{-1}(j)}$的 <strong>source node（起始节点）</strong>$v_{j}^{e}$在原图$G^{n}$中都表示相同的节点，那么在线图$G^{e}$ 中，$v_{i}^{e}$和$v_{j}^{e}$之间就会存在一条有向边$r^{e}_{ij}$。</p></li><li><p>$r^e_{ij}$中包含了 <strong>meta-path</strong> $r^{n}_{f^{-1}(i)} \circ r^{n}_{f^{-1}(j)}$的信息（包含了原图中两个节点的信息 ）</p></li></ul><p>我们防止回溯的情况，其中两个反向边不会在$G^e$中连接，如图所示</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204181557589.png" alt="image-20220418155708541"></p><p>只使用$R^{n}$的局部关系作为 <strong>line graph</strong> $G^{e}$的节点集，避免在 <strong>line graph</strong> 中创建太多节点。对称的是，在<strong>line graph</strong>中$R^{e}$中的每条边，可以被唯一映射到原图中的节点集 $V^{n}$中，如下图</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204191844962.png" alt="在这里插入图片描述"></p><h1 id="3-Method"><a href="#3-Method" class="headerlink" title="3. Method"></a>3. Method</h1><p>在构建了线图之后，我们利用经典的 <strong>encoder-decoer</strong> 结构为骨干作为我们的模型，LGESQL由三部分组成：</p><ul><li>graph input model（图形输入模块）</li><li>line graph enhanced hidden module（线图增强隐藏层模块）</li><li>graph output module（图形输出模块）</li></ul><p>前两个模块旨在将输入的异构图$G^{n}$映射成 <strong>node embeddings X</strong>，<strong>X</strong> 的维度是$|V^{n}| * d$，其中d是图的 hidden size。</p><p><strong>图输出模块</strong>检索并将 X 转化为目标<strong>SQL查询y</strong>。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204191848087.png" alt="image-20220419184810002"></p><h2 id="3-1-Graph-Input-Module"><a href="#3-1-Graph-Input-Module" class="headerlink" title="3.1 Graph Input Module"></a>3.1 Graph Input Module</h2><p>这一部分的模型是为了获取节点和边的<strong>初始表示</strong></p><ul><li>对于边，初始的<strong>局部</strong>的边特征$\mathbb{Z}^{0}\in \mathbb{R}^{|V^{e}|\times d}$以及<strong>非局部</strong>特征$\mathbb{Z}_{nlc}\in \mathbb{R}^{(|R^{n}|-|V^{e}|)\times d} $都是直接从参数矩阵获得的。</li><li>对于节点，我们可以从<strong>GLOVE</strong>或者预训练的模型（PLM）如<strong>BERT</strong>中获取。</li></ul><h3 id="3-1-1-Glove"><a href="#3-1-1-Glove" class="headerlink" title="3.1.1 Glove"></a>3.1.1 Glove</h3><p>对于每个在<strong>question</strong>、<strong>table</strong>或<strong>column</strong>中的名称，使用Glove词向量时可以不考虑上下文直接在词典中拿到它们的embedding。然后将它们（这里的它们是指一个名称中包含的若干个word）按照所属部分（即是属于question，还是table，还是column）类型分别送入对应的BiLSTM中，并concatenate两端的hidden state。将得到的节点表示进行stack组成初始节点embedding矩阵$X^0\in \mathbb{R}^{|V^n|\times d}$。</p><h3 id="3-1-2-PLM"><a href="#3-1-2-PLM" class="headerlink" title="3.1.2 PLM"></a>3.1.2 PLM</h3><p>使用预训练模型时，我们首先将所有的question和schema中的名字组成一个序列</p><script type="math/tex; mode=display">[CLS]q_1​q_2​⋯q_{∣Q∣}​[SEP]t_{10}​t_1​c_{10}^{t_1}​​c_1^{t_1}​​c_{20}^{t_1}​​c_2^{t_1}​​⋯t_{20}​t_2​c_{10}^{t_2}​​c_1^{t_2}​​c_{20}^{t_2}​​c_2^{t_2}​​⋯[SEP]</script><p>其中类型信息$t_{i0}$和$c_{j0}$被插入到schema item之前。由于每个word都被分词成subword-level，所以要获取每个word的表示向量需要再加入一个subword attentive pooling layer来获取word的表示。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192050320.png" alt="image-20220419205050282"></p><p>其计算过程与attention的计算差别不大。获得每个word的词表示后，也分别将它们按照所属部分类型分别送入对应的BiLSTM中，并concatenate两端的hidden state。将得到的节点表示进行stack组成初始节点embedding矩阵$X^0\in \mathbb{R}^{|V^n|\times d}$。</p><h2 id="3-2-Line-Graph-Enhanced-Hidden-Module"><a href="#3-2-Line-Graph-Enhanced-Hidden-Module" class="headerlink" title="3.2 Line Graph Enhanced Hidden Module"></a>3.2 Line Graph Enhanced Hidden Module</h2><p>包含 L 个堆叠的 <strong>dual relational graph attention network</strong> <strong>（Dual RGAT）</strong> 层</p><ul><li>在每个层 $l$ 中含有两个 RGAT模块分别捕获 <strong>original graph</strong> 和 <strong>line graph</strong> 的结构信息。</li><li>每个图中的节点embedding都表示了对偶图中的边的特征信息。在这两个图中，一个图的 <strong>node embeddings</strong> 在另外的一个图中充当 <strong>edge features</strong> <strong>（边的特征）</strong>。例如， <strong>original graph</strong> $G^{n}$中的边的特征是由 <strong>line graph</strong> $G^{e}$中的 <strong>node embeddings</strong> 提供的。下图中节点图$G^{n}$使用的边特征 z （边1-4和2-4）来源于线图中对应的“节点”特征。</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192100710.png" alt=""></p><ul><li>我们使用$\mathbb{X}^{l}\in \mathbb{R}^{|V^{n}|\times d}$来代表<strong>original graph</strong> $G^{n}$ 在第 $l$ 层的RGAT的 <strong>node embedding</strong> 输入矩阵，对于 $v_{i}^{n}$ 节点，我们的 <strong>embedding</strong> 用 $x_{i}^{l}$ 表示。</li><li>相同地，使用 $\mathbb{Z}^{l}\in \mathbb{R}^{|V^{e}|\times d}$  以及 $z_{i}^{l}$ 来代表 <strong>line graph</strong> $G^{e}$ 的节点（这里指局部关系，即一跳邻居）</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192101671.png" alt="image-20220419210125632"></p><ul><li>其中，$\mathbb{Z}_{nlc}$ 代表 <strong>original graph</strong> $G^{n}$ 的非局部边特征。</li><li>用上面公式可以看出，计算 <strong>original graph</strong> $G^{n}$ 的表示 $\mathbb{X}^{l}\in \mathbb{R}^{|V^{n}|\times d}$ 时会考虑局部和非局部关系；而在计算 <strong>line graph</strong> $G^{e}$ 的表示 $\mathbb{Z}^{l}\in \mathbb{R}^{|V^{e}|\times d}$ 时只考虑局部关系。</li></ul><h3 id="3-2-1-RGAT-for-the-Original-Graph"><a href="#3-2-1-RGAT-for-the-Original-Graph" class="headerlink" title="3.2.1 RGAT for the Original Graph"></a>3.2.1 RGAT for the Original Graph</h3><p>这一部分的作用是在<strong>计算节点的表示时增加邻居节点以及对应边的信息</strong>。</p><p>给定 <strong>original graph</strong>  $G^{n}$ ，第 $l$ 层的输出 $x_{i}^{l+1}$ 的计算方式如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192106109.png" alt="image-20220419210619058"></p><ul><li>∣∣ 表示拼接操作，H是head 的数量，FFN()代表 <strong>feedforward neural network</strong>。</li><li>$N_i^n$ 代表节点 $v_{i}^{n}$ 的 <strong>receptive field</strong>（<strong>感受野，应该是指跟 $i$ 节点相连的节点</strong>）</li><li>$\psi(r^n_{ij})$会返回一个表示 $r_{ji}^{n}$ 的 $d$ 维向量（这个向量代表了 <strong>line graph</strong> $G^{e}$ 中的一个节点，因为 $G^{e}$ 中的节点是 $G^{n}$ 中的边）</li><li>$[.]_h^H$ 操作首先将向量平均分割为 Ｈ个部分并返回第ｈ个分区，由于<strong>Line Graph</strong>的构建只利用了1阶连接关系，对于节点图中的高阶邻域，论文采取<strong>静态动态混合特征</strong>或<strong>多头多视图拼接</strong>的方法，将更远的节点及其边类型引入到节点图中对节点特征的更新中，这些包含远距离信息的边特征不会迭代更新，仅仅使用静态参数初始化。</li></ul><p><strong>Mixed Static and Dynamic Embeddings</strong>　(混合静态和动态嵌入)</p><ul><li>如果 $r_{ji}^{n}$ 是一个局部关系（一跳邻居），$\psi (r_{ji}^{n})$ 会从 <strong>line graph</strong> $G^{e}$ 中返回对应节点的 <strong>embedding</strong>  $z_{f(j,i)}^{l}$。</li><li>否则，$\psi (r_{ji}^{n})$ 会返回非局部关系矩阵 $\mathbb{Z}_{nlc}$ 中对应的向量。</li><li>邻居函数 $N_{i}^{n}$ 会返回节点 $v_{i}^{n}$ 的所有邻居节点的集合 $V^{n}$ ，这个集合在所有的<strong>head</strong>都是共享的。</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192119355.png" alt="image-20220419211959304"></p><p><strong>Multi-head Multi-view Concatenation</strong> （多头多视图拼接）</p><ul><li>将<strong>多头注意力模块</strong>分成两部分。</li><li>一半的 <strong>head</strong> 中，邻居函数 $N_{i}^{n}$ 会返回节点 $v_{i}^{n}$ 的一跳邻居的集合。在这种方法中，$\psi (r_{ji}^{n})$ 会按照从第  $l$ 层获取的 $Z^{l}$ 中取得对应节点的特征 $z_{f(j,i)}^{l}$（在这一部分，每个节点只接受相邻的节点信息）</li><li>另外一半的 <strong>head</strong> 中，每个节点都可以接触到局部和非局部的邻居，$\psi (.)$会从嵌入矩阵 $\mathbb{Z}_{nlc}\cup \mathbb{Z}^{0}$ 中获取静态的特征embedding。</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192127421.png" alt=""></p><p><strong>RGAT模块</strong> 对 <strong>局部</strong> 和 <strong>非局部</strong> 关系的处理是不同的，相对来说，对局部边特征的处理更仔细。</p><h3 id="3-2-2-RGAT-for-the-Line-Graph"><a href="#3-2-2-RGAT-for-the-Line-Graph" class="headerlink" title="3.2.2 RGAT for the Line Graph"></a>3.2.2 RGAT for the Line Graph</h3><p>这一部分的作用是在<strong>计算边的表示时加入边的两端的节点信息以及相邻边的信息</strong>。</p><p>计算方式跟上面类似，这里不再细说</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192129000.png" alt="image-20220419212947947"></p><ul><li>$\psi (r_{ji}^{e})$ 返回 $G^{e}$ 中边 $r_{ji}^{e}$ 的特征向量，即返回 $G^{e}$ 中节点  $v^{e}_{i}$ 的对应于 $G^{n}$ 中的有向边的起始节点的 <strong>embeddings</strong>（举个例子，在下图中，$\phi(r_{e1 e2}^{e}$) 会返回 <strong>original graph</strong> 中两个圆圈的那个节点的 <strong>embeddings</strong> ）</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204191844962.png" alt=""></p><ul><li>因为我们只在 <strong>line graph</strong> 中考虑局部信息，所以 $N_{i}^{e}$ 只包含 1跳邻居。</li><li>还有另外的一点需要注意，在这一部分，边的特征 $r_{ji}^{e}$ 被放在了 <strong>attention</strong> 的 <strong>query</strong> 部分而不是 <strong>key</strong> 部分,因为计算attention分数  $\tilde{\beta}_{ji}^{h}$ 时，跟起始节点 $j$ 是无关的。</li></ul><blockquote><p>因为在<strong>attention</strong>中，算的是<strong>Q</strong>对<strong>K</strong>的相似度，如果放在key部分，则算的是起始节点 $j$ 对目标节点 $i$ 的相似度，但是在 line graph 中，如下图，(1-4,4-5)和(2-4,4-5)中间的边都表示原图中的4号节点，如果不换位置，则算的是1-4以及2-4对4-5的相似度，但是不管相似度哪个大，都是表示4号节点；再举个现实生活中的例子，两个男生追一个女生，哪个男生能追到女生并不是看哪个男生更喜欢女生，而是看女生更喜欢哪个男生。</p></blockquote><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192136211.png" alt="在这里插入图片描述"></p><h2 id="3-3-Graph-Output-Module"><a href="#3-3-Graph-Output-Module" class="headerlink" title="3.3 Graph Output Module"></a>3.3 Graph Output Module</h2><p>该模块包括两个任务:一个用于 text-to-SQL 的 <strong>decoder</strong>，另一个用于执行称为 <strong>graph pruning</strong>（图剪枝）的辅助任务。</p><p>我们使用下标表示具有特定类型的 <strong>node embeddings</strong> 集合，例如， $X_{q}$ 表示所有 <strong>question</strong> 的 <strong>node embeddings</strong> 的矩阵。</p><h3 id="3-3-1-Text-to-SQL-Decoder"><a href="#3-3-1-Text-to-SQL-Decoder" class="headerlink" title="3.3.1 Text-to-SQL Decoder"></a>3.3.1 Text-to-SQL Decoder</h3><p>Decoder部分采用的是与RATSQL一致的grammar-based syntactic neural decoder来首先生成AST，然后再由AST语法树生成SQL语句。</p><h3 id="3-3-2-Graph-Prunning"><a href="#3-3-2-Graph-Prunning" class="headerlink" title="3.3.2 Graph Prunning"></a>3.3.2 Graph Prunning</h3><p>一个强大的<strong>编码器</strong>应该将不相关的 <strong>schema items(模式项目)</strong> 与目标查询中使用的 <strong>golden schema items</strong>(正确的schema items) 区分开来(意思是把SQL查询中没有用到的items排除)。</p><p>在 图6 中，面向问题的模式子图（在阴影区域的上方）可以很容易地被提取出来。意图(要select的) c2 和约束(限制select的) c5 通常在问题中被明确提及，通过 <strong>attention机制</strong> 或 <strong>schema linking(模式链接)</strong> 确定。连接节点如t1、c3、c4、t2可以通过模式图的 1跳 连接来推断，形成一个连接的组件</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192142519.png" alt="image-20220419214240455"></p><p>为了引入这种归纳性偏见，我们设计了一个辅助任务，根据<strong>schema node</strong>与问题的相关性和模式图的稀疏结构来决定对<strong>schema node</strong>进行分类。</p><p>首先,我们用 <strong>question</strong> 的 <strong>node embeddings </strong> $X_{q}$ 以及 <strong>schema</strong> 的 <strong>node embeddings</strong> $s_{i}$ 来计算两者的 <strong>attention</strong> , 目的是用来获取 <strong>context vector</strong>  $\tilde{x}_{s_{i}}$。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192144322.png" alt="image-20220419214436270"></p><p>然后，使用 <strong>biaffine binary classifier(二元二进制分类器)</strong> 来确定 <strong>context vector</strong>  $\tilde{x}_{s_{i}}$ 和 <strong>schema</strong> 的 <strong>node embeddings</strong>  $s_{i}$ 是否是相互关联的。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192145648.png" alt="image-20220419214524612"></p><p>如果 <strong>schema</strong> 的 <strong>node embeddings</strong>  $s_{i}$ 出现在<strong>目标SQL</strong>语句中,<strong>schema item</strong> 的 <strong>label</strong>  $y_{s_{i}}^{g}$ 会是 1,使用交叉熵来计算 <strong>loss</strong>。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192146677.png" alt="image-20220419214651627"></p><p>这个任务和主任务一同进行<strong>多任务学习</strong>，对提升主任务效果比较大。</p><h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="主要结果"><a href="#主要结果" class="headerlink" title="主要结果"></a>主要结果</h2><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192148745.png" alt="image-20220419214826695"></p><p>实验结果见上图，可以看到无论是使用PLM还是Glove，模型都取得了领先于之前最好的模型的表现。在各个分组的性能如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192149191.png" alt="image-20220419214916140"></p><h2 id="消融实验"><a href="#消融实验" class="headerlink" title="消融实验"></a>消融实验</h2><h3 id="LGESQL不同的组件的实验表现"><a href="#LGESQL不同的组件的实验表现" class="headerlink" title="LGESQL不同的组件的实验表现"></a>LGESQL不同的组件的实验表现</h3><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192150541.png" alt="image-20220419215004491"></p><p>可以看到使用Line graph比不使用性能普遍高出1个百分点。而且<strong>Non-local relation</strong>是比较重要的。同时，多任务学习的<strong>Graph pruning</strong>模块也对最后结果有着不错的促进作用。</p><h3 id="预训练模型的影响"><a href="#预训练模型的影响" class="headerlink" title="预训练模型的影响"></a>预训练模型的影响</h3><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192151138.png" alt="image-20220419215116077"></p><p>实验显示使用ELECTRA比BERT的效果要好。</p><h3 id="案例研究"><a href="#案例研究" class="headerlink" title="案例研究"></a>案例研究</h3><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192151643.png" alt="image-20220419215154593"></p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>在本工作中，我们利用Line graph来更新异质图中的边特征，用于Text-to-SQL的任务。通过对Line graph中的结构连接的迭代，局部边可以结合多跳关系特征并捕获重要的meta-path。通过进一步集成非本地关系，编码器可以从多个视图学习，并使用快捷方式关注远程节点。在未来，我们将研究更多有用的meta-path，并探索更有效的方法来处理不同的基于meta-path的邻居。</p><p><strong>本文的贡献在于提出了利用对偶图来获取边的局部信息</strong></p><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="局部和非局部关系"><a href="#局部和非局部关系" class="headerlink" title="局部和非局部关系"></a>局部和非局部关系</h2><p>实验中用到的所有局部关系如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192154160.png" alt="image-20220419215417083"></p><p>其中作者保留了NO-MATCH关系。</p><h2 id="Text-to-SQL-Decoder的细节"><a href="#Text-to-SQL-Decoder的细节" class="headerlink" title="Text-to-SQL Decoder的细节"></a>Text-to-SQL Decoder的细节</h2><h3 id="ASDL-Grammar"><a href="#ASDL-Grammar" class="headerlink" title="ASDL Grammar"></a>ASDL Grammar</h3><p>所使用的ASDL语法如下</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192155294.png" alt="image-20220419215523213"></p><h3 id="Decoder架构"><a href="#Decoder架构" class="headerlink" title="Decoder架构"></a>Decoder架构</h3><p>Decoder中使用了一种新型的LSTM叫ON-LSTM，可以将结构信息考虑在内，详情参考<a href="https://www.linkresearcher.com/theses/54a0ba37-b625-4200-9065-5120b94933b1">ICLR 2019最佳论文 | ON-LSTM：用有序神经元表达层次结构</a>。</p><h3 id="Graph-pruning"><a href="#Graph-pruning" class="headerlink" title="Graph pruning"></a>Graph pruning</h3><p>这里作者做了一个实验，比较共享多少层给两个任务效果最好，结果发现直接多任务学习即共享所有层效果最佳。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204192157271.png" alt="image-20220419215709212"></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://arxiv.org/abs/2106.01093v3">LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations</a></li><li><a href="https://blog.csdn.net/weixin_43829169/article/details/121678305?ops_request_misc=%7B%22request%5Fid%22%3A%22165000849416780264019897%22%2C%22scm%22%3A%2220140713.130102334.pc%5Fall.%22%7D&amp;request_id=165000849416780264019897&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-2-121678305.142^v9^control,157^v4^control&amp;utm_term=+Line+Graph+Enhanced+Text-to-SQL+Model+with+Mixed+Local+and+Non-Local+Relations&amp;spm=1018.2226.3001.4187">LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations 论文笔记_weixin_43829169的博客-CSDN博客</a></li><li><a href="https://blog.csdn.net/u011426236/article/details/119392517">论文笔记：LGESQL: Line Graph Enhanced Text-to-SQL Model with Mixed Local and Non-Local Relations_11好好学习，天天向上的博客-CSDN博客</a></li></ol>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>RAT-SQL阅读笔记</title>
      <link href="/2022/04/12/RAT-SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/04/12/RAT-SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本篇论文研究的问题是：当把自然语言问题翻译成SQL查询，从数据库中搜集数据来回答问题时，现在语义解析模型很难泛化到未见过的数据库模式上。主要的泛化挑战有两个：1、语义parser对数据库关系进行可访问的编码，2、对数据库表列和它们在给定查询问题中的提及进行对齐建模。针对这两个挑战，作者提出了一个统一的框架，基于关系感知的self-attention机制，以解决模式编码、模式链接以及text2sql编码器中的特征表示。作者把这个框架应用到Spider数据集上，取得了57.2% exact match accuracy，在当时实现了SOTA。和BERT一起用，达到了65.6%。此外，最重要的是 模型在对模式链接和对齐的理解上有了质的提高。</p></blockquote><p>论文地址：ACL2020 <a href="https://arxiv.org/abs/1911.04942">RAT-SQL: Relation-Aware Schema Encoding and Linking for Text-to-SQL Parsers </a></p><p>代码地址：<a href="https://github.com/Microsoft/rat-sql">rat-sql</a></p><h1 id="1、现有的方案"><a href="#1、现有的方案" class="headerlink" title="1、现有的方案"></a>1、现有的方案</h1><p>现有的方案：由于text2sql是一个seq2seq的任务，utterance translation to sql query，所以研究的论文的点主要有：</p><ul><li>针对encoder编码器的优化</li><li>生成sql解码器的优化</li><li>查询语言和sql之间的差异，使用formal language（形式化查询语言），来更好的对齐查询语言和sql语句</li><li>Encoder embedding的预训练</li><li>Decoder的预训练</li></ul><p>本文的方案：这篇论文主要的工作是encoder部分优化。</p><h1 id="2、提出问题"><a href="#2、提出问题" class="headerlink" title="2、提出问题"></a>2、提出问题</h1><p>论文首先就<strong>Schema Encoding</strong>和<strong>Schema Linking</strong>进行了介绍。Schema Encoding顾名思义就是对表结构（表名、列名、列类型、主键、外键等等）进行编码，以便后续模型训练使用。Schema Linking则是要把Question中表述的内容与具体的表名和列名对齐。</p><p>由于spider数据集的train、test是没有重叠部分，所以是zero shot的预测，这种情况下，模型对schema的泛化能力就至关重要了。</p><p>这个泛化问题的挑战主要有三方面：</p><ul><li>任何Text-to-SQL解析模型都必须将模式编码为适合于解码可能涉及给定列或表的SQL查询的表示形式。</li><li>这些表示应该编码schema模式的所有信息，比如它的column types列类型、外键关系和用于数据库连接的主键。</li><li>该模型必须能识别用于指代列和表的查询语言，这可能与训练中看到的参考语言有所不同，也就是模型得有 schema linking 能力（schema linking： 可以把问题中提到的实体引用与预期的模式列或表很好的对齐）</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121027488.png" alt="image-20220412102717411"></p><p>图1解释了在linking时存在ambiguity 的情况，问题中的<em>“model”指的是</em> car_names.model 而不是 model_list.model；为了解决ambiguity 的情况，the semantic parser 需要同时考虑 schema relations 和语义信息。</p><p>之前的工作<a href="https://www.aclweb.org/anthology/P19-1448/">GNN encoder</a>使用图神经网络，主要是用有向图来表示外键之间的关系，但是这种方法有两个缺点：</p><ol><li>没有将schema与question结合起来编码，因此，在列和问题词被表示之后（这里的表示主要是embedding词向量和schema的一些内容），对schema linking模式链接的推理会变得很困难</li><li>将模式编码期间的信息传播限制在预定义的外键关系图中，这就没有考虑全局信息，之前的研究中已经证明全局信息对有效的表示结构关系非常重要。</li></ol><p>这篇论文提出的RAT-SQL方案，是对数据库schema 和 问题question的关系结构有效编码；它使用 relation-aware self-attention （关系感知的self-attention <a href="https://aclanthology.org/N18-2074/">Self-Attention with Relative Position Representations - ACL Anthology</a>），把 schema entities 模式实体和问题词的全局推理，与对预定义模式关系的结构化推理 结合了起来。通过实验证明，RAT-SQL能够建立更准确的问题与模式列和表的真实对齐的内部表示。</p><h1 id="3、Relation-Aware-Self-Attention"><a href="#3、Relation-Aware-Self-Attention" class="headerlink" title="3、Relation-Aware Self-Attention"></a>3、Relation-Aware Self-Attention</h1><p>Transformer结构是一系列self-attention layers的叠加。考虑 $x_i$为输入，self-attention layer的输出为 $y_i$。每一层的处理过程如下。其中FC为全连接层，LayerNorm是进行normalization。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121606192.png" alt="image-20220412160634148"></p><p>从Transformer架构理解，$e_{ij}$为$x_i$的query vector与$x_j$的key vector的点积得分；$\alpha_{ij}$为$x_i$对不同词向量的注意力权重；$z_i$为$x_i$的自注意力输出。普通Transformer结构擅长学习到不同$x_i$之间的关系，但如果$x_i$之间有预先定义好的关联关系，就无法表示出。</p><p>Shaw et al. (2018)提出了relation-aware self-attention的方法。即关系感知的自注意力，这是一个对半结构化输入序列embedding的模型，它联合编码了输入中预先存在的关系结构，以及同一嵌入中序列元素之间的诱导 “软 “关系。schema 的 embedding和 linking 自然而然作为一个feature。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121618939.png" alt="image-20220412161825903"></p><p>RAT-SQL基于此进行了进一步的引申，使得该Transformer结构可以表示任意的关系信息。$r_{ij}$表示两个输入元素$x_i$和$x_j$的已知关系编码，定义$r_{ij}^K=r_{ij}^V=Concat(\rho_{ij}^{(1)},…,\rho_{ij}^{(R)})$。<strong>其中假设有R个关系对，而$\rho_{ij}^{(s)}$为通过学习得到的对关系$R^{(s)}$的向量表示</strong>。部分关系如下表：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121633863.png" alt="image-20220412163331799"></p><h1 id="4、RAT-SQL"><a href="#4、RAT-SQL" class="headerlink" title="4、RAT-SQL"></a>4、RAT-SQL</h1><p>模型结构示意图如下，分为encoding和decoding两个阶段，下面分步解释。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121658306.jpg" alt="RAT-SQL"></p><h2 id="4-1-构造Graph-G-Q"><a href="#4-1-构造Graph-G-Q" class="headerlink" title="4.1 构造Graph $G_Q$"></a>4.1 构造Graph $G_Q$</h2><p>图顶点集合 $V_Q$ 由三部分组成：column names，table names以及question words。对于column，同时在顶点label中加入column type。</p><p>这里需要注意的是，有的列是主键，有的是外键，列的type有NUMBER和TEXT。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121701737.png" alt="image-20220412170155683"></p><p>本文使用有向图来表示数据库schema，node节点包括表和列，表的label是表名单词，列字段的label会在列单词前加type，边是数据库已经定义的关系。</p><blockquote><p>这里需要注意的：图对已有信息都覆盖了，但是对那些没见过的schema就不能精确编码了。所以，本文希望通过联合学习question和schema来让模型掌握这种能力，能够对已有信息未见过的schema精确编码。因此，作者定义了一个新的图：question-contextualized schema graph ：node包括question words， 边是已知schema 边 + question words 和 schema 的 特殊关系（完全匹配、部分匹配）</p></blockquote><p>图的边集合$E_Q$由三部分组成：首先是Database Schema所定义的表连接关系，table和column的包含关系等等；其次是通过schema linking得到的question和schema之间的对应关系；最后是为辅助relation-aware self-attention而定义的Auxiliary Relations。各种edges的类型整理如下图，具体定义请参考论文。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121704259.jpg" alt="v2-2ac086a4d53aa0b68cccdb50b10e04a1_r"></p><h2 id="4-2-Relation-Aware-Input-Encoding"><a href="#4-2-Relation-Aware-Input-Encoding" class="headerlink" title="4.2 Relation-Aware Input Encoding"></a>4.2 Relation-Aware Input Encoding</h2><p>针对每个表、列节点，作者这里使用了glove的embedding来获得词向量，并用BiLSTM来处理词向量，并用另外独立的BiLSTM来获取question的embedding</p><p>输入X是所有节点表示的集合：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121707311.png" alt="image-20220412170610001"></p><p>这里的encoder会堆叠多层的Relation-aware self-attention层，最后将第N层的输出作为编码器的最终输出。</p><p>同时，论文也使用预训练模型BERT来获取初始节点Embedding。同时也添加了许多附加关系，使得节点之间两两互有关系，构成一个<strong>完全图</strong>。</p><h2 id="4-3-Schema-Linking"><a href="#4-3-Schema-Linking" class="headerlink" title="4.3 Schema Linking"></a>4.3 Schema Linking</h2><blockquote><p>由于我们的输入是一张张图，图的边包括已存在主外键关系，这里我们需要构建问题中提到的词与数据库中已存在的表、列（字段）的边。这里的关系有两种：1、匹配的名称（ matching names ）；2、匹配的列的值（matching values）</p></blockquote><p><strong>Name-based linking</strong>：包含完全匹配和部分匹配；完全匹配就是字符串和数据库的表列名完全相同，部分匹配是字符串只是表列名的子集。这里使用N-Gram进行匹配，当然效果是很差的。</p><p><strong>Value-based Linking</strong>：主要是确定question中的某个词是否是数据库表中某列的一个值。这对于模型的表现提升很大。这块其实比较难，因为列的值千变万化，且有的需要背景知识。（这块作者是说，将这块外包给数据库引擎来做，不需要将模型暴露给数据，通过数据库索引和文本搜索快速检索单词匹配，这里具体是如何？作者并没有解释清楚。）</p><h2 id="4-4-Memory-Schema-Alignment-Matrix"><a href="#4-4-Memory-Schema-Alignment-Matrix" class="headerlink" title="4.4 Memory-Schema Alignment Matrix"></a>4.4 Memory-Schema Alignment Matrix</h2><p>一般出现在SQL句子中的表和列，一般在question中会提到相关的词语；为了验证，使用relation-aware 的attention来作为一个指针机制。使用Encoder最后一层的输出来计算一个alignment矩阵，用于可视化schema linking的对齐效果。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121730618.png" alt="image-20220412172954528"></p><p>公式中的对齐矩阵应该类似于真实的离散对齐，所以需要尊重某些约束条件，如稀疏性，不过随着编码器被充分参数化后，稀疏性往往随着学习而产生，我们也可以使用一个明确的目标来鼓励它。</p><blockquote><p>（作者在附录中证明了，这个方式只在这个对齐方式，在早期的实验中可以帮助模型，后期对模型的performance没有帮助；或者在transformer层较少的时候有用）</p></blockquote><h2 id="4-5-Decoder"><a href="#4-5-Decoder" class="headerlink" title="4.5 Decoder"></a>4.5 Decoder</h2><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121736504.png" alt="image-20220412173635449"></p><p>在encoder输出的向量之上通过使用LSTM输出一系列解码器动作来生成作为深度优先遍历顺序的抽象语法树AST（Abstract Syntax Tree）：</p><ol><li>将最后生成的节点扩展成一条语法规则，用APPLY RULE来生成基本结构；</li><li>当完成一个叶子结点时，从schema中选择一个column或table。用SELECT TABLE或者SELECT COLUMN来完成table name或column name的选择填充。</li><li>AST生成后，可进一步推断出最终的SQL query。</li></ol><blockquote><p>详情参考论文： Pengcheng Yin and Graham Neubig. 2017. A Syntactic Neural Model for General-Purpose Code Generation. In <em>Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pages 440–450.</p></blockquote><h1 id="5-实验"><a href="#5-实验" class="headerlink" title="5 实验"></a>5 实验</h1><p>作者对输入的文本做了lemmatized ，这个在做n-gram匹配时可以用得上；作者这里尝试了 4、6、8层的Rat-layers，本实验主要基于Spider、WikiSQl这两个数据集。</p><h2 id="5-1-在Spider数据集上的结果"><a href="#5-1-在Spider数据集上的结果" class="headerlink" title="5.1 在Spider数据集上的结果"></a>5.1 在Spider数据集上的结果</h2><p>在Spider数据集上实验结果如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121739800.png" alt="image-20220412173908737"></p><p>在各个难度子集上的结果如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121739777.png" alt="image-20220412173940728"></p><p>同时，文章进行了消融实验Ablation Study，结果如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121740102.png" alt="image-20220412174007047"></p><p><strong>结果显示使用了 schema-linking、graph-relation、value-linking会进一步提高表现。</strong></p><h2 id="5-2-在WikiSQL数据集上的结果"><a href="#5-2-在WikiSQL数据集上的结果" class="headerlink" title="5.2 在WikiSQL数据集上的结果"></a>5.2 在WikiSQL数据集上的结果</h2><p>作者也尝试在WikiSQL数据集上进行了评估。实验结果如下：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121742394.png" alt="image-20220412174224358"></p><p>可以看到，模型相比于WIkiSQL上一些单纯的”slot filling”的方法也取得了具有竞争性的表现。</p><h1 id="6-讨论"><a href="#6-讨论" class="headerlink" title="6 讨论"></a>6 讨论</h1><h2 id="6-1-Alignment的作用"><a href="#6-1-Alignment的作用" class="headerlink" title="6.1 Alignment的作用"></a>6.1 Alignment的作用</h2><p>将question中对应的columns和tables识别出来，不仅仅是提升了SQL中对应columns/tables的准确率。这种对齐（alignment）还为question中其他部分的识别提供了一个良好的基准。如下图中，不只是question中“horsepower”被识别出与“column:horsepower”对应，同时整个word span “the largest horsepower”也被对齐到“column:horsepower”。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121746537.png" alt="image-20220412174647463"></p><p>图中展示了模型对图1示例生成的对齐。可以看到查询问题和 表列的对齐效果；对于引用列的三个单词((“cylinders”, “model”, “horsepower”)，对齐矩阵正确地标识了它们对应的列，其他的词受到了这三个词很大的影响，导致稀疏的span-to-column对齐，例如“largest horsepower” 到 horsepower.。表cars_data和cars_names是由单词cars隐式提到的，所有被模型识别出来了，但cars_maker 却没有，说明模型在对齐上做的还算可以。</p><h2 id="6-2-使用Schema-Linking的必要性"><a href="#6-2-使用Schema-Linking的必要性" class="headerlink" title="6.2 使用Schema Linking的必要性"></a>6.2 使用Schema Linking的必要性</h2><p>为了探讨模型选择了错误的表列的频率，作者做了一个oracle experiment（让模型中特定模块永远产出正确的结果然后再评估模型准确率）：在每一个语法非最终的终端，解码器被强迫选择正确的产生方式使得和ground truth完全匹配，后续的解码流程基于这个选择进行。结果如下图</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202204121756986.png" alt="image-20220412175635934"></p><p>这里的Oracle columns指的是总是给模型提供正确的table/column，Oracle sketch指的是给出正确的AST结构。可以看到，如果都可以正确给出，模型可以达到99.4的准确率，证明了模型中的语法基本可以覆盖所有的Spider中的例子。</p><h2 id="6-3-误差分析"><a href="#6-3-误差分析" class="headerlink" title="6.3 误差分析"></a>6.3 误差分析</h2><p>主要有三种误差：</p><ol><li>18%的错误预测是因为SQL语法表达不同，而执行结果是正确的。（例如ORDER BY C LIMIT 1 vs.SELECT MIN(C)）</li><li>39%的错误预测是因为SELECT 子句中的错误/缺失/多余的column。这是schema linking的一个限制，部分错误较难解决，因为一些questions中并未明确所需column。</li><li>29%的错误是因为WHERE语句不完整。一个常见的例子是特定领域的措辞，例如“older than 21”，这需要背景知识将其映射到 age &gt; 21 而不是 age &lt; 21。此类错误在域内微调后消失。</li></ol><h1 id="创新点"><a href="#创新点" class="headerlink" title="创新点"></a>创新点</h1><p>RATSQL是ACL 2020上的一篇用于解决Spider数据集上Text-to-SQL问题的论文，后续的LGESQL也是在篇论文的基础上改进的。文章的主要创新点在于：</p><ol><li>在Encoder中加入relation-awareself-attention组成RAT层代替transformer层对输入进行encode，relation一共有33种，每两个单词之间都会有一个relation。利用schema linking同时把显式关系（schema）和隐式关系（question和schema之间的linking）都考虑在encoding中，完善了模型的表示能力。假设一共有R种不同的关系，rij可以定义为R个特征向量的拼接，对于每一种关系，如果xi和xj有这一关系，就将对应的特征向量作为参数进行学习，否则就置为零向量。</li><li>构造Graph，图顶点集合Vq由三部分组成：column names，table names以及question words。对于column，同时在顶点label中加入column type。边集合Eq 由三部分组成：首先是Database Schema所定义的表连接关系，table和column的包含关系等等；其次是通过schema linking得到的question和schema之间的对应关系；最后是为辅助relation-aware self-attention而定义的Auxiliary Relations。</li><li>在encoder输出的向量之上通过一系列的预测动作来构造AST（Abstract Syntax Tree）：首先，用APPLYRULE来生成基本结构；然后，用SELECTTABLE或者SELECTCOLUMN来完成table name或column name的选择填充。AST生成后，可进一步推断出最终的SQL query。</li><li>在encoder最后一层计算一个对齐矩阵，用于可视化查看模型在question word和schema word之间的对齐效果。</li></ol>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IRNet阅读笔记</title>
      <link href="/2022/03/29/IRNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/03/29/IRNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="1-提出问题"><a href="#1-提出问题" class="headerlink" title="1. 提出问题"></a>1. 提出问题</h1><p><strong>Mismatch problem</strong>： SQL本身的设计目的是为了准确地向数据库传达执行细节，因此会有与自然言语（后面缩写为NL）表达不一致的地方。比如下例中，FROM、GROUP BY和HAVING等语句，在NL中并没有直接表达出，需要根据问题与表结构才能推断得出。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/image-20220329215521974.png" alt="image-20220329215521974"></p><p><strong>Lexical problem</strong>：Spider数据集跨越不同领域，测试数据的schema中有大量（~35%）的词从未在训练集中出现，这要求模型有更好的泛化性。</p><h1 id="2-IRNet的思路"><a href="#2-IRNet的思路" class="headerlink" title="2. IRNet的思路"></a>2. IRNet的思路</h1><h2 id="2-1-引入中间表示层SemQL"><a href="#2-1-引入中间表示层SemQL" class="headerlink" title="2.1 引入中间表示层SemQL"></a>2.1 引入中间表示层SemQL</h2><p>为解决Mismatch的问题，设计了一种特定于领域的语言SemQL，作为NL和SQL之间的中间表示。让SemQL和NL之间有更好的映射关系，而SQL可以根据语法规则从SemQL推断生成。</p><p>SemQL将GROUPBY,HAVING and FROM clauses都去掉，WHERE和HAVING这些conditions都被统一到Filter关键字。这些操作在之后的推断阶段能够确定性地转换将其转换成SQL。先来举个例子说一下为什么是确定性的呢，例如GROUPBY从句后边跟着的column通常是SELECT从句后边的column 或者 该column是表的主键，在这个表中一个聚合函数被应用到这个表的一个column中。每个C都有一个T对应，这样能够有效的防止多个表中有相同的column name。<img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/无标题_看图王.png" alt="SemQL语法"></p><p>上图左边是<strong>SemQL的语法结构</strong>，Z是根节点，R表示一个SQL单句，如”Select Filter“表示当前的SQL单句里包含有SELECT和WHERE两个语句。Order和Superlative与SQL中的ORDER BY对应，其中Superlative可以映射到ORDER BY + LIMIT。比如成绩最高的5个人，最后在SQL中表达为ORDER BY grade desc LIMIT 5。</p><p>上图右边是从图1中NL对应的SemQL。</p><h2 id="2-2-模式链接Schema-Linking"><a href="#2-2-模式链接Schema-Linking" class="headerlink" title="2.2 模式链接Schema Linking"></a>2.2 模式链接Schema Linking</h2><p><strong>任务目标</strong>：识别问题中提到的columns 和 tables，并且根据问题中提及的方式给每个columns赋予不同的类型（Table、Column、Value）。</p><p><strong>识别的方式为字符串匹配</strong>：用n-grams枚举出所有可能的单词组合，其中n的取值为1~6。如果ngram可以完全匹配或者部分匹配上某个Column的名称，则识别为Column。用同样的方法识别Table。如果某个n-grams同时被识别为Column和Table，那么默认它是一个Column。识别完成后，Questions会被分成各个Span（分段），每个Span会进一步标识一个具体的类别，如Column、Table、None等。对于被识别为列的span，如果它们与模式中的列名完全匹配，为这些列分配 EXACT MATCH类型，否则分配 PARTIAL MATCH类型。</p><p><strong>Value的识别</strong>：论文中没有用到<strong>DB Content</strong>，而是使用<strong>ConceptNet</strong>。如果n-gram开始和结束都是引号，则默认为database中的value。把Value Span到ConceptNet中进行查询，找到对应的Concept再来跟Column Name对比，如果能匹配上，就能确定Value和Column之间的对应关系。比如，“Masters”是一种“Degree”，如果Column中有“Degree”，那么“Masters”就会被认为是Degree Column的一个枚举值。</p><p>这里留下一个改进的空间：使用DB Content可以提升Column和Value的识别能力。而在实际应用中，不少情况下DB Content是可以获取的，只是有一定的资源开销。</p><h2 id="2-3-模型"><a href="#2-3-模型" class="headerlink" title="2.3 模型"></a>2.3 模型</h2><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/20220329222525.png" alt="模式连接"></p><p><strong>输入</strong>：</p><ul><li>问题Natural Language（Figure4的最下面）</li><li>数据库模式A database schema （Figure4的最上面左边）</li><li>模式链接结果The schema linking results（Figure4的最上面右边）</li></ul><p>为了解决the lexical problem，将schema linking的结果应用到NL和Schema中。设计了a memory augmented pointer network去 select columns，这个network决定是从memory中选择column还是从schema of database中选择。</p><p><strong>NL Encoder</strong>：<strong>对Question进行编码。</strong>把Question按照Schema Linking分成不同的Spans；把每个Span中word的embedding及Span类型的embedding求平均后作为Span的embedding；在所有span的embedding之上，用一个Bi-LSTM进行编码后，将隐层作为输出。</p><p><strong>Schema Encoder：对Database Schema进行编码。</strong>（a）把Schema中Column words的embedding取平均作为初始表示；（b）在span embedding的基础上用attention得到context vector；（c）把column type也转化为一个embedding vecotr。以上（a）、（b）、（c）相加得到schema encoder。</p><script type="math/tex; mode=display">\begin{align*}g_{k}^{i}&=\frac{(\hat{e_{c}}^{i})^{T}e^{k}_x}{||\hat{e_{c}}^{i}||||e^{k}_x||} \\c_{c}^{i}&=\sum_{k=1}^{L}g_{k}^{i}e_{x}^{k} \\e_{c}^{i}&=\hat{e_{c}}^{i}+c_{c}^{i}+\varphi_{i}\end{align*}</script><p><strong>Decoder：生成SemQL。</strong> 此处IRNet定义了三种不同类型的操作——$APPLYRULE、SELECTCOLUMN、SELECTTABLE$。APPLYRULE选择特定的规则生成语法树，而SELECTCOLUMN和SELECTTABLE选择相应的column或table进行填充。</p><p>为了提升模型的准确率，IRNet使用了memory机制和coarse-to-fine的形式。（1）<strong>Memory：帮助细化column的选择机制。</strong>当一个column被选择后，将会把这个column从schema中移除，同时记录在memory中。下次当这个column再次出现，算法将决定是从scheam中进行选择，还是从memory中进行选择。（2）<strong>Coarse-to-fine：分两阶段由粗到细生成SemQL。</strong>第一阶段生成一个SemQL查询的skeleton，第二阶段通过选择column和table在skeleton中填入细节，执行过程如下图所示。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/20220330180519.png" alt="coarsetofine"></p><h1 id="3-实验"><a href="#3-实验" class="headerlink" title="3. 实验"></a>3. 实验</h1><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/20220330223506.png" alt="精确匹配结果"></p><p>IRNet的表现明显优于所有的基线。在测试集上，它比SyntaxSQLNet获得了27.0%的绝对性能提升。与执行大规模数据扩充的SyntaxSQLNet(augment)相比，它也获得了19.5%的绝对改善。当加入BERT时，SyntaxSQLNet和IRNet的性能都得到了很大的提高，它们在开发集和测试集上的精度差距也加大了。</p><p>论文最后进行了错误案例分析。在Dev Set上IRNet共有483个错误预测。<strong>主要分为三类：Column Prediction（占比32.3%），Nested Query（23.9%），Operator（12.4%）</strong>。</p><ul><li>Column Prediction错误：基于字段值（cell value）没能正确找到对应的column。</li><li>Nested Query错误：大部分是由于Extra Hard level的复杂嵌套查询没能准确构造。</li><li>Operator错误：部分operators的选择要求机器有一定常识，如“from old to young”需要把年龄降序排列。</li></ul><p>加入BERT后，Column Prediction和Operator两类错误有所改善。引入BERT进行输入编码的方法如下图所示。同样以Question、Database Schema、Schema Linking Results作为输入。对应每个span取words和type的平均值作为该span的encoding。对Schema，在column每个word上用一个Bi-LSTM输出隐层，加上column type的encoding作为column最后的encoding。</p><p>根据论文描述，一些可能的改进思路：</p><ol><li>改善Column Prediction：现有column的识别基于string-match，如果改为embedding-match应该可以改善column的识别准确率；利用DB Content提升从cell value识别column的能力。</li><li>改善Nested Query：Extra Hard level的训练样本有限，可考虑通过数据增强来提升模型效果。</li><li>对SemQL的改善：继续加强SemQL来改善NL和SQL之间的mismatch部分，解决self join等表达问题。</li></ol><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p>1.<a href="https://arxiv.org/abs/1905.08205"> Towards Complex Text-to-SQL in Cross-Domain Database with Intermediate Representation (arxiv.org)</a></p><p>2.<a href="https://github.com/zhanzecheng/IRNet">IRNet (github.com)</a></p>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SyntaxSQLNet阅读笔记</title>
      <link href="/2022/03/24/SyntaxSQLNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/03/24/SyntaxSQLNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="1-问题定义"><a href="#1-问题定义" class="headerlink" title="1 问题定义"></a>1 问题定义</h1><p>这项工作旨在解决复杂的文本到SQL的任务，涉及多个表、SQL子句和嵌套查询。此外，使用单独的数据库进行训练和测试，旨在开发推广到新数据库的模型。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202203242050617.png" alt="image-20220324205025518"></p><p>​                                    SyntaxSQLNet利用SQL的树结构进行解码</p><p><strong>Dataset</strong>：使用<a href="https://github.com/taoyds/spider">Spider</a>作为主要数据集，它包含10181个问题，5693个独特的复杂SQL查询，以及200个具有多个表的数据库。</p><p><strong>任务和挑战</strong>：</p><ul><li>该数据集包含大量复杂的SQL标签，与以前的数据集(如WikiSQL)相比，它们涉及更多的表、SQL子句和嵌套查询。为WikiSQL任务开发的现有模型无法处理Spider数据集中那些复杂的SQL查询。</li><li>数据集包含200个数据库(∼138个域)，不同的数据库用于训练和测试。与大多数先前的语义解析任务(例如，ATIS)不同，该任务需要模型来概括新的、看不见的数据库。</li></ul><p>在这个任务中，我们在来自不同数据库的不同复杂SQL查询上训练和测试模型。这旨在确保模型只有在真正理解给定数据库下问题的含义时，才能做出正确的预测，而不是仅仅通过记忆。</p><h1 id="2-SyntaxSQLNet的思路"><a href="#2-SyntaxSQLNet的思路" class="headerlink" title="2 SyntaxSQLNet的思路"></a>2 SyntaxSQLNet的思路</h1><p>将解码器构造为递归模块的集合。为了充分利用SQL查询的良好定义的结构，使用SQL特定的语法来指导解码过程。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202203241803929.png" alt="image-20220324180300821"></p><p>图2是在解码过程中使用的模块和 SQL 语法。圆形符号表示一个 SQL 关键字，如SELECT、WHERE、表列等。方形符号表示一个模块，该模块从其对应的具有相同颜色的令牌实例中预测下一个 SQL 关键字。</p><h2 id="2-1-模块概述"><a href="#2-1-模块概述" class="headerlink" title="2.1 模块概述"></a>2.1 模块概述</h2><p>SyntaxSQLNet模型将SQL语句的预测分解为9个模块，解码时由预定义的SQL语法确定这9个模块的调用顺序，从而引入结构信息。树的生成顺序为深度优先。分解出的9个模块有：</p><ul><li><strong>IUEN模块</strong>：预测INTERCEPT、UNION、EXCEPT、NONE（是否调用自身来生成嵌套查询）。</li><li><strong>KW模块</strong>：预测WHERE、GROUP BY、ORDER BY、SELECT关键字。spider数据集中的所有查询都有 SELECT。</li><li><strong>COL模块</strong>：预测列名。</li><li><strong>OP模块</strong>：预测&gt;、&lt;、=、&gt;=、&lt;=、!=、LIKE、NOT IN、IN、BETWEEN等运算符。</li><li><strong>AGG模块</strong>：预测MAX、MIN、SUM、COUNT、AVG 和 NONE 等聚合函数。</li><li><strong>Root/Terminal模块</strong>：预测子查询或终结符。</li><li><strong>AND/OR模块</strong>：预测条件表达式间的关系（AND、OR）</li><li><strong>DESC/ASC/LIMIT模块</strong>：预测与ORDER BY相关的关键字。只有在预测有ORDER BY才会调用。</li><li><strong>HAVING模块</strong>：预测与GROUP BY相关的Having从句。只有在预测有GROUP BY才会调用。</li></ul><h2 id="2-2-SQL语法"><a href="#2-2-SQL语法" class="headerlink" title="2.2 SQL语法"></a>2.2 SQL语法</h2><p>在解码过程中，给定当前的<strong>SQL关键字</strong>和<strong>SQL历史</strong>（我们为了得到当前关键字而浏览的token），确定要调用哪个模块，并预测要生成的下一个SQL标记。</p><p>为了在解码时调用HAVING和OP等模块，不仅要检查当前token实例的类型，还要查看之前解码的SQL token的类型对于HAVING模块是否为GROUP，对于OP模块是否为WHERE或HAVING。</p><h2 id="2-3-输入Encoder"><a href="#2-3-输入Encoder" class="headerlink" title="2.3 输入Encoder"></a>2.3 输入Encoder</h2><p>每个模块的输入包括三种类型的信息：问题、表模式和当前SQL解码历史路径。通过BiLSTM对问题进行编码。</p><h3 id="2-3-1-Table-Aware列表示"><a href="#2-3-1-Table-Aware列表示" class="headerlink" title="2.3.1 Table-Aware列表示"></a>2.3.1 Table-Aware列表示</h3><p>为了在测试中对新的数据库进行泛化，必须使模型学会从数据库模式中获取必要的信息，因此同时使用表名和列名来构建列嵌入。</p><ol><li>首先对表名进行embedding，得到每个表的<strong>表名向量</strong>。</li><li>对每个列名进行embedding，得到初始的<strong>列名向量</strong>。</li><li>将表名向量与列的类型信息（字符串或数字，主/外键）相连接产生<strong>列向量</strong>。</li><li>用一个BiLSTM连接数据库中的所有列，以获得高级列嵌入。</li></ol><p>我们的编码方案可以有效地捕获数据库模式中的<strong>全局（表名）</strong>和<strong>本地（列名和类型）</strong>信息，以在给定数据库的上下文中理解问题。</p><h3 id="2-3-2-SQL解码历史"><a href="#2-3-2-SQL解码历史" class="headerlink" title="2.3.2 SQL解码历史"></a>2.3.2 SQL解码历史</h3><p>通过传递 SQL 历史记录，每个模块在递归 SQL 生成过程中每次调用它时都能够根据历史记录预测不同的输出。SQL 历史记录可以提高每个模块在长而复杂的查询上的性能，因为历史记录有助于模型捕获子句之间的关系。</p><h2 id="2-4-模块详情"><a href="#2-4-模块详情" class="headerlink" title="2.4 模块详情"></a>2.4 模块详情</h2><p>与SQLNet类似，为每个模块采用了基于草图的方法，以避免在基于 seq2seq 的 SQL 生成模型中发生的顺序问题。</p><p>给定一个embedding$H_2$，计算embbeding$H_1$的条件嵌入$H_{1/2}$。</p><script type="math/tex; mode=display">H_{1/2}=\operatorname{softmax}(H_{1}WH_2^{T})H_{1}</script><p>从给定的分数矩阵 U 中得到概率分布的公式为：</p><script type="math/tex; mode=display">P(U)=\operatorname{softmax}(V\tanh(U))</script><p>将LSTM在<strong>问题嵌入</strong>、<strong>路径历史</strong>和<strong>列嵌入</strong>上的隐藏状态分别表示为$H_Q、H_{HS}$和$H_{COL}$。将LSTM在多重关键词嵌入和关键词嵌入上的隐藏状态分别表示为$H_{MKW}$和$H_{KW}$。每个模块的输出计算如下:</p><p><strong>IUEN模块</strong>：从{INTERCEPT, UNION, EXCEPT, NONE} 中选择一个调用，计算公式为：</p><script type="math/tex; mode=display">P_{IUEN}=P(W_{1}H^T_{Q/MKW}+W_{2}H^T_{HS/MKW}+W_{3}H^T_{MKW})</script><p><strong>KW模块</strong>：首先预测SQL查询中的关键词数量，然后从{SELECT, WHERE, GROUP BY, ORDER BY}中预测关键词。</p><script type="math/tex; mode=display">P_{KW}^{num}=P(W_1^{num}{H^{num}_{Q/KW}}^{T}+W_{2}^{num}{H^{num}_{HS/KW}}^{T}) \\\\P_{KW}^{val}=P(W_1^{val}{H^{val}_{Q/KW}}^{T}+W_{2}^{val}{H_{HS/KW}^{val}}^T+W^{val}_3{H_{KW}}^T)</script><p><strong>COL模块</strong>：首先预测 SQL 查询中的列数，然后预测要使用哪些列。</p><script type="math/tex; mode=display">P_{COL}^{num}=P(W_1^{num}{H^{num}_{Q/COL}}^{T}+W_{2}^{num}{H^{num}_{HS/COL}}^{T}) \\\\P_{COL}^{val}=P(W_1^{val}{H^{val}_{Q/COL}}^{T}+W_{2}^{val}{H_{HS/COL}^{val}}^T+W^{val}_3{H_{COL}}^T)</script><p><strong>OP模块</strong>：对于 WHERE 子句中来自 COL 模块的每个预测列，首先预测其上的运算符数量，然后从 {=, &gt;, &lt;, &gt;=, &lt;=, ! = , LIKE, NOTIN, IN, BETWEEN}中选择操作符。$H_{CS}$表示COL模块一个预测列的嵌入。</p><script type="math/tex; mode=display">P_{OP}^{num}=P(W_1^{num}{H^{num}_{Q/CS}}^{T}+W_{2}^{num}{H^{num}_{HS/CS}}^{T}+W^{num}_3{H_{CS}}^T) \\\\P_{OP}^{val}=P(W_1^{val}{H^{val}_{Q/CS}}^{T}+W_{2}^{val}{H_{HS/CS}^{val}}^T+W^{val}_3{H_{CS}}^T)</script><p><strong>AGG模块</strong>：对于 SELECT 子句中 COL 模块中的每个预测列，首先预测其上的聚合器数量，然后从 {MAX, MIN, SUM, COUNT, AVG, NONE}  预测要使用哪些聚合器</p><script type="math/tex; mode=display">P_{AGG}^{num}=P(W_1^{num}{H^{num}_{Q/CS}}^{T}+W_{2}^{num}{H^{num}_{HS/CS}}^{T}+W^{num}_3{H_{CS}}^T) \\\\P_{AGG}^{val}=P(W_1^{val}{H^{val}_{Q/CS}}^{T}+W_{2}^{val}{H_{HS/CS}^{val}}^T+W^{val}_3{H_{CS}}^T)</script><p><strong>Root/Terminal模块</strong>：对于 WHERE 子句中 COL 模块中的每个预测列，首先调用 OP 模块，然后预测下一个解码步骤是“ROOT”节点还是值终端节点。</p><script type="math/tex; mode=display">P_{RT}=P(W_1H_{Q/CS}^{T}+W_{2}H_{HS/CS}^{T}+W_3H_{CS}^T)</script><p><strong>AND/OR模块</strong>：对于从COL模块预测的每一个条件列（当列数大于1），预测{AND，OR}连接词。</p><script type="math/tex; mode=display">P_{AO}=P(W_1H_Q^T+W_2H_{HS}^T)</script><p><strong>DESC/ASC/LIMIT</strong>：对于ORDER BY子句中COL模块的每个预测列，从{DESC，ASC，DESC LIMIT，ASC LIMIT}进行预测关键词。</p><script type="math/tex; mode=display">P_{DAL}=P(W_1H_{Q/CS}^{T}+W_{2}H_{HS/CS}^{T}+W_3H_{CS}^T)</script><p><strong>HAVING模块</strong>：对于GROUP BY子句中的COL模块中的每个预测列，预测它是否在HAVING BY子句中</p><script type="math/tex; mode=display">P_{HAVING}=P(W_1H_{Q/CS}^{T}+W_{2}H_{HS/CS}^{T}+W_3H_{CS}^T)</script><h2 id="2-5-递归SQL生成"><a href="#2-5-递归SQL生成" class="headerlink" title="2.5 递归SQL生成"></a>2.5 递归SQL生成</h2><p>SQL生成过程是一个递归地激活不同模块的过程。如图2所示，采用一个堆栈来组织解码过程。在每个解码步骤中，从堆栈中弹出一个SQL关键词，并根据语法调用一个模块来预测下一个关键词，然后将预测的关键词推入堆栈。解码过程一直持续到堆栈为空。</p><blockquote><p>在第一个解码步骤中只用ROOT来初始化一个堆栈。在下一步，堆栈会弹出ROOT。如图2所示，ROOT激活IUEN模块来预测是否有EXCEPT、INTERSECT或UNION。如果是这样，下一步就会有两个子查询被生成。</p><p>如果模型预测为NONE，它将被推入堆栈。堆栈在下一步弹出NONE。例如，在图2中，当前弹出的标记是SELECT，它是一个关键字（KW）类型的实例。它调用COL模块来预测一个列名，该列名将被推到堆栈中。</p></blockquote><h1 id="3-数据增强"><a href="#3-数据增强" class="headerlink" title="3 数据增强"></a>3 数据增强</h1><p>该工作提出了一种针对text2sql任务的数据增强方法，生成跨领域、更多样的训练数据。通过该技术，模型的精确匹配率提高了7.5%。</p><p><strong>具体做法</strong>为：</p><ul><li>对SPIDER中的每条数据，将值和列名信息除去，得到一个模板；对处理后的SQL模版进行聚类，通过规则去除比较简单的模板，并依据模板出现的频率，挑选50个复杂SQL模板；人工核对SQL-问句对，确保SQL模板中每个槽在问句中都有对应类型的信息。</li><li>得到一一对应的模板后，应用于WikiSQL数据库：首先随机挑选10个模板，然后从库中选择相同类型的列，最后用列名和值填充SQL模板和问句模板。通过该方法，作者最终在18000的WikiSQL数据库上得到了新的98000组训练数据，同时在训练的时候也利用了WikiSQL数据集原有的训练数据。</li></ul><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/abs/1810.05237">SyntaxSQLNet: Syntax Tree Networks for Complex and Cross-DomainText-to-SQL Task</a></p>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HydraNet阅读笔记</title>
      <link href="/2022/03/16/HydraNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2022/03/16/HydraNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>以前的所有工作揭示了WikiSQL上<strong>NL2SQL</strong>的几个主要挑战：</p><ol><li>如何融合来自编码器处理的<strong>自然语言问题（NL question ）</strong>和<strong>表模式的信息（table schema）</strong>。</li><li>如何保证解码器处理输出的SQL查询<strong>可执行</strong>且<strong>准确</strong>。</li><li>如何利用预训练语言模型。</li></ol><h1 id="输入表示（Input-Representation）"><a href="#输入表示（Input-Representation）" class="headerlink" title="输入表示（Input Representation）"></a>输入表示（Input Representation）</h1><p>给定一个问题q和候选列$c_1, c_2, . . . ,c_k$，表示层的输入变成了<strong>每个列</strong>的列文本和query文本组成的对（Concat($\phi_{c_i},t_{c_i},c_i$),q），$\phi_{c_i}$表示文本中$c_i$列的类型（分别是<code>string</code>和<code>real</code>），这个信息在X-SQL中是用type embedding来注入模型的，但这里直接作为文本进行输入。这样对于每个列都是BERT标准的sentence pair输入任务；$t_{c_i}$是$c_i$所属表的表名；Concat()是将文本列表连接成一个字符串的函数。</p><script type="math/tex; mode=display">[CLS],x_1,x_2,...,x_m,[SEP],y_1,y_2,...,y_n,[SEP]</script><p>其中$x_1,x_2,…,x_m$是列表示Concat的token，$y_1,y_2,…,y_n$是问题q的token。</p><h1 id="SQL查询表示和任务"><a href="#SQL查询表示和任务" class="headerlink" title="SQL查询表示和任务"></a>SQL查询表示和任务</h1><p>在本文中，考虑没有嵌套结构的SQL查询，形式如下：</p><blockquote><p>”sql”:{<br>        “select”: [(agg1,scol1),(agg2,scol2),…]<br>        “from”: [table1,table2,…]<br>        “where”: [(wcol1,op1,value1),(wcol2,op2,value2),…]<br>        }</p></blockquote><p>将上述SQL查询中的对象分为两类：</p><ol><li>与具体列有关的任务，如W-COL，W-OP，W-VAL。这些任务被建模成sentence pair输入的分类任务和阅读理解任务；</li><li>与具体列无关的任务，如W-NUM和S-NUM。在X-SQL里由于一次输入了所有列，这俩可以一次性得到的；但由于HydraNet把列打散了，要么通过设定<strong>阈值</strong>来选取，要么需要对所有列的结果<strong>加权</strong>来得到。</li></ol><p>对于第一种类型，使用$h_{[CLS]},h_1^{c_i},…,h_m^{c_i},h_{[SEP]},h_1^q,…,h_n^q,h_{[SEP]}$表示base模型的输出序列嵌入。</p><ol><li>对于聚合算子$a_j，令P(a_{j}|c_{i},q)=\operatorname{softmax}(W^{agg}[j,:] \cdot h_{[CLS]})$。在训练期间，屏蔽掉不在聚合算子训练任务的 select 子句中的列。</li><li>对于条件运算符$o_j,令P(o_j|c_{i},q)=\operatorname{softmax}(W^{op}[j,:]\cdot h_{[CLS]})$。在训练期间，屏蔽了条件运算符训练任务中不在 where 子句中的列。</li><li>对于where 子句中value开始和结束的索引，令$P(y_{j}=\operatorname{start}|c_{i},q)=\operatorname{softmax}(W^{start} \cdot h_j^q)$和$P(y_{j}=\operatorname{end}|c_{i},q)=\operatorname{softmax}(W^{\operatorname{end}}\cdot h_{j}^{q})$。在训练期间，对于不在 where 子句中的列，开始和结束索引设置为 0。</li></ol><p>对于没有关联的全局对象，$P(z|q)=\sum_{c_{i}}P(z|c_{i},q)P(c_{i}|q)$。将$P(z|c_{i},q)$看做句子对分类，$P(c_{i},q)$看做列$c_i$和问题$q$之间的相似性。</p><ol><li>对于select子句的数量$n_s$，$P(n_s|q)=\sum_{c_{i}}P(n_s|c_{i},q)P(c_{i}|q)$</li><li>对于where子句的数量$n_w$，$P(n_w|q)=\sum_{c_{i}}P(n_w|c_{i},q)P(c_{i}|q)$</li></ol><h1 id="列排序"><a href="#列排序" class="headerlink" title="列排序"></a>列排序</h1><p>对于每个问题q，$S_q$是SELECT子句中的列集，$W_q$是WHERE子句中的列集。$R_q=S_q \bigcup W_q$表示SQL查询中的相关列集。将候选列集表示为$C_q={c_1,c_2,…,c_k}$。</p><p>排序任务分为三个：</p><ol><li>SELECT-Rank：根据q的SELECT子句是否包含$c_i,即c_i \in S_q$，对$c_i \in C_q$进行排序。</li><li>WHERE-Rank：根据q的WHERE子句是否包含$c_i,即c_i \in W_q$，对$c_i \in C_q$进行排序。</li><li>Relevance-Rank：根据q的SQL查询是否包含$c_i$,即$c_i \in R_q$，对$c_i \in C_q$进行排序。</li></ol><p>$P(c_{i}\in S_{q}|q)=\operatorname{sigmoid}(w_{sc}\cdot h_{[CLS]})$,$P(c_{i}\in W_{q}|q)=\operatorname{sigmoid}(w_{wc}\cdot h_{[CLS]})$,$P(c_{i}\in R_{q}|q)=\operatorname{sigmoid}(w_{rc}\cdot h_{[CLS]})$分别表示SELECT-Rank，WHERE-Rank，Relevance-Rank的排序分数。通过保留前$n_s,n_w$个列来构成SELECT子句和WHERE子句。</p><script type="math/tex; mode=display">\begin{align}\hat{n}_{s}&=\operatorname{argmax}P(n_{s}|q)=\sum_{c_i\in C_q}P(n_s|c_i,q)P(c_{i}\in R_q|q)\tag{1} \\\hat{n}_{w}&=\operatorname{argmax}P(n_{w}|q)=\sum_{c_i\in C_q}P(n_w|c_i,q)P(c_{i}\in R_q|q) \tag{2}\end{align}</script><h1 id="推理"><a href="#推理" class="headerlink" title="推理"></a>推理</h1><p>在推理过程中，首先从模型输出中获得每个单独任务的预测标签。然后按照以下步骤构造预测的SQL查询：</p><ol><li>计算每个对的所有子任务结果；</li><li>分别通过等式 1 和 2 获得预测的W-NUM和SEL-NUM；</li><li>对每个对针对select进行排序，选出得分最高的SEL-NUM个列及其相关的agg作为条件；</li><li>对每个对针对where进行排序，选出得分最高的W-NUM个列及其相关的val、op作为条件；</li><li>对于多表情况，综合多张表的前四步结果。</li></ol><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>展示 HydraNet 在 WikiSQL 数据集上的结果，并将其与其他最先进的方法进行比较。</p><ul><li>应用和不应用执行引导解码 (EG) 的不同方法的结果。</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202203191627791.png" alt="image-20220319162741706"></p><blockquote><ol><li>使用 BERT-Large-Uncased 的 HydraNet 明显优于使用相同基本模型的 SQLova，甚至与使用 MT-DNN 作为基本模型的 X-SQL  一样好。MT-DNN 已证明明显优于 BERT-Large-Uncased（Liu 等人，2019a），并且在 GLUE Benchmark 3 上的得分与  RoBERTa 相似。这意味着 HydraNet <strong>更擅长利用预训练的 Transformer 模型</strong>。</li><li>通过比较开发集和测试集的准确性，发现HydraNet也表现出更好的泛化能力，这是因为它只在基本模型的输出中添加了全连接层，这比X-SQL和SQLova的<strong>输出架构更简单且参数更少</strong>。</li></ol></blockquote><ul><li>应用和不应用 EG 的每个任务的准确性。</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202203191635450.png" alt="image-20220319163528401"></p><blockquote><p>在测试集上，HydraNet 的 SELECT 列预测准确率最高，WHERE 列预测准确率与 X-SQL 的 几乎相同。这证明了列-问题对排序机制在列比较和列选择方面与全列的问题排序机制是一样好的。</p></blockquote><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本文研究了如何在WikiSQL任务中利用BERT等预先训练好的语言模型。将文本到sql表述为一个列式混合排序问题，并提出了一个名为HydraNet的整洁网络结构，它最好地利用了预先训练的语言模型。提出的模型结构简单。</p><p><strong>在SELECT子句预测上HydraNet的准确率最高，在WHERE子句预测上X-SQL的准确率最高。</strong></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/abs/2008.04759">Hybrid Ranking Network for Text-to-SQL (arxiv.org)</a></p>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>交叉熵、相对熵（KL散度）</title>
      <link href="/2022/03/14/%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%88KL%E6%95%A3%E5%BA%A6%EF%BC%89/"/>
      <url>/2022/03/14/%E4%BA%A4%E5%8F%89%E7%86%B5%E3%80%81%E7%9B%B8%E5%AF%B9%E7%86%B5%EF%BC%88KL%E6%95%A3%E5%BA%A6%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="信息量"><a href="#信息量" class="headerlink" title="信息量"></a>信息量</h1><p>任何事件都会承载着一定的信息量，包括已经发生的事件和未发生的事件，只是它们承载的信息量会有所不同。如昨天下雨这个已知事件，因为已经发生，既定事实，那么它的信息量就为0。如明天会下雨这个事件，因为未有发生，那么这个事件的信息量就大。</p><p>已知某个事件的信息量是与它发生的概率有关，可以通过如下公式计算信息量：</p><blockquote><p>假设$X$是一个离散型随机变量，其取值集合为$\chi$，概率分布函数$ p ( x ) = Pr(X = x) , x \in \chi $，则定义事件$X=x_0$的信息量为：</p><script type="math/tex; mode=display">I(x_0)=-log(p(x_0))</script></blockquote><h1 id="熵"><a href="#熵" class="headerlink" title="熵"></a>熵</h1><p>当一个事件发生的概率为$p(x)$，那么它的信息量是$-log(p(x))$。</p><p>把这个事件的所有可能性罗列出来，就可以求得该事件信息量的期望，信息量的期望就是熵，所以熵的公式为：</p><blockquote><p>假设事件X共有n种可能，发生$x_i$的概率为$p(x_i)$，那么该事件的熵$H(X)$为：</p><script type="math/tex; mode=display">H(X)=-\sum_{i=1}^np(x_i)log(p(x_i))</script><p>然而有一类比较特殊的问题，比如投掷硬币只有两种可能，字朝上或花朝上。买彩票只有两种可能，中奖或不中奖。我们称之为<strong>0-1分布问题</strong>（二项分布的特例），对于这类问题，熵的计算方法可以简化为如下算式：</p><script type="math/tex; mode=display">H(X)=-\sum_{i=1}^np(x_i)log(p(x_i))=-p(x)log(p(x))-(1-p(x))log(1-p(x))</script></blockquote><h1 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h1><p>相对熵又称KL散度,如果我们对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 <strong>KL散度</strong>（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异。</p><p>在机器学习中，P往往用来表示样本的<strong>真实分布</strong>，Q用来表示模型所<strong>预测的分布</strong>，那么KL散度就可以计算两个分布的差异，也就是<strong>Loss损失值</strong>。</p><script type="math/tex; mode=display">D_{KL}(p||q)=\sum_{i=1}^{n}p(x_{i})\log(\frac{p(x_{i})}{q(x_{i})})</script><p>从KL散度公式中可以看到Q的分布越接近P（Q分布越拟合P），那么散度值越小，即损失值越小。</p><blockquote><ul><li>不是一个<strong>对称量</strong>，即$KL(p||q) \neq KL(q||p)$。</li><li><strong>KL散度大于等于0</strong>。KL 散度看做两个分布 p(x) 和 q(x)之间不相似程度的度量。</li><li><strong>最小化 Kullback-Leibler 散度等价于最大化似然函数</strong>。</li></ul></blockquote><h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>将KL散度公式进行变形：</p><script type="math/tex; mode=display">\begin{align*}D_{KL}(p||q)&=\sum_{i=1}^np(x_i)log(\frac{p(x_i)}{q(x_i)}) \\&=\sum_{i=1}^{n}p(x_{i})\log(p(x_{i}))-\sum_{i=1}^{n}p(x_{i})\log(q(x_{i})) \\&=-H(p(x))+[-\sum_{i=1}^{n}p(x_{i})\log(q(x_{i}))]\end{align*}</script><p>等式的前一部分就是p的熵，等式后一部分就是交叉熵：</p><script type="math/tex; mode=display">H(p,q)=-\sum_{i=1}^np(x_i)log(q(x_i))</script><blockquote><ul><li>在机器学习中，我们需要评估label和predicts之间的差距，使用KL散度刚刚好，即$D_{KL(y||\widetilde{y})}$。</li><li>由于KL散度中的前一部分<strong>熵−H(y)不变</strong>，故在优化过程中，只需要关注交叉熵就可以了。所以一般在机器学习中直接用交叉熵做loss，评估模型。</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树遍历的递归实现（先序、中序、后序和层次遍历）</title>
      <link href="/2022/02/21/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E7%9A%84%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%85%88%E5%BA%8F%E3%80%81%E4%B8%AD%E5%BA%8F%E3%80%81%E5%90%8E%E5%BA%8F%EF%BC%89/"/>
      <url>/2022/02/21/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86%E7%9A%84%E9%80%92%E5%BD%92%E5%AE%9E%E7%8E%B0%EF%BC%88%E5%85%88%E5%BA%8F%E3%80%81%E4%B8%AD%E5%BA%8F%E3%80%81%E5%90%8E%E5%BA%8F%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>由二叉树的定义可知，一棵二叉树由根结点、左子树和右子树三部分组成。因此，只要遍历了这三个部分，就可以实现遍历整个二叉树。若以D、L、R分别表示遍历根结点、左子树、右子树，则二叉树的递归遍历可以有一下四种方式：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/* 二叉树遍历框架 */</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">traverse</span>(<span class="params">TreeNode root</span>):</span></span><br><span class="line">    // 前序遍历</span><br><span class="line">    traverse(root.left)</span><br><span class="line">    // 中序遍历</span><br><span class="line">    traverse(root.right)</span><br><span class="line">    // 后序遍历</span><br></pre></td></tr></table></figure><h2 id="1、先序遍历（DLR）"><a href="#1、先序遍历（DLR）" class="headerlink" title="1、先序遍历（DLR）"></a>1、先序遍历（DLR）</h2><p><strong>口诀：</strong>根左右。前序遍历首先访问根结点然后遍历左子树，最后遍历右子树。在遍历左、右子树时，仍然先访问根节点，然后遍历左子树，最后遍历右子树。<br>若二叉树为空则结束返回，否则：<br>（1）访问根结点。<br>（2）前序遍历左子树<strong>。</strong><br>（3）前序遍历右子树 。<br>需要注意的是：遍历左右子树时仍然采用前序遍历方法。</p><p>举例：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202202212143928.png" alt="img"></p><blockquote><p>先序遍历结果：ABDEC</p><p>注意：已知后序遍历和中序遍历，就能确定先序遍历。</p></blockquote><p>代码：先序遍历：根节点-&gt;左子树-&gt;右子树</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 前序打印二叉树（递归）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preOrderTraverse</span>(<span class="params">node</span>):</span></span><br><span class="line">     <span class="keyword">if</span> node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">         <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">     <span class="built_in">print</span>(node.val)</span><br><span class="line">     preOrderTraverse(node.left)</span><br><span class="line">     preOrderTraverse(node.right)</span><br></pre></td></tr></table></figure><h2 id="2、中序遍历（LDR）"><a href="#2、中序遍历（LDR）" class="headerlink" title="2、中序遍历（LDR）"></a>2、中序遍历（LDR）</h2><p><strong>口诀：</strong>左根右。中序遍历首先遍历左子树，然后访问根结点，最后遍历右子树。</p><p>举例：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202202212148868.png" alt="img"></p><blockquote><p>中序遍历结果：DBEAC</p><p>注：二叉搜索树题目一般和中序遍历相关。</p></blockquote><p>代码：中序遍历：左子树-&gt;根节点-&gt;右子树</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 中序打印二叉树（递归）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inOrderTraverse</span>(<span class="params">node</span>):</span></span><br><span class="line">     <span class="keyword">if</span> node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">         <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">     inOrderTraverse(node.left)</span><br><span class="line">     <span class="built_in">print</span>(node.val)</span><br><span class="line">     inOrderTraverse(node.right)</span><br></pre></td></tr></table></figure><h2 id="3、后序遍历（LRD）"><a href="#3、后序遍历（LRD）" class="headerlink" title="3、后序遍历（LRD）"></a>3、后序遍历（LRD）</h2><p><strong>口诀：</strong>左右根。后序遍历首先遍历左子树，然后遍历右子树，最后访问根结点，在遍历左、右子树时，仍然先遍历左子树，然后遍历右子树，最后遍历根结点。即：<br>若二叉树为空则结束返回，<br>否则：<br>（1）后序遍历左子树<br>（2）后序遍历右子树<br>（3）访问根结点</p><p>举例：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202202212149735.png" alt="img"></p><blockquote><p>后序遍历结果：DEBCA</p><p>注：已知前序遍历和中序遍历，就能确定后序遍历。</p></blockquote><p>代码：后序遍历：左子树-&gt;右子树-&gt;根节点</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 后序打印二叉树（递归）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">postOrderTraverse</span>(<span class="params">node</span>):</span></span><br><span class="line">     <span class="keyword">if</span> node <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">         <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">     postOrderTraverse(node.left)</span><br><span class="line">     postOrderTraverse(node.right)</span><br><span class="line">     <span class="built_in">print</span>(node.val)</span><br></pre></td></tr></table></figure><h2 id="4、层次遍历"><a href="#4、层次遍历" class="headerlink" title="4、层次遍历"></a>4、层次遍历</h2><blockquote><p>（1）根结点入队列</p><p>（2）根结点出队列，根结点的左子树、右子树相继入队列</p><p>（3）根结点的左子树结点出队列，左子树结点的左子树、右子树相继入队列</p><p>（4）…….</p></blockquote><p>举例：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202202212159169.png" alt="img"></p><blockquote><p>层次遍历结果：ABCDEG</p></blockquote><p>代码：按层遍历：从上到下、从左到右按层遍历</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先进先出选用队列结构</span></span><br><span class="line">  <span class="keyword">import</span> queue</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">layerTraverse</span>(<span class="params">head</span>):</span></span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">      que = queue.Queue()      <span class="comment"># 创建先进先出队列</span></span><br><span class="line">      que.put(head)</span><br><span class="line">      <span class="keyword">while</span> <span class="keyword">not</span> que.empty():</span><br><span class="line">          head = que.get()    <span class="comment"># 弹出第一个元素并打印</span></span><br><span class="line">          <span class="built_in">print</span>(head.val)</span><br><span class="line">          <span class="keyword">if</span> head.left:       <span class="comment"># 若该节点存在左子节点,则加入队列（先push左节点）</span></span><br><span class="line">              que.put(head.left)</span><br><span class="line">          <span class="keyword">if</span> head.right:      <span class="comment"># 若该节点存在右子节点,则加入队列（再push右节点）</span></span><br><span class="line">              que.put(head.right)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 二叉树 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>X-SQL阅读笔记</title>
      <link href="/2021/12/04/X-SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/12/04/X-SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202112041713303.png" alt="image-20211204171307209"></p><p>X-SQL使用BERT预训练模型的上下文输出增强结构化模式表示，并结合类型信息学习下游任务的新模式表示。</p><p>整个体系结构由三层组成：<strong>序列编码器</strong>，<strong>上下文增强模式编码器</strong>，<strong>输出层</strong>。</p><h1 id="序列编码器"><a href="#序列编码器" class="headerlink" title="序列编码器"></a>序列编码器</h1><p>X-SQL在序列编码器中使用类似于BERT结构的模型，但有以下不同：</p><ul><li>每个表架构都会附加一个特殊的空列[EMPTY]。</li><li>Segment embeddings被type embeddings取代，四种不同的类型分别是：问题，类别列，数值列和特殊的空列。</li><li>使用MT-DNN初始化编码器，并且把BERT中的<strong>[CLS]</strong>标识符改为<strong>[CTX]</strong>，为了强调上下文信息是在那里捕获的，而不是下游任务的表示。</li></ul><h1 id="上下文增强模式编码器"><a href="#上下文增强模式编码器" class="headerlink" title="上下文增强模式编码器"></a>上下文增强模式编码器</h1><p>编码器的输出形式为$h_{[CTX]},h_{q_1},…,h_{q_n},h_{[SEP]},h_{C_{11}},…,h_{[SEP]},h_{C_{21}},…,h_{[SEP]},…,h_{[EMPTY]},h_{[SEP]}$</p><p>每个问句中的token编码为$h_{q_i}$，数据库表中第i列的第j个token编码为$h_{C_{ij}}$。上下文增强模式编码器通过$h_{[CTX]}$捕捉的全局语境信息俩加强原始编码器的输出，从而为每一列i学习一个新的表示$h_{C_i}$。</p><script type="math/tex; mode=display">h_{C_{i}}=\sum_{i=1}^{n_{i}}\alpha_{i}hC_{i}\tag{1}</script><p>其中$\alpha_{it}=SOFTMAX(s_{it})$。对齐模型$s_{it}$表示第i列的第t个token与全局上下文的匹配程度，f使用简单点积。</p><script type="math/tex; mode=display">s_{it}=f(Uh_{[CTX]}/\sqrt{d},Vh_{C_{it}}/\sqrt{d})</script><p>虽然在<strong>序列编码器</strong>的输出中捕获到一定程度的上下文，但是由于自注意力机制只集中于某些区域，因此这种影响是有限的。而<strong>[CTX]</strong>中捕获的全局上下文信息足够多样化，因此被用来补充<strong>序列编码器</strong>的模式表示。</p><h1 id="输出层"><a href="#输出层" class="headerlink" title="输出层"></a>输出层</h1><p>该任务被分解为6个子任务，分别是预测<code>W-NUM(条件个数）</code>，<code>W-COL（条件对应列，column_index）</code>，<code>W-OP(条件运算符,operator_index)</code>，<code>W-VAL(条件目标值，condition)</code>，<code>S-COL(查询目标列，sel)</code>，<code>S-AGG(查询聚合操作，agg)</code>，每个子任务预测最终SQL程序的一部分。</p><p>首先引入一个任务相关的子网络，它使用上下文$h_{CTX}$调节模式表示$h_{Ci}$。</p><script type="math/tex; mode=display">r_{C_{i}}=\operatorname{LayerNorm}(U^{\prime}h_{[crx]}+V^{\prime}h_{C_{i}})\tag{2}</script><blockquote><p>与公式1不同的是，此计算是针对每个子任务单独完成的，以便更好的将模式表示与每个子任务应关注的自然语言问句中的特定部分对齐。</p></blockquote><p><strong>S-COL</strong>：预测SELECT子句的列，是从所有的列中选一个。S-COL只依赖于$r_{Ci}$。</p><script type="math/tex; mode=display">p^{S-COL}(C_{i})=\operatorname{SOFTMAX}(W^{S-COL}r_{C_{i}})</script><p><strong>S-AGG</strong>：预测SELECT选择的列的聚合器，从可选的6种操作中选一个。为了增强聚合器依赖于所选列类型的直觉（例如MIN聚合器不使用字符串类型的列），显式地将列类型嵌入添加到模型中。</p><script type="math/tex; mode=display">p^{S-AGG}(A_{j}|C_{i})=\operatorname{SOFTMAX}(W^{S-AGG}[j,:]\times \\ \tag{3}\operatorname{LayerNorm}(U^{\prime\prime}h_{CTX}+V^{\prime\prime}hc_{i}+E^T_{C_i})</script><p><strong>W-NUM</strong>：使用$W^{W-NUM}h_{[CTX]}$查找WHERE子句的数量，由于数据集中绝大多数标签的条件都不会超过4个，<code>W-NUM</code>的预测被建模为一个<code>分类任务</code>，直接取<code>[CTX]</code>token的输出加全连接，这个结果需要最先计算得到。建模为四分类，每个标签代表最终SQL的1到4个WHERE子句。无法预测空WHERE子句的情况，空WHERE子句通过K-L散度由W-COL解决。</p><p><strong>W-COL</strong>：基于W-NUM预测的数量，为WHERE子句选择得分最高的列。</p><script type="math/tex; mode=display">p^{W-COL}(C_{i})=\operatorname{SOFTMAX}(W^{W-COL}r_{C_i})</script><p><strong>W-OP</strong>：为给定的WHERE列选择最可能的操作符，从可选的4种条件操作种选一个。</p><script type="math/tex; mode=display">P^{W-OP}(O_j|C_i) = \operatorname{SOFTMAX}(W^{W-OP}[j,:]r_{C_i})</script><p><strong>W-VAL</strong>：WHERE子句的预测值。被当做<code>阅读理解任务</code>，因为这个值只能从query获取，所以和阅读理解一样，是一个span prediction任务，即预测value值的start/end position。</p><script type="math/tex; mode=display">\begin{align*}p_{start}^{W-VAL}(q_j|C_i) &= \operatorname{SOFTMAX} g(U^{start}h_{q_j}+V^{start}r_{C_i})\\p_{end}^{W-VAL}(q_j|C_i) &= \operatorname{SOFTMAX} g(U^{end}h_{q_j}+V^{end}r_{C_i})\\其中：g(x)&:=Wx+b\end{align*}</script><h1 id="训练和推理"><a href="#训练和推理" class="headerlink" title="训练和推理"></a>训练和推理</h1><p>在训练期间，优化目标是单个子任务损失的总和。任务S-COL、S-AGG、W-NUM、W-OP和W-VAL使用交叉熵损失，W-COL的损失定义为$D(Q||P^{W-COL})$之间的KL散度。基本事实Q的分布计算如下：</p><ul><li>如果没有WHERE子句，$Q_{[EMPTY]}$接收特殊列[EMPTY]的概率为1。</li><li>对于n≥1个WHERE子句，每个WHERE列接收概率为$\frac{1}{n}$。</li></ul><p>如果得分最高的列是特殊列[EMPTY]，我们忽略W-NUM的输出并返回空的WHERE子句。否则，选择前W-NUM个W-COL中的非[EMPTY]列。</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p>使用WikiSQL数据集的默认train/dev/test拆分。评价标准是逻辑形式准确性（SQL查询的精确匹配）和执行准确性（预测的SQL查询导致正确答案的比率）。逻辑形式准确性是我们在训练期间优化的指标。</p><ul><li>不同方法在推理期间应用和不应用执行指导(EG)的结果</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202203121609965.png" alt="image-20220312160906924"></p><blockquote><p>在没有EG的情况下，X-SQL在逻辑形式准确性方面提供了2.6%的绝对改进(83.3对80.7)，在测试集上执行准确性方面提供了2.5%的改进。</p><p>即使有了EG，X-SQL在逻辑形式精度上还是好了2.4%，在执行精度上也是好了2.2%。值得注意的是，X-SQL+EG是第一个在测试集上超过90%准确率的模型。</p><p>根据Hwang等人(2019年)的说法，对于开发集，人类性能估计为88.2%。XSQL是第一个在没有执行指导的帮助下，比人类表现更好的模型。</p></blockquote><ul><li>不同方法的每个子任务的准确性</li></ul><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202203121614887.png" alt="image-20220312161455853"></p><blockquote><p>任务W-COL在没有EG的情况下显示了1.1%的绝对增益，在有EG的情况下显示了1.7%的绝对增益。将此归因于我们使用<strong>KL散度</strong>将where列预测形式化为列表排序问题的新方法。</p><p>另一个显著的改进是W-VAL任务，没有EG时的绝对增益为1.2%，有EG时的绝对增益为2.0%。这部分是由于列集预测(即W-COL)的改进，因为值生成高度依赖于where子句的预测列集。</p></blockquote><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>提出一个新的模型X-SQL，展示了它在WikiSQL任务上的出色性能，并在所有指标上实现了新的最先进水平。虽然围绕损失目标的贡献可能受到WikiSQL使用的特定SQL语法的限制，但如何利用上下文信息以及如何使用模式类型可以立即应用于涉及结构化数据的预训练语言模型的其他任务。未来的工作包括试验更复杂的数据集，如Spider(Yuetal.,2018b)。</p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://www.microsoft.com/en-us/research/publication/x-sql-reinforce-context-into-schema-representation/#:~:text=X-SQLproposestoenhancethestructuralschemarepresentation,WikiSQLdatasetandshowitsnewstate-of-the-artperformance.">X-SQL:reinforceschemarepresentationwithcontext-MicrosoftResearch</a></p>]]></content>
      
      
      <categories>
          
          <category> -[论文阅读] -[Text2SQL] </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -[论文阅读] -[Text2SQL] </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TypeSQL阅读笔记</title>
      <link href="/2021/11/29/TypeSQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/29/TypeSQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111292141258.png" alt="image-20211129214152174"></p><p>该模型基于SQLNet，使用模版填充的方法生成SQL语句。为了更好地建模文本中出现的罕见实体和数字，TypeSQL显式地赋予每个单词类型，使用3个独立模型来预测模版填充值。</p><p><strong>创新点</strong>：每一个文本中都对其进行类别识别，然后完成作为预先设定的条件放到模型中，这样可以最大限度地融合文本地类型。</p><h1 id="输入预处理的类型识别"><a href="#输入预处理的类型识别" class="headerlink" title="输入预处理的类型识别"></a>输入预处理的类型识别</h1><p>将问句分割n-gram （n取2到6），并搜索数据库表、列。对于匹配成功的部分赋值column类型赋予数字、日期四种类型：INTEGER、FLOAT、DATE、YEAR。对于命名实体，通过搜索FREEBASE，确定5种类型：PERSON，PLACE，COUNTREY，ORGANIZATION，SPORT。这五种类型包括了大部分实体类型。当可以访问数据库内容时，进一步将匹配到的实体标记为具体列名（而不只是column类型）</p><h1 id="输入编码"><a href="#输入编码" class="headerlink" title="输入编码"></a>输入编码</h1><p>输入编码器由两个BI-LSTM组成：BI_LSTM$^{QT}$（Question，Type）和BI_LSTM$^{COL}$（Column）。将问题中的单词及其对应的类型一起输入进BI_LSTM$^{QT}$中，将数据库中的列名输入进BI_LSTM$^{COL}$，那么输出的隐藏状态分别是$H^{QT}$和$H^{COL}$。</p><p>对于列名编码，SQLNet是对每一个列名使用BI-LSTM。而TypeSQL首先计算列名中单词嵌入的平均值，之后使用一个BI-LSTM进行编码。这种编码方法将结果提高了1.5%，并将训练时间缩短了一半。尽管列名的顺序并不重要，但这种改进归因于LSTM可以捕获它们的出现和关系。</p><h1 id="槽位填充模型"><a href="#槽位填充模型" class="headerlink" title="槽位填充模型"></a>槽位填充模型</h1><p>SQLNet为模版中的每一种成分设定了单独的模型；TypeSQL对此进行了改进，对于相似的成分，例如SELECT_COL 和COND_COL以及#COND（条件数），这些信息间有依赖关系，通过合并为单一模型，可以更好建模。TypeSQL使用3个独立模型来预测模版填充，：</p><ul><li>MODEL_COL：SELECT_COL，#COND，COND_COL</li><li>MODEL_AGG：AGG</li><li>MODEL_OPVAL：OP, COND_VAL</li></ul><p>其中每个模型中共享BI_LSTM$^{QT}$和BI_LSTM$^{COL}$的参数（总共6个BI-LSTM）。</p><p>三个模型都使用SQLNet中提出的column-attention机制来计算问题和类型的加权表示$H_{QT/CLO}$。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111292223754.png" alt="image-20211129222357711"></p><p>输入矩阵的每一行运用softmax得到$\alpha_{QT/COL}$注意力分数矩阵，$H_{QT/COL}$是问题和类型的加权表示。</p><h2 id="MODEL-COL"><a href="#MODEL-COL" class="headerlink" title="MODEL_COL"></a>MODEL_COL</h2><p><strong>SELECT_COL</strong>：使用$H_{QT/COL}$预测</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111292231306.png" alt="image-20211129223142283"></p><p><strong>#COND</strong>：与SQLNet不同，TypeSQL使用一种更简单的方法计算系统中的条件数，最大条件数设置为4。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111292233947.png" alt="image-20211129223335925"></p><p><strong>COND_COL</strong>：SQLNet经常在COND_COL选择与SELECT_COL相同的列名，为了避免这个问题，使用问题和类型的加权和来预测，该加权和以在SELECT_COL $H_{QT/SCOL}$中选择的列为条件。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111301606704.png" alt="image-20211130160636676"></p><h2 id="MODEL-AGG"><a href="#MODEL-AGG" class="headerlink" title="MODEL_AGG"></a>MODEL_AGG</h2><p><strong>AGG</strong>：以在SELECT_COL $H_{QT/SCOL}$中选择的列为条件的问题和类型的加权和来预测。AGG从{NULL，MAX，MIN，COUNT，SUM，AVG}中选择。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111301611526.png" alt="image-20211130161102503"></p><h2 id="MODEL-OPVAL"><a href="#MODEL-OPVAL" class="headerlink" title="MODEL_OPVAL"></a>MODEL_OPVAL</h2><p><strong>OP</strong>：对于每个条件列预测，从{=，&gt;，&lt;}选择</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111301615957.png" alt="image-20211130161509934"></p><p><strong>COND_VAL</strong>：从问题中为每个预测的COND_COL生成一个子字符串。编码器采用BI-LSTM，使用指针网络计算编码器中下一个token的分布。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111301637499.png" alt="image-20211130163707475"></p><p>其中h是前一个生成的单词的隐状态</p><h1 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h1><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111301638593.png" alt="image-20211130163846542"></p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111301638862.png" alt="image-20211130163859821"></p><h1 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h1><p><a href="https://arxiv.org/abs/1804.09769">TypeSQL: Knowledge-based Type-Aware Neural Text-to-SQL Generation</a></p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> Text2SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> Text2SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQLNet阅读笔记</title>
      <link href="/2021/11/18/SQLNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/18/SQLNet%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>该论文首次提出基于草图（sketch）的生成模型。基于草图的方法要完成的测序任务属于固定模式，不需要预测 SQL 语句中的所有内容，即只预测关键内容。（下图 a 中的标有“$”的部分）<br><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111182056374.png" alt="image-20211118094519481"></p><p>为了解决Seq2SQL使用强化学习效果不明显和WHERE子句条件“顺序问题”的问题，SQLNet将SQL语句分成了SELECT和WHERE两个部分，每个部分设置了几个槽位，只需向槽位中填入相应的符号即可。</p><p>SELECT子句部分与Seq2SQL类似，不同地方在于WHERE子句，它使用了一种sequence-to-set（由序列生成集合）机制，用于选取目标SQL语句中的WHERE子句可能出现的列。</p><h2 id="基于列注意的序列到集合预测"><a href="#基于列注意的序列到集合预测" class="headerlink" title="基于列注意的序列到集合预测"></a>基于列注意的序列到集合预测</h2><h3 id="Sequence-to-set预测"><a href="#Sequence-to-set预测" class="headerlink" title="Sequence-to-set预测"></a>Sequence-to-set预测</h3><p>预测哪些列名出现在感兴趣的子集中，而不是生成一系列列名。<img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111182056976.png" alt="image-20211118095355629"></p><p>$E_{col}$和$E_Q$是分别为列名col和问句Q的在双向LSTM中的隐藏状态，$u_c$和$u_q$是两个可训练列向量。</p><h2 id="Column-attention"><a href="#Column-attention" class="headerlink" title="Column attention"></a>Column attention</h2><p>由于$E_Q$仅计算问句的隐藏状态，因此无法记住在预测特定列名时有用的特定信息。如“number”与预测WHERE子句中的列“No.”更相关。当预测某一特定列时，embedding应该反映自然语言问题中最相关的信息。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111182056054.png" alt="image-20211118101121271"></p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202112022231849.png" alt="image-20211202223104789"></p><p>$w$是注意力权重，$H_Q$是问句Q经过LSTM的隐藏状态。</p><p>使用$E_{Q|col}$代替$E_Q$，得到了在WHERE子句中预测列名的最终模型：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111181019965.png" alt="image-20211118101943941"></p><h2 id="SQLNet-MODEL"><a href="#SQLNet-MODEL" class="headerlink" title="SQLNet MODEL"></a>SQLNet MODEL</h2><h3 id="WHERE子句预测"><a href="#WHERE子句预测" class="headerlink" title="WHERE子句预测"></a>WHERE子句预测</h3><p><strong>列槽位</strong>。首先预测WHERE子句中的条件个数k，然后选取概率最高的前k个列。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111181159487.png" alt="image-20211118115922455"></p><p>SQLNet选择使$P_{col}(K|Q)$最大化的列数K。</p><p><strong>OP槽位</strong>。对于WHERE子句中的每一列，预测OP操作符是一个三分类问题：{=，&gt;，&lt;}。SQLNet在OP预测中使用列注意来捕获依赖关系。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111181624768.png" alt="image-20211118162414734"></p><p><strong>VALUE槽位</strong>。SQLNet使用一个sequence-to-sequence结构来生成子字符串。编码器使用Bi-LSTM。解码器使用具有列注意机制的指针网络计算下一个token的分布。h是之前生成的序列的隐藏状态。自然语言问题中每个token的LSTM输出为$H_Q^i$</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111182056727.png" alt="image-20211118163318756"></p><h3 id="SELECT子句预测"><a href="#SELECT子句预测" class="headerlink" title="SELECT子句预测"></a>SELECT子句预测</h3><p>SELECT子句有一个聚合操作符和一个列名。SELECT子句中对列名的预测与WHERE子句类似。主要的区别是，对于select子句，我们只需要从所有列中选择一列。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111182056843.png" alt="image-20211118164419495"></p><p>对于聚合操作符，假设SELECT子句预测的列名是col，我们可以简单地计算：</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111181646226.png" alt="image-20211118164608205"></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>SQLNet-GENERATING STRUCTURED QUERIES FROM NATURAL LANGUAGE WITHOUT REINFORCEMENT LEARNING</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> Text2SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> Text2SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Seq2SQL阅读笔记</title>
      <link href="/2021/11/16/Seq2SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"/>
      <url>/2021/11/16/Seq2SQL%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="Seq2SQL"><a href="#Seq2SQL" class="headerlink" title="Seq2SQL"></a>Seq2SQL</h2><p>将生成的SQL语句分为三个部分：聚合操作Aggregation（SUM、COUNT、MIN、MAX等）；SELECT：选取列；WHERE：查询条件。</p><p>首先对query的聚合操作进行分类，并添加一个空操作符表示无聚合。接着指向输入表中对应于SELECT列的一列。最后通过pointer network生成SQL查询语句。</p><h3 id="聚合操作"><a href="#聚合操作" class="headerlink" title="聚合操作"></a>聚合操作</h3><p><strong>聚合操作的选择取决于问题</strong>。采用注意力机制进行分类。$ a_t^{inp}=W^{inp}h_t^{enc} $,代表输入序列的第t个token的注意力得分，权重矩阵与第t个token的输入编码乘积。归一化总的注意力得分，$ B^{inp}=softmax(a^{inp})$。输入表示$k^{agg}$是由归一化分数$B^{inp}$加权的输入编码$h^{enc}$之和。</p><script type="math/tex; mode=display">k^{agg}=\sum_tB_t^{inp}h_t^{enc}</script><p>$a^{agg}$表示聚合操作的得分，如COUNT，MIN，MAX和无聚合操作NULL。通过对输入表示$k^{agg}$应用多层感知机（MLP）来计算$a^{agg}$。</p><script type="math/tex; mode=display">a^{agg}=W^{agg}tanh(V^{agg}k^{agg}+b^{agg})+c^{agg}</script><p>最后通过softmax函数获取预测出的聚合操作$B^{agg}=softmax(a^{agg})$。</p><h2 id="SELECT-列"><a href="#SELECT-列" class="headerlink" title="SELECT 列"></a>SELECT 列</h2><p><strong>选择列取决于表列和问题</strong>。给定列表示和问题表示的列表，选择与问题最匹配的列。</p><script type="math/tex; mode=display">h_c^{j,t}=LSTM(emb(x^c_{j,t}),h^c_{j,t-1})\\e_j^c=h^c_{j,T_j}</script><p>$h_{j,t}^c$表示第j列的第t个encoder状态。将最后一个encoder状态设为$e_j^c$。</p><script type="math/tex; mode=display">a_j^{sel}=W^{sel}tanh(V^{sel}k^{sel}+V^ce_j^c)\\B^{sel}=softmax(a^{sel})</script><p>$k^{sel}$的构造与$k^{agg}$一样，但使用了不附带条件的权重。</p><h2 id="WHERE-子句"><a href="#WHERE-子句" class="headerlink" title="WHERE 子句"></a>WHERE 子句</h2><p><strong>使用指针网络训练WHERE子句</strong>。但是对于很多查询语句来说，WHERE子句的写法并不唯一，条件可以交换顺序，例如：</p><p>SELECT name FROM insurance WHERE age &gt; 18 AND gender =“male”;</p><p>$SELECT$ name FROM insurance WHERE gender = “male”AND age &gt; 18;</p><p>这可能导致原本正确的输出被判断为错误的。于是作者提出利用强化学习基于查询结果来进行优化。在解码器部分，对可能的输出进行采样，产生若干个SQL语句，每一句表示为y=[y1, y2 … yT]，用打分函数对每一句进行打分：</p><script type="math/tex; mode=display">R(q(y),q_g)= = \begin{cases}-2 & q(y)不是有效的SQL语句 \\-1 & q(y)是有效的SQL语句但得到一个不正确答案\\+1 & q(y)是有效的SQL语句并且得到正确答案\end{cases}</script><p>loss是对可能的WHERE子句的负预期回报，$L^{whe}=-E_y{[R(q(y)),q_g]}$。</p><p><img src="https://raw.githubusercontent.com/MCZ777/pic/main/img/202111161541254.png" alt="image-20211116105440662"></p><p>$p_y^{y_t}$表示在时间步t期间选择token$y_t$的概率。</p><h2 id="混合目标函数"><a href="#混合目标函数" class="headerlink" title="混合目标函数"></a>混合目标函数</h2><p>使用梯度下降法最小化目标函数$L=L^{agg}+L^{sel}+L^{whe}$。总梯度来自预测SELECT列的交叉损失、预测聚合操作的交叉损失以及策略学习的梯度的权重总和。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><p>Seq2SQL- Generating Structured Queries from Natural Language using Reinforcement Learning</p>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> Text2SQL </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
            <tag> Text2SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用Pytorch实现自动写诗</title>
      <link href="/2021/11/02/%E7%94%A8Pytorch%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%86%99%E8%AF%97/"/>
      <url>/2021/11/02/%E7%94%A8Pytorch%E5%AE%9E%E7%8E%B0%E8%87%AA%E5%8A%A8%E5%86%99%E8%AF%97/</url>
      
        <content type="html"><![CDATA[<h2 id="数据集"><a href="#数据集" class="headerlink" title="数据集"></a>数据集</h2><p>本次实验数据集来自GitHub上中文诗词爱好者收集的5万多首唐诗原文。原始文件是Json文件和Sqlite数据库的存储格式，此项目在此基础上做了两个修改：</p><ul><li>繁体中文改为简体中文：原始数据是繁体中文，更能保存诗歌的意境，但是对于习惯简体中文来说有点别扭。</li><li>把所有数据进行截断和补齐成一样的长度：由于不同诗歌的长度不一样，不易拼接成一个batch，因此需要将它们处理成一样的长度。</li><li>将原始数据集处理成一个numpy的压缩包tang.npz，里面包含三个对象：<ul><li>data（57580，125）的numpy数组，总共有57580首诗歌，每首诗歌长度为125字符。在诗歌的前面和后面加上起始符<START>和终止符<END>。长度不足125的诗歌，在前面补上空格（用&lt;/s&gt;表示）。对于长度超过125的诗歌，把结尾的词截断。之后将每个字都转成对应的序号。</li><li>word2ix：每个词和它对应的序号。</li><li>ix2word：每个序号和它对应的词。</li></ul></li></ul><p>在data.py中主要有以下三个函数：</p><ul><li>_parseRawData：解析原始的json数据，提取成list。</li><li>pad_sequences：将不同长度的数据截断或补齐成一样的长度。</li><li>get_data：给主程序调用的接口。如果如果二进制文件存在，直接读取二进制的numpy文件，否则读取原始文本文件进行处理，并把处理结果保存为二进制文件。</li></ul><p>data.py中的get_data代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_data</span>(<span class="params">opt</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    @param opt 配置选项 Config对象</span></span><br><span class="line"><span class="string">    @return word2ix: dict,每个字对应的序号，形如u&#x27;月&#x27;-&gt;100</span></span><br><span class="line"><span class="string">    @return ix2word: dict,每个序号对应的字，形如&#x27;100&#x27;-&gt;u&#x27;月&#x27;</span></span><br><span class="line"><span class="string">    @return data: numpy数组，每一行是一首诗对应的字的下标</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">if</span> os.path.exists(opt.pickle_path):</span><br><span class="line">        data = np.load(opt.pickle_path, allow_pickle=<span class="literal">True</span>)</span><br><span class="line">        data, word2ix, ix2word = data[<span class="string">&#x27;data&#x27;</span>], data[<span class="string">&#x27;word2ix&#x27;</span>].item(), data[<span class="string">&#x27;ix2word&#x27;</span>].item()</span><br><span class="line">        <span class="keyword">return</span> data, word2ix, ix2word</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 如果没有处理好的二进制文件，则处理原始的json文件</span></span><br><span class="line">    data = _parseRawData(opt.author, opt.constrain, opt.data_path, opt.category)</span><br><span class="line">    words = &#123;_word <span class="keyword">for</span> _sentence <span class="keyword">in</span> data <span class="keyword">for</span> _word <span class="keyword">in</span> _sentence&#125;</span><br><span class="line">    word2ix = &#123;_word: _ix <span class="keyword">for</span> _ix, _word <span class="keyword">in</span> <span class="built_in">enumerate</span>(words)&#125;</span><br><span class="line">    word2ix[<span class="string">&#x27;&lt;EOP&gt;&#x27;</span>] = <span class="built_in">len</span>(word2ix)  <span class="comment"># 终止标识符</span></span><br><span class="line">    word2ix[<span class="string">&#x27;&lt;START&gt;&#x27;</span>] = <span class="built_in">len</span>(word2ix)  <span class="comment"># 起始标识符</span></span><br><span class="line">    word2ix[<span class="string">&#x27;&lt;/s&gt;&#x27;</span>] = <span class="built_in">len</span>(word2ix)  <span class="comment"># 空格</span></span><br><span class="line">    ix2word = &#123;_ix: _word <span class="keyword">for</span> _word, _ix <span class="keyword">in</span> <span class="built_in">list</span>(word2ix.items())&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 为每首诗歌加上起始符和终止符</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(data)):</span><br><span class="line">        data[i] = [<span class="string">&quot;&lt;START&gt;&quot;</span>] + <span class="built_in">list</span>(data[i]) + [<span class="string">&quot;&lt;EOP&gt;&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 将每首诗歌保存的内容由‘字’变成‘数’</span></span><br><span class="line">    <span class="comment"># 形如[春,江,花,月,夜]变成[1,2,3,4,5]</span></span><br><span class="line">    new_data = [[word2ix[_word] <span class="keyword">for</span> _word <span class="keyword">in</span> _sentence]</span><br><span class="line">                <span class="keyword">for</span> _sentence <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 诗歌长度不够opt.maxlen的在前面补空格，超过的，删除末尾的</span></span><br><span class="line">    pad_data = pad_sequences(new_data,</span><br><span class="line">                             maxlen=opt.maxlen,</span><br><span class="line">                             padding=<span class="string">&#x27;pre&#x27;</span>,</span><br><span class="line">                             truncating=<span class="string">&#x27;post&#x27;</span>,</span><br><span class="line">                             value=<span class="built_in">len</span>(word2ix) - <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存成二进制文件</span></span><br><span class="line">    np.savez_compressed(opt.pickle_path,</span><br><span class="line">                        data=pad_data,</span><br><span class="line">                        word2ix=word2ix,</span><br><span class="line">                        ix2word=ix2word)</span><br><span class="line">    <span class="keyword">return</span> pad_data, word2ix, ix2word</span><br></pre></td></tr></table></figure><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h2><p>输入的字词序号经过nn.Embedding得到相应词的词向量表示，然后利用两层的LSTM提取词的所有隐藏元的信息，再利用隐藏元的信息进行分类，判断输出属于每一个词的概率。这里输入（input）的数据形状是(seq_len,batch_size)，如果输入的尺寸是(batch_size,seq_len)，需要在输入LSTM之前进行转置操作(variable.transpose)。</p><p>模型构建的代码在model.py中：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PoetryModel</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, vocab_size, embedding_dim, hidden_dim</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(PoetryModel, self).__init__()</span><br><span class="line">        self.hidden_dim = hidden_dim</span><br><span class="line">        self.embeddings = nn.Embedding(vocab_size, embedding_dim)</span><br><span class="line">        self.lstm = nn.LSTM(embedding_dim, self.hidden_dim, num_layers=<span class="number">2</span>)</span><br><span class="line">        self.linear1 = nn.Linear(self.hidden_dim, vocab_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, <span class="built_in">input</span>, hidden=<span class="literal">None</span></span>):</span></span><br><span class="line">        seq_len, batch_size = <span class="built_in">input</span>.size()</span><br><span class="line">        <span class="keyword">if</span> hidden <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment">#  h_0 = 0.01*torch.Tensor(2, batch_size, self.hidden_dim).normal_().cuda()</span></span><br><span class="line">            <span class="comment">#  c_0 = 0.01*torch.Tensor(2, batch_size, self.hidden_dim).normal_().cuda()</span></span><br><span class="line">            h_0 = <span class="built_in">input</span>.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">            c_0 = <span class="built_in">input</span>.data.new(<span class="number">2</span>, batch_size, self.hidden_dim).fill_(<span class="number">0</span>).<span class="built_in">float</span>()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            h_0, c_0 = hidden</span><br><span class="line">        <span class="comment"># size: (seq_len,batch_size,embeding_dim)</span></span><br><span class="line">        embeds = self.embeddings(<span class="built_in">input</span>)</span><br><span class="line">        <span class="comment"># output size: (seq_len,batch_size,hidden_dim)</span></span><br><span class="line">        output, hidden = self.lstm(embeds, (h_0, c_0))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># size: (seq_len*batch_size,vocab_size)</span></span><br><span class="line">        output = self.linear1(output.view(seq_len * batch_size, -<span class="number">1</span>))</span><br><span class="line">        <span class="keyword">return</span> output, hidden</span><br></pre></td></tr></table></figure><p>模型训练代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        <span class="built_in">setattr</span>(opt, k, v)  <span class="comment"># setattr() 函数指定对象opt的指定属性k的值v。</span></span><br><span class="line"></span><br><span class="line">    opt.device = t.device(<span class="string">&#x27;cuda&#x27;</span>) <span class="keyword">if</span> opt.use_gpu <span class="keyword">else</span> t.device(<span class="string">&#x27;cpu&#x27;</span>)</span><br><span class="line">    device = opt.device</span><br><span class="line">    vis = Visualizer(env=opt.env)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 获取数据</span></span><br><span class="line">    data, word2ix, ix2word = get_data(opt)</span><br><span class="line">    data = t.from_numpy(data)</span><br><span class="line">    dataloader = t.utils.data.DataLoader(data,</span><br><span class="line">                                         batch_size=opt.batch_size,</span><br><span class="line">                                         shuffle=<span class="literal">True</span>,</span><br><span class="line">                                         num_workers=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 模型定义</span></span><br><span class="line">    model = PoetryModel(<span class="built_in">len</span>(word2ix), <span class="number">128</span>, <span class="number">256</span>)</span><br><span class="line">    optimizer = t.optim.Adam(model.parameters(), lr=opt.lr)</span><br><span class="line">    criterion = nn.CrossEntropyLoss()</span><br><span class="line">    <span class="keyword">if</span> opt.model_path:  <span class="comment"># 若预训练模型存在，直接调用</span></span><br><span class="line">        model.load_state_dict(t.load(opt.model_path))</span><br><span class="line">    model.to(device)</span><br><span class="line"></span><br><span class="line">    loss_meter = meter.AverageValueMeter()  <span class="comment"># 添加单值数据，进行取平均及方差计算。</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(opt.epoch):</span><br><span class="line">        loss_meter.reset()  <span class="comment"># 使用重置(清空序列)</span></span><br><span class="line">        <span class="keyword">for</span> ii, data_ <span class="keyword">in</span> tqdm.tqdm(<span class="built_in">enumerate</span>(dataloader)):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># 训练</span></span><br><span class="line">            data_ = data_.long().transpose(<span class="number">1</span>, <span class="number">0</span>).contiguous()  <span class="comment"># contiguous()断开和转置前的数据的依赖</span></span><br><span class="line">            data_ = data_.to(device)</span><br><span class="line">            optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 输入是除最后一个字前面的所有字</span></span><br><span class="line">            <span class="comment"># 输出是从第二个字到最后一个字</span></span><br><span class="line">            <span class="comment"># 如:输入床前明月，输出前明月光</span></span><br><span class="line">            input_, target = data_[:-<span class="number">1</span>, :], data_[<span class="number">1</span>:, :]</span><br><span class="line">            output, _ = model(input_)</span><br><span class="line">            loss = criterion(output, target.view(-<span class="number">1</span>))</span><br><span class="line">            loss.backward()  <span class="comment"># 反向传播计算得到每个参数的梯度值</span></span><br><span class="line">            optimizer.step()  <span class="comment"># 梯度下降执行一步参数更新</span></span><br><span class="line"></span><br><span class="line">            loss_meter.add(loss.item())  <span class="comment"># 添加loss</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># 可视化</span></span><br><span class="line">            <span class="keyword">if</span> (<span class="number">1</span> + ii) % opt.plot_every == <span class="number">0</span>:</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> os.path.exists(opt.debug_file):</span><br><span class="line">                    ipdb.set_trace()</span><br><span class="line"></span><br><span class="line">                vis.plot(<span class="string">&#x27;loss&#x27;</span>, loss_meter.value()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">                <span class="comment"># 诗歌原文</span></span><br><span class="line">                poetrys = [[ix2word[_word] <span class="keyword">for</span> _word <span class="keyword">in</span> data_[:, _iii].tolist()]</span><br><span class="line">                           <span class="keyword">for</span> _iii <span class="keyword">in</span> <span class="built_in">range</span>(data_.shape[<span class="number">1</span>])][:<span class="number">16</span>]</span><br><span class="line">                vis.text(<span class="string">&#x27;&lt;/br&gt;&#x27;</span>.join([<span class="string">&#x27;&#x27;</span>.join(poetry) <span class="keyword">for</span> poetry <span class="keyword">in</span> poetrys]), win=<span class="string">u&#x27;origin_poem&#x27;</span>)</span><br><span class="line"></span><br><span class="line">                gen_poetries = []</span><br><span class="line">                <span class="comment"># 分别以这几个字作为诗歌的第一个字，生成8首诗</span></span><br><span class="line">                <span class="keyword">for</span> word <span class="keyword">in</span> <span class="built_in">list</span>(<span class="string">u&#x27;春江花月夜凉如水&#x27;</span>):</span><br><span class="line">                    gen_poetry = <span class="string">&#x27;&#x27;</span>.join(generate(model, word, ix2word, word2ix))</span><br><span class="line">                    gen_poetries.append(gen_poetry)</span><br><span class="line">                vis.text(<span class="string">&#x27;&lt;/br&gt;&#x27;</span>.join([<span class="string">&#x27;&#x27;</span>.join(poetry) <span class="keyword">for</span> poetry <span class="keyword">in</span> gen_poetries]), win=<span class="string">u&#x27;gen_poem&#x27;</span>)</span><br><span class="line"></span><br><span class="line">        t.save(model.state_dict(), <span class="string">&#x27;%s_%s.pth&#x27;</span> % (opt.model_prefix, epoch))  <span class="comment"># 保存模型</span></span><br></pre></td></tr></table></figure><p>这里需要注意的是数据，以“床前明月光”这句诗为例，输入是“床前明月”，预测的目标是“前明月光”：</p><ul><li>输入“床”的时候，网络预测下一个字的目标是“前”</li><li>输入“前”的时候，网络预测下一个字的目标是“明”</li><li>。。。。。。</li></ul><p>这种错位的方式，通过data_ [:-1,:]和data_ [1:,:]实现。前者包含从第0个词直到最后一个词（不包括），后者是第一个词到结尾（包括最后一个词）。</p><h2 id="生成诗歌"><a href="#生成诗歌" class="headerlink" title="生成诗歌"></a>生成诗歌</h2><p>本项目实现两种生成诗歌的方式：给定诗歌的开头接着写诗歌；藏头诗。</p><h3 id="续写诗歌"><a href="#续写诗歌" class="headerlink" title="续写诗歌"></a>续写诗歌</h3><p>这种生成方式是根据给定部分文字，然后接着完成诗歌余下的部分。</p><ul><li>首先利用给定的文字，计算隐藏元，并预测下一个词。</li><li>将上一步的隐藏元和输出作为新的输入，继续预测新的输出和计算隐藏元。</li><li>。。。。。。</li></ul><p>这里还有一个选项是<strong>prefix_word</strong>，可以控制生成的诗歌的意境和长短。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate</span>(<span class="params">model, start_words, ix2word, word2ix, prefix_words=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    给定几个词，根据这几个词接着生成一首完整的诗歌</span></span><br><span class="line"><span class="string">    start_words：u&#x27;春江潮水连海平&#x27;</span></span><br><span class="line"><span class="string">    比如start_words 为 春江潮水连海平，可以生成：</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    results = <span class="built_in">list</span>(start_words)</span><br><span class="line">    start_word_len = <span class="built_in">len</span>(start_words)</span><br><span class="line">    <span class="comment"># 手动设置第一个词为&lt;START&gt;，设置开始标识</span></span><br><span class="line">    <span class="built_in">input</span> = t.Tensor([word2ix[<span class="string">&#x27;&lt;START&gt;&#x27;</span>]]).view(<span class="number">1</span>, <span class="number">1</span>).long()</span><br><span class="line">    <span class="keyword">if</span> opt.use_gpu:</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.cuda()</span><br><span class="line">    hidden = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用以控制生成诗歌的意境和长短（五言还是七言）</span></span><br><span class="line">    <span class="keyword">if</span> prefix_words:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> prefix_words:</span><br><span class="line">            output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">            <span class="comment"># new()函数用来创建与输入的type和device都相同的空tensor</span></span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.data.new([word2ix[word]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(opt.max_gen_len):</span><br><span class="line">        output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i &lt; start_word_len:</span><br><span class="line">            <span class="comment"># “床前明月光”五个字依次作为输入，计算隐藏元</span></span><br><span class="line">            w = results[i]</span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.data.new([word2ix[w]]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 用预测的词作为新的输入，计算隐藏元和预测新的输出</span></span><br><span class="line">            top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item() <span class="comment"># 返回output中top1的第一个位置</span></span><br><span class="line">            w = ix2word[top_index]</span><br><span class="line">            results.append(w)</span><br><span class="line">            <span class="built_in">input</span> = <span class="built_in">input</span>.data.new([top_index]).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">if</span> w == <span class="string">&#x27;&lt;EOP&gt;&#x27;</span>:</span><br><span class="line">            <span class="comment"># 遇到结束符就结束</span></span><br><span class="line">            <span class="keyword">del</span> results[-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><h3 id="藏头诗"><a href="#藏头诗" class="headerlink" title="藏头诗"></a>藏头诗</h3><p>生成藏头诗的步骤如下：</p><ul><li>输入藏头的字，开始预测下一个字。</li><li>上一步预测的字作为输入，继续预测下一个字。</li><li>重复上一步，直到输出的字是“。”或者“！”，说明一句诗结束了，可以继续输入下一句藏头的字，跳到第一步。</li><li>重复上述步骤直到所有藏头的字都输入完毕。</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_acrostic</span>(<span class="params">model, start_words, ix2word, word2ix, prefix_words=<span class="literal">None</span></span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    生成藏头诗</span></span><br><span class="line"><span class="string">    start_words : u&#x27;深度学习&#x27;</span></span><br><span class="line"><span class="string">    生成：</span></span><br><span class="line"><span class="string">    深木通中岳，青苔半日脂。</span></span><br><span class="line"><span class="string">    度山分地险，逆浪到南巴。</span></span><br><span class="line"><span class="string">    学道兵犹毒，当时燕不移。</span></span><br><span class="line"><span class="string">    习根通古岸，开镜出清羸。</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    results = []</span><br><span class="line">    start_word_len = <span class="built_in">len</span>(start_words)</span><br><span class="line">    <span class="built_in">input</span> = (t.Tensor([word2ix[<span class="string">&#x27;&lt;START&gt;&#x27;</span>]]).view(<span class="number">1</span>, <span class="number">1</span>).long())</span><br><span class="line">    <span class="keyword">if</span> opt.use_gpu:</span><br><span class="line">        <span class="built_in">input</span> = <span class="built_in">input</span>.cuda()</span><br><span class="line">    hidden = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    index = <span class="number">0</span>  <span class="comment"># 用来指示已经生成了多少句藏头诗</span></span><br><span class="line">    <span class="comment"># 上一个词</span></span><br><span class="line">    pre_word = <span class="string">&#x27;&lt;START&gt;&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> prefix_words:</span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> prefix_words:</span><br><span class="line">            output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">            <span class="built_in">input</span> = (<span class="built_in">input</span>.data.new([word2ix[word]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(opt.max_gen_len):</span><br><span class="line">        output, hidden = model(<span class="built_in">input</span>, hidden)</span><br><span class="line">        top_index = output.data[<span class="number">0</span>].topk(<span class="number">1</span>)[<span class="number">1</span>][<span class="number">0</span>].item()</span><br><span class="line">        w = ix2word[top_index]</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (pre_word <span class="keyword">in</span> &#123;<span class="string">u&#x27;。&#x27;</span>, <span class="string">u&#x27;！&#x27;</span>, <span class="string">&#x27;&lt;START&gt;&#x27;</span>&#125;):</span><br><span class="line">            <span class="comment"># 如果遇到句号，感叹号，藏头的词送进去生成</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> index == start_word_len:</span><br><span class="line">                <span class="comment"># 如果生成的诗歌已经包含全部藏头的词，则结束</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 把藏头的词作为输入送入模型</span></span><br><span class="line">                w = start_words[index]</span><br><span class="line">                index += <span class="number">1</span></span><br><span class="line">                <span class="built_in">input</span> = (<span class="built_in">input</span>.data.new([word2ix[w]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 否则的话，把上一次预测是词作为下一个词输入</span></span><br><span class="line">            <span class="built_in">input</span> = (<span class="built_in">input</span>.data.new([word2ix[w]])).view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        results.append(w)</span><br><span class="line">        pre_word = w</span><br><span class="line">    <span class="keyword">return</span> results</span><br></pre></td></tr></table></figure><h3 id="命令行接口"><a href="#命令行接口" class="headerlink" title="命令行接口"></a>命令行接口</h3><p>上述两种生成诗歌的方法还需要提供命令行接口，实现方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen</span>(<span class="params">**kwargs</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    提供命令行接口，用以生成相应的诗</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> kwargs.items():</span><br><span class="line">        <span class="built_in">setattr</span>(opt, k, v)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 加载数据和模型</span></span><br><span class="line">    data, word2ix, ix2word = get_data(opt)</span><br><span class="line">    model = PoetryModel(<span class="built_in">len</span>(word2ix), <span class="number">128</span>, <span class="number">256</span>);</span><br><span class="line">    map_location = <span class="keyword">lambda</span> s, l: s</span><br><span class="line">    state_dict = t.load(opt.model_path, map_location=map_location)</span><br><span class="line">    model.load_state_dict(state_dict)</span><br><span class="line">    <span class="keyword">if</span> opt.use_gpu:</span><br><span class="line">        model.cuda()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># python2和python3 字符串兼容</span></span><br><span class="line">    <span class="keyword">if</span> sys.version_info.major == <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">if</span> opt.start_words.isprintable():</span><br><span class="line">            start_words = opt.start_words</span><br><span class="line">            prefix_words = opt.prefix_words <span class="keyword">if</span> opt.prefix_words <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            start_words = opt.start_words.encode(<span class="string">&#x27;ascii&#x27;</span>, <span class="string">&#x27;surrogateescape&#x27;</span>).decode(<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">            prefix_words = opt.prefix_words.encode(<span class="string">&#x27;ascii&#x27;</span>, <span class="string">&#x27;surrogateescape&#x27;</span>).decode(</span><br><span class="line">                <span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">if</span> opt.prefix_words <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        start_words = opt.start_words.decode(<span class="string">&#x27;utf8&#x27;</span>)</span><br><span class="line">        prefix_words = opt.prefix_words.decode(<span class="string">&#x27;utf8&#x27;</span>) <span class="keyword">if</span> opt.prefix_words <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 编码问题，半角改成全角，古诗中都是全角符号</span></span><br><span class="line">    start_words = start_words.replace(<span class="string">&#x27;,&#x27;</span>, <span class="string">u&#x27;，&#x27;</span>) \</span><br><span class="line">        .replace(<span class="string">&#x27;.&#x27;</span>, <span class="string">u&#x27;。&#x27;</span>) \</span><br><span class="line">        .replace(<span class="string">&#x27;?&#x27;</span>, <span class="string">u&#x27;？&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断是藏头诗还是普通诗歌</span></span><br><span class="line">    gen_poetry = gen_acrostic <span class="keyword">if</span> opt.acrostic <span class="keyword">else</span> generate</span><br><span class="line"></span><br><span class="line">    result = gen_poetry(model, start_words, ix2word, word2ix, prefix_words)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;&#x27;</span>.join(result))</span><br></pre></td></tr></table></figure><h2 id="实验结果分析"><a href="#实验结果分析" class="headerlink" title="实验结果分析"></a>实验结果分析</h2><p>生成藏头诗：</p><blockquote><p><strong>深山喷雾吼不灭，胡兵髓菜无冬冬。度胡胡马邯郸市，碣兵走马车声里。学军使者何在人，杀我营营军未起。习家车马不可渝，当时汉武无能犒。</strong></p></blockquote><p>生成续写诗：</p><blockquote><p><strong>深度学习狐狸父，截泻巉巖石翎粟。巨鼇万里云气浮，英雄夜立乾坤宝。炎炎咆哮不可测，苍苍云飞生沙徼。三峡浮云起孤云，千门万户横河北。回看龙虎出汉宫，飞来势稍凌穹穹。天下一星出天马，五月四面蛟龙泉。射洪万国天一隅，苍蝇不碍蟾蜍倾。羣仙星龛不可测，霹雳怒之瞥如昨。安危熊鹜势相续，狮子黄金射雕髪。星纲照耀光照明，朝开紫极连云行。耿晨告止赴丰奏，回望关河信横旋。回头向日傍山去，却踏青云出帝城。西陵不死乌鸢死，</strong></p></blockquote><p>生成的很多诗歌都是高质量的，有些甚至已经学会了简单的对偶和押韵。如果生成的诗歌长度足够长，会发现生成的诗歌意境会慢慢改变，以至于和最开始毫无关系。</p><p>意境、格式和韵脚等信息都保存在隐藏元之中，随着输入的不断变化，隐藏元保存的信息也在不断变化，有些信息即使经过了很长时间依旧可以保存下来（比如诗歌的长短，五言还是七言），而有些信息随着输入变化也发生较大的改变。</p><p>总体上，程序生成的诗歌效果还不错，字词之间的组合也比较有意境，但是诗歌缺乏一个主题，很难从一首诗歌中得到一个主旨。这是因为随着诗歌长度的增长，即使LSTM也会忘记几十个字之前的输入。</p><h2 id="参考文章"><a href="#参考文章" class="headerlink" title="参考文章"></a>参考文章</h2><p>1.<a href="https://github.com/chenyuntc/pytorch-book/tree/master/chapter09-neural_poet_RNN">pytorch-book/chapter09-neural_poet_RNN</a></p><p>2.<a href="https://github.com/chinese-poetry/chinese-poetry/tree/master/json">chinese-poetry</a></p><p>3.深度学习框架PyTorch：入门与实践_陈云(著)  第九章</p>]]></content>
      
      
      <categories>
          
          <category> NLP项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LSTM </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>TextCNN</title>
      <link href="/2021/10/25/TextCNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
      <url>/2021/10/25/TextCNN%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h1 id="TextCNN"><a href="#TextCNN" class="headerlink" title="TextCNN"></a>TextCNN</h1><ul><li><p>原理：核心点在于使用卷积来捕捉局部相关性，在文本分类任务中可以利用CNN提取句子中类似n-gram的关键信息。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/f68ee418c9058df7.png" alt="textcnn"></p></li><li><p>textcnn详细过程：第一层是图中最左边的7乘5的句子矩阵，每行是词向量，维度=5。然后经过不同 filter_size的一维卷积层（这里是2,3,4），每个filter_size 有filter_num（这里是2）个输出 channel。第三层是一个1-max pooling层，这样不同长度句子经过pooling层之后都能变成定长的表示了，最后接一层全连接的 softmax 层，输出每个类别的概率。</p></li><li><p>一维卷积(conv-1d)：经过词向量表达的文本为一维数据，因此在TextCNN卷积用的是一维卷积。</p></li></ul><p><strong>使用数据集：</strong><a href="https://pan.baidu.com/s/1hugrfRu#qfud">CNEWS</a></p><p><strong>pytorch代码实现：</strong><a href="https://github.com/MCZ777/NLP-task-baseline/tree/main/textcnn_baseline">textcnn_baseline</a></p><p><strong>TextCNN网络</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TextCNN</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;CNN模型&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(TextCNN, self).__init__()</span><br><span class="line">        self.embedding = nn.Embedding(<span class="number">5000</span>, <span class="number">64</span>)</span><br><span class="line">        self.conv = nn.Sequential(nn.Conv1d(in_channels=<span class="number">64</span>, out_channels=<span class="number">256</span>, kernel_size=<span class="number">5</span>),</span><br><span class="line">                                  nn.ReLU(),</span><br><span class="line">                                  nn.MaxPool1d(kernel_size=<span class="number">596</span>))</span><br><span class="line">        self.f1 = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, x</span>):</span></span><br><span class="line">        x = self.embedding(x)  <span class="comment"># batch_size,length,embedding_size  64*600*64</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># 将tensor的维度换位。64*64*600</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># batch_size,卷积核个数out_channels，(句子长度-kernel_size)/步长+1</span></span><br><span class="line">        x = self.conv(x)  <span class="comment"># Conv1后64*256*596,ReLU后不变,MaxPool1d后64*256*1;</span></span><br><span class="line"></span><br><span class="line">        x = x.view(-<span class="number">1</span>, x.size(<span class="number">1</span>))  <span class="comment"># 64*256</span></span><br><span class="line">        x = F.dropout(x, <span class="number">0.8</span>)</span><br><span class="line">        x = self.f1(x)  <span class="comment"># 64*10 batch_size * class_num</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>参考</strong></p><ol><li>Convolutional Neural Networks for Sentence Classification</li><li><a href="https://github.com/Alic-yuan/nlp-beginner-finish/">https://github.com/Alic-yuan/nlp-beginner-finish/</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP项目 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2021/10/25/hello-world/"/>
      <url>/2021/10/25/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Text2SQL综述</title>
      <link href="/2021/09/24/Text2SQL%E7%BB%BC%E8%BF%B0/"/>
      <url>/2021/09/24/Text2SQL%E7%BB%BC%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><p>Text-to-SQL系统能够将自然语言描述转化成对应的SQL查询语句，这项技术能够有效地辅助人们对海量的数据库进行查询。因此，该项研究引起了工业界和学术界的广泛关注。其中，WikiSQL、Spider等大规模标注数据集进一步指出了该技术目前面临的挑战：泛化性（跨领域）、复杂性（SQL语法）、正确性（问题和表格的对齐关系），也促进了一系列后续算法的研究与系统的开发。</p><p>在这里，我们给出Text-to-SQL任务一个相对正式的定义：在给定关系型数据库（或表）的前提下，由用户的提问生成相应的SQL查询语句。下图是一个具体的实例，问题为：有哪些系的教员平均工资高于总体平均值，请返回这些系的名字以及他们的平均工资值。可以看到该问题对应的SQL语句是很复杂的，并且有嵌套关系。</p><p><img src="https://img12.360buyimg.com/ddimg/jfs/t1/200804/36/13422/71331/6177b9d5Ea3cf4c3d/346cef3efb25db82.jpg" alt="Spider数据集的样例"></p><center>Spider数据集的样例</center><h1 id="相关数据集介绍"><a href="#相关数据集介绍" class="headerlink" title="相关数据集介绍"></a>相关数据集介绍</h1><p><img src="https://img14.360buyimg.com/ddimg/jfs/t1/210442/25/6862/150286/6177b984Eedf69cb5/09575946956a40fb.png" alt="现有的Text2SQL数据集"></p><center>现有的Text2SQL数据集</center><h2 id="早期数据集：ATIS-amp-GeoQuery"><a href="#早期数据集：ATIS-amp-GeoQuery" class="headerlink" title="早期数据集：ATIS&amp;GeoQuery"></a>早期数据集：ATIS&amp;GeoQuery</h2><p>ATIS来源于机票订阅系统，由用户提问生成SQL语句，是一个单一领域且上下文相关的数据集。GeoQuery来源于美国的地理，包括880条的提问与SQL语句，是一个单一领域且上下文无关的数据集。</p><h2 id="WikiSQL"><a href="#WikiSQL" class="headerlink" title="WikiSQL"></a>WikiSQL</h2><p>ATIS&amp;GeoQuery这两个数据集存在着数据规模小（SQL不足千句），标注简单等问题。于是，2017年，VictorZhong等研究人员基于维基百科，标注了80654的训练数据，涵盖了26521个数据库，取名为WikiSQL。这个大型数据集一经推出，便引起学术界的广泛关注。因为它对模型的设计提出了新的挑战，需要模型更好的建构Text和SQL之间的映射关系，更好的利用表格中的属性，更加关注解码的过程。在后续工作中产生了一系列优秀的模型，其中的代表工作：seq2sql、SQLNet、TypeSQL。</p><p>项目链接：<a href="https://github.com/salesforce/WikiSQL">https://github.com/salesforce/WikiSQL</a></p><h2 id="Spider"><a href="#Spider" class="headerlink" title="Spider"></a>Spider</h2><p>但是WikiSQL也存在着问题，它的每个问题只涉及一个表格，而且也仅支持比较简单的SQL操作，这个不是很符合我们日常生活中的场景。现实生活中存在着医疗、票务、学校、交通等各个领域的数据库，而且每个数据库又有数十甚至上百个表格，表格之间又有着复杂的主外键联系。</p><p>于是，2018年，耶鲁大学的研究人员推出了Spider数据集，这也是目前最复杂的Text-to-SQL数据集。它有以下几个特点：1）领域比较丰富，拥有来自138个领域的200多个数据库，每个数据库平均对应5.1个表格，并且训练集、测试集中出现的数据库不重合。2）SQL语句更为复杂，包含orderBy、union、except、groupBy、intersect、limit、having 关键字，以及嵌套查询等。作者根据SQL语句的复杂程度（关键字个数、嵌套程度）分为了4种难度，值得注意的是，WikiSQL在这个划分下只有EASY难度。Spider相比WikiSQL，对模型的跨领域、生成复杂SQL的能力提出了新的要求，目前的最佳模型也只有60%左右的准确度。</p><p>挑战赛链接：<a href="https://yale-lily.github.io/spider">https://yale-lily.github.io/spider</a></p><p>下面是Hard和Extra Hard的实例：</p><p><img src="https://z3.ax1x.com/2021/09/24/4DO4Mj.png" alt=""></p><center>Spider数据集中Hard以及Extra Hard难度的样例</center><h2 id="中文CSpider"><a href="#中文CSpider" class="headerlink" title="中文CSpider"></a>中文CSpider</h2><p>西湖大学在EMNLP2019上提出了一个中文text-to-sql的数据集CSpider，主要是选择Spider作为源数据集进行了问题的翻译，并利用SyntaxSQLNet作为基线系统进行了测试，同时探索了在中文上产生的一些额外的挑战，包括中文问题对英文数据库的对应问题(question-to-DBmapping)、中文的分词问题以及一些其他的语言现象。</p><p>挑战赛链接：<a href="https://taolusi.github.io/CSpider-explorer/">https://taolusi.github.io/CSpider-explorer/</a></p><h2 id="Sparc"><a href="#Sparc" class="headerlink" title="Sparc"></a>Sparc</h2><p>耶鲁大学的研究团队后续又推出了SParC，即Spider的上下文相关版本。数据库基于Spider，模拟了用户进行数据库查询的过程：用户通过若干条相关的提问最后达到一个最终查询目的。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/2459654518f1e0c6.png" alt=""></p><center>Sparc数据集中的样例</center><p>挑战赛链接：<a href="https://yale-lily.github.io/sparc">https://yale-lily.github.io/sparc</a></p><h2 id="CHASE"><a href="#CHASE" class="headerlink" title="CHASE"></a><strong>CHASE</strong></h2><p>微软亚洲研究院联合北航和西安交大在ACl2021上提出了全新跨领域多轮交互 Text2SQL 中文数据集 CHASE，它有以下特点：</p><ol><li><p><strong>跨领域，</strong>包含 280 个不同领域的数据库，且 train/dev/test 不重复；</p></li><li><p><strong>大规模，</strong>包含 5459 个多轮问题组成的列表，一共 17940 个<query, SQL>二元组；</p></li><li><p><strong>多轮交互，</strong>同一个列表的问题之间会有实体省略等交互现象，类似于 SParc 和 CoSQL；</p></li><li><p><strong>中文数据集，</strong>问题和数据库表名、列名、其中的元素都是中文，相比之下，CSpider 只是将表名、列名字段翻译为中文。</p></li><li><p><strong>标注信息丰富</strong>，除了 query 和 SQL，CHASE 额外标注了（1）<strong>上下文依赖关系</strong>，包括 Coreference 共指、Ellipsis 省略；（2）<strong>模式链接关系</strong>，对于 query 中提到的表名和列名信息进行了标记。</p><p><img src="https://z3.ax1x.com/2021/09/24/4DKqOA.png" alt="CHASE"></p></li></ol><p>挑战赛链接：<a href="https://xjtu-intsoft.github.io/chase/">https://xjtu-intsoft.github.io/chase/</a></p><h1 id="评价方法"><a href="#评价方法" class="headerlink" title="评价方法"></a>评价方法</h1><p>Text-to-SQL的评价方法主要包含两种，一个是精确匹配率（exact match ），另一个是执行正确率（executionaccuracy）。精确匹配指预测得到的SQL与正确的SQL语句在SELECT、WHERE等模块达到字符串完全匹配，即整句匹配；执行正确是指，执行预测得到的SQL语句，数据库能够返回正确答案。目前WikiSQL支持exactmatch和execution accuracy，Spider仅支持exact match。</p><h2 id="模型介绍"><a href="#模型介绍" class="headerlink" title="模型介绍"></a>模型介绍</h2><p>在深度学习的研究背景下，很多研究人员将Text-to-SQL看作一个类似神经机器翻译的任务，主要采取seq2seq的模型框架。<strong>基线模型seq2seq在加入Attention、Copying等机制</strong>后，能够在ATIS、GeoQuery数据集上达到84%的精确匹配，但是在WikiSQL上只能达到23.3%的精确匹配，37.0%的执行正确率；在Spider上则只能达到5～6%的精确匹配。</p><p>究其原因，可以从编码和解码两个角度来看。首先编码方面，自然语言问句与数据库之间需要形成很好的对齐或映射关系，即问题中到底涉及了哪些表格中的哪些实体词，以及问句中的词语触发了哪些选择条件、聚类操作等；另一方面在解码部分，SQL作为一种形式定义的程序语言，本身对语法的要求更严格（关键字顺序固定）以及语义的界限更清晰，失之毫厘差之千里。普通的seq2seq框架并不具备建模这些信息的能力。</p><p>于是，<strong>主流模型的改进与后续工作</strong>主要围绕着以下几个方面展开：通过更强的表示（BERT、XLNet）、更好的结构（GNN）来显式地加强Encoder端的对齐关系及利用结构信息；通过树形结构解码、填槽类解码来减小搜索解空间，以增加SQL语句的正确性；通过中间表示等技术提高SQL语言的抽象性；通过定义新的对齐特征，利用重排序技术，对beamsearch得到的多条候选结果进行正确答案的挑选；以及非常有效的数据增强方法。</p><h3 id="Pointer-Network"><a href="#Pointer-Network" class="headerlink" title="Pointer Network"></a>Pointer Network</h3><p>传统的seq2seq模型的解码器部分所使用的单词表是固定的，即在生成序列中都是从固定的单词表中进行选取。但Text-to-SQL不同于一般的seq2seq任务，它的生成序列中可能出现：a）问句中的单词；b) SQL关键字；c)对应数据库中的表名、列名。</p><p>Pointer Network很好地解决了这一问题，其输出所用到的词表是随输入而变化的。具体做法是利用注意力机制，直接从输入序列中选取单词作为输出。</p><p>在Text-to-SQL任务中，可以考虑把用户的提问以及目标SQL语句可能出现的其他词作为输入序列 (列名单词序列；SQL的关键字表；问题的单词序列），利用PointerNetwork直接从输入序列中选取单词作为输出。在解码器的每一步，与编码器的每一个隐层状态计算注意力分数，取最大值作为当前的输出以及下一步的输入。</p><h3 id="Seq2SQL"><a href="#Seq2SQL" class="headerlink" title="Seq2SQL"></a>Seq2SQL</h3><p>Pointer Network虽然一定程度上解决了问题，但是它并没有利用到SQL语句固有的语法结构。Seq2SQL将生成的SQL语句分为三个部分：聚合操作：（SUM、COUNT、MIN、MAX等）、SELECT：选取列、WHERE：查询条件。每一部分使用不同的方法进行计算。</p><p>SELECT与聚合操作，均采用了注意力机制进行分类。WHERE子句可以利用前面介绍的Pointer Network进行训练，但是对于很多查询来说，WHERE子句的写法并不是唯一的，例如：</p><p>SELECT name FROM insurance WHERE age &gt; 18 AND gender =“male”;</p><p>SELECT name FROM insurance WHERE gender = “male”AND age &gt; 18;</p><p>这可能导致原本正确的输出被判断为错误的。于是作者提出利用强化学习基于查询结果来进行优化。在解码器部分，对可能的输出进行采样，产生若干个SQL语句，每一句表示为y=[y1, y2 … yT]，用打分函数对每一句进行打分：</p><p><img src="https://z3.ax1x.com/2021/09/24/4Dl8JK.png" alt=""></p><h3 id="SQLNet"><a href="#SQLNet" class="headerlink" title="SQLNet"></a>SQLNet</h3><p><img src="https://z3.ax1x.com/2021/09/24/4DlHlF.png" alt=""></p><center>根据SQL Sketch，将Seq2Seq形式的序列生成转化为Seq2SET形式的槽值填充</center><p>该论文首次提出基于草图（sketch）的生成模型。基于草图的方法要完成的测序任务属于固定模式，不需要预测 SQL 语句中的所有内容，即只预测关键内容。（上图 a 中的标有“$”的部分）</p><p>为了解决Seq2SQL使用强化学习效果不明显的问题，SQLNet将SQL语句分成了SELECT和WHERE两个部分，每个部分设置了几个槽位，只需向槽位中填入相应的符号即可。</p><p>SELECT子句部分与Seq2SQL类似，不同地方在于WHERE子句，它使用了一种sequence-to-set（由序列生成集合）机制，用于选取目标SQL语句中的WHERE子句可能出现的列。对于表中的每一列给出一个概率。之后计算出WHERE子句中的条件个数k，然后选取概率最高的前k个列。最后通过注意力机制进行分类得到操作符和条件值。</p><p>基于草图的 Text2SQL 的第一项工作是 SQLNet，它将 Text2SQL 任务转换为六个子任务。这些子任务可预测需要填充的草图中标有“$”的部分。对于 WikiSQL 数据集，基于草图的后续研究也使用类似的任务划分，例如 TypeSQL，MQAN，SQLova，X-SQL等。SQLova 和 X-SQL 引入了预训练的模型BERT，它们效果更好并基本达到 WikiSQL 数据集的极限。</p><h3 id="TypeSQL"><a href="#TypeSQL" class="headerlink" title="TypeSQL"></a>TypeSQL</h3><p><img src="https://z3.ax1x.com/2021/09/24/4D1kmd.png" alt=""></p><center>TypeSQL示意图：显式地赋予每个单词类型</center><p>该模型基于SQLNet，使用模版填充的方法生成SQL语句。为了更好地建模文本中出现的罕见实体和数字，TypeSQL显式地赋予每个单词类型。</p><p>类型识别过程：将问句分割n-gram （n取2到6），并搜索数据库表、列。对于匹配成功的部分赋值column类型赋予数字、日期四种类型：INTEGER、FLOAT、DATE、YEAR。对于命名实体，通过搜索FREEBASE，确定5种类型：PERSON，PLACE，COUNTREY，ORGANIZATION，SPORT。这五种类型包括了大部分实体类型。当可以访问数据库内容时，进一步将匹配到的实体标记为具体列名（而不只是column类型）</p><p>SQLNet为模版中的每一种成分设定了单独的模型；TypeSQL对此进行了改进，对于相似的成分，例如SELECT_COL 和COND_COL以及#COND（条件数），这些信息间有依赖关系，通过合并为单一模型，可以更好建模。TypeSQL使用3个独立模型来预测模版填充值：</p><ul><li><p>MODEL_COL：SELECT_COL，#COND，COND_COL</p></li><li><p>MODEL_AGG：AGG</p></li><li><p>MODEL_OPVAL：OP, COND_VAL</p></li></ul><h3 id="SyntaxSQLNet"><a href="#SyntaxSQLNet" class="headerlink" title="SyntaxSQLNet"></a>SyntaxSQLNet</h3><p>相比于之前decoder输出一段线性的文本，SyntaxSQLNet将解码的过程引入了结构性信息，即解码的对象为SQL语句构成的树结构。通过该技术，模型的精确匹配率提高了14.8%。</p><p><img src="https://z3.ax1x.com/2021/09/24/4D3Vu4.png" alt=""></p><center>SyntaxSQLNet利用SQL的树结构进行解码</center><p>SyntaxSQLNet将SQL语句的预测分解为9个模块，每个模块对应了SQL语句中的一种成分。解码时由预定义的SQL文法确定这9个模块的调用顺序，从而引入结构信息。树的生成顺序为深度优先。分解出的9个模块有：</p><ul><li>IUEN模块：预测INTERCEPT、UNION、EXCEPT、NONE（嵌套查询相关）</li><li>KW模块：预测WHERE、GROUP BY、ORDER BY、SELECT关键字</li><li>COL模块：预测列名</li><li>OP模块：预测&gt;、&lt;、=、LIKE等运算符</li><li>AGG模块：预测MAX、MIN、SUM等聚合函数</li><li>Root/Terminal模块：预测子查询或终结符</li><li>Module模块：预测子查询或终结符</li><li>AND/OR模块：预测条件表达式间的关系</li><li>DESC/ASC/LIMIT模块：预测与ORDERBY相关联的关键字</li><li>HAVING模块：预测与GROUPBY相关的Having从句</li></ul><p>该工作同时提供了一种针对text2sql任务的数据增强方法，生成跨领域、更多样的训练数据。通过该技术，模型的精确匹配率提高了7.5%。</p><p>具体做法为：对SPIDER中的每条数据，将值和列名信息除去，得到一个模板；对处理后的SQL模版进行聚类，通过规则去除比较简单的模板，并依据模板出现的频率，挑选50个复杂SQL模板；人工核对SQL-问句对，确保SQL模板中每个槽在问句中都有对应类型的信息。</p><p>得到一一对应的模板后，应用于WikiSQL数据库：首先随机挑选10个模板，然后从库中选择相同类型的列，最后用列名和值填充SQL模板和问句模板。通过该方法，作者最终在18000的WikiSQL数据库上得到了新的98000组训练数据，同时在训练的时候也利用了WikiSQL数据集原有的训练数据。</p><h3 id="IRNet"><a href="#IRNet" class="headerlink" title="IRNet"></a>IRNet</h3><p>与SyntaxSQLNet类似，IRNet定义了一系列的CFG文法，将SQL转发为语法树结构。可以将其看作一种自然语言与SQL语句间的中间表示（作者称之为SemQL），整个parsing的过程也是针对SemQL进行的。如图所示：</p><p><img src="https://z3.ax1x.com/2021/09/24/4DtH4s.png" alt=""></p><center>IRNet利用定义的CFG将SQL转化为更抽象的SemQL</center><p>作者另一部分的改进主要在schemelinking，即如何找到问题中所提到的表格与列。他将问题中可能出现的实体分为3类：表格名、列名、表中的值。根据3类实体的不同，具体做法分为：a）表格名和列名：以n-gram的形式枚举问题中的span，然后和表格名、列名进行匹配。可以看到下图中的Question中对应的单词有的被标成了Column或者Table。b) 表中的值：将问题中以引号为开头结尾的span，送给ConceptNet进行查询，再将返回结果中的 ‘is a type of’/‘related terms’关系的词与列名进行匹配。</p><p><img src="https://z3.ax1x.com/2021/09/24/4DtL3q.png" alt=""></p><center>IRNet的模型结构</center><h3 id="Global-GNN"><a href="#Global-GNN" class="headerlink" title="Global-GNN"></a>Global-GNN</h3><p>为了更好的利用关系型数据库的结构信息，BenBogin等研究人员提出使用图网络来建模表格名和列名。如下图所示：圆圈加粗的结点代表表格，不加粗的结点代表列名；双向边代表表格和列名的从属关系；红虚边和蓝虚边代表主外键关系。橙色节点代表与问题有关的结果，淡色为无关。</p><p><img src="https://z3.ax1x.com/2021/09/24/4DtXvV.png" alt=""></p><center>左：模型输入样例（问题、表名、列名、主外键关系）</center><center>右：用于建模表格结构信息的GNN（包括两类节点，三类边）</center><p>除此之外，该团队还提出了一种基于全局信息重排序的做法。首先先看下面这个例子，我们不知道name到底指向的是singer还是song，但是我们可以观察到nation只在singer中出现，所以应该是singer.name。这样做globalreasoning，就能减小歧义性。</p><p><img src="https://z3.ax1x.com/2021/09/24/4DtxDU.png" alt=""></p><center>根据全局信息，我们可以通过country来确定第一个name为singer.name而不是song.name（name和nation同属于一个实体）</center><p><img src="https://z3.ax1x.com/2021/09/24/4DNl8I.png" alt="GlobalGNN的工作流程"></p><center>GlobalGNN的工作流程</center><h3 id="RAT-SQL"><a href="#RAT-SQL" class="headerlink" title="RAT-SQL"></a>RAT-SQL</h3><p>该工作可以看作图网络GNN的后续工作，作者在Table、Column、Question三者之间定义了更多的边（共33种），是目前榜单上的最强模型。</p><p><img src="https://z3.ax1x.com/2021/09/24/4DNTsK.png" alt="RAT-SQL的样例：引入了Question节点"></p><center>RAT-SQL的样例：引入了Question节点</center><p>Global-GNN只建模了Table和Column的联系，RAT-SQL在此基础上又在图中加入Question的节点，而且利用字符串匹配的方法丰富了边的类型。</p><center>表2 RAT SQL新定义的边关系</center><p><img src="https://z3.ax1x.com/2021/09/24/4DU9L8.png" alt=""></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>目前，SQL生成任务因其实用的应用场景，引起了学术界和工业界的广泛关注。目前大家的做法也是百花齐放：中间表示、树形解码、图网络建模Quetion和数据库间的关系、重排序、数据增强。但目前的模型，还不能很好解决复杂的操作，例如IRNet在Hard和ExtraHard的准确率也仅为48.1%和25.3%。期待后面能有更加有效、简洁、优雅的工作出现。</p><center>表3 模型效果比较（Acc ex: 执行结果；Acc em: SQL完全匹配；Acc qu: SQL无序匹配）</center><p><img src="https://z3.ax1x.com/2021/09/24/4DUEJs.png" alt=""></p><h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><ol><li><a href="https://www.jiqizhixin.com/articles/2019-12-27-11">一文了解Text-to-SQL</a></li><li><a href="https://zhuanlan.zhihu.com/p/384293232">ACL 2021｜CHASE:首个跨领域多轮Text2SQL中文数据集</a></li></ol>]]></content>
      
      
      <categories>
          
          <category> Text2SQL </category>
          
          <category> 综述 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Text2SQL </tag>
            
            <tag> NLP综述 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BPE分词、LabelSmoothing标签平滑正则化</title>
      <link href="/2021/09/16/BPE%E5%88%86%E8%AF%8D%E3%80%81LabelSmoothing%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91%E6%AD%A3%E5%88%99%E5%8C%96/"/>
      <url>/2021/09/16/BPE%E5%88%86%E8%AF%8D%E3%80%81LabelSmoothing%E6%A0%87%E7%AD%BE%E5%B9%B3%E6%BB%91%E6%AD%A3%E5%88%99%E5%8C%96/</url>
      
        <content type="html"><![CDATA[<h1 id="BPE-Byte-Pair-Encoding-分词"><a href="#BPE-Byte-Pair-Encoding-分词" class="headerlink" title="BPE(Byte Pair Encoding)分词"></a>BPE(Byte Pair Encoding)分词</h1><p>BPE是一种根据字节对进行编码的算法。主要目的是为了数据压缩，算法描述为字符串里频率最常见的一对字符被一个没有在这个字符中出现的字符代替的层层迭代过程。<strong>基本思路是将使用最频繁的字节用一个新的字节组合代替</strong>，比如用字符的n-gram替换各个字符。例如，假设(‘A’, ‘B’) 经常顺序出现，则用一个新的标志’AB’来代替它们。</p><p>Transformer NLP 预训练模型都通过 embedding 词典来表征词义，当遇见没见过的词的时候，以前是用”&lt; u nk&gt;”代替，这样会造成对新事物(新词、网络词、简写词)理解能力很差，BPE就是来解决这个问题的。英文中会有词根和造词现象例如: “greenhand” 如果你的词典中没有这个词，那么就可以把它拆成 <strong>“green”,“hand”</strong>两个词，这里green 向量会跟发芽一类的词相近有新生的意思，hand有操作、手的意思那么就不难推测出greenhand是新手。这个过程中会参考一个词典，这个词典从上往下是一个个词对，对应的顺序就是它出现的频率，越往上的词越高频率。对应中文相当于分词了。</p><p>2016年，Sennrich提出采用分词算法（word segmentation）构建BPE，并将其应用于机器翻译任务中。论文提出的基本思想是，给定语料库，初始词汇库仅包含所有的单个字符。然后，模型不断地将出现频率最高的n-gram pair作为新的n-gram加入到词汇库中，直到词汇库的大小达到我们所设定的某个目标为止。该算法在论文：<a href="https://arxiv.org/abs/1508.07909">https://arxiv.org/abs/1508.07909</a> Neural Machine Translation of Rare Words with Subword Units详细介绍</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/06b962ac09ba0533.jpg" alt="算法"></p><h2 id="训练过程："><a href="#训练过程：" class="headerlink" title="训练过程："></a>训练过程：</h2><p>对于使用子词作为基本单位进行训练的神经机器翻译模型，训练的第一步就是根据语料生成bpe的code资源，以英文为例，该资源会将训练语料以字符为单位进行拆分，按照字符对进行组合，并对所有组合的结果根据出现的频率进行排序，出现频次越高的排名越靠前，排在第一位的是出现频率最高的子词。如图所示：e &lt;/w&gt;为出现频率最高的子词，其中&lt;/w&gt;表示这个e是作为单词结尾的字符。训练过程结束，会生成codec文件。如下图所示：</p><p><img src="https://tva1.sinaimg.cn/large/005Nrl8dly8guircvvxgtj604z0e2mxn02.jpg" alt=""></p><h2 id="解码过程"><a href="#解码过程" class="headerlink" title="解码过程"></a>解码过程</h2><p>以单词“where”为例，首先按照字符拆分开，然后查找codec文件，逐对合并，优先合并频率靠前的字符对。85 319 9 15 表示在该字符对在codec文件中的评率排名。</p><p><img src="https://tva4.sinaimg.cn/large/005Nrl8dly8guirhflkxnj30pd0x0gn5.jpg" alt=""></p><p>最终where&lt;/w&gt;可以在codec文件中被找到，因此where的bpe分词结果为where&lt;/w&gt;，对于其他并不能像where一样能在codec文件中找到整个词的词来说，bpe分词结果以最终查询结束时的分词结果为准。</p><h2 id="sentencepiece工具包"><a href="#sentencepiece工具包" class="headerlink" title="sentencepiece工具包"></a>sentencepiece工具包</h2><p>sentencepiece是一个google开源的自然语言处理工具包，支持bpe、unigram等多种分词方法。其优势在于：bpe、unigram等方法均假设输入文本是已经切分好的，只有这样bpe才能统计词频（通常直接通过空格切分）。但问题是，汉语、日语等语言的字与字之间并没有空格分隔。<strong>sentencepiece提出，可以将所有字符编码成Unicode码（包括空格），通过训练直接将原始文本（未切分）变为分词后的文本，从而避免了跨语言的问题。</strong></p><p>实验表明，当英文的词表大小设定为32000时，可以覆盖数据集中词频为10以上的bpe组合；中文词表大小设定为32000时，覆盖了数据集中词频为21以上的bpe组合。进一步扩大词表，会导致词表过大，含有的非常用词增多，模型的训练参数也会增多。因此，我们设定中英文词表大小均为32000。根据<a href="https://github.com/google/sentencepiece">sentencepiece</a>代码说明，<strong>character_coverage</strong>这一参数在字符集较大的中文分词中设置为0.995，在字符集较小的英文分词中设置为1。</p><h1 id="LabelSmoothing"><a href="#LabelSmoothing" class="headerlink" title="LabelSmoothing"></a>LabelSmoothing</h1><p>Label Smoothing 是一种正则化的方法，对标签平滑化处理以防止过拟合。</p><p>对于分类问题，特别是多分类问题，常常把向量转换成one-hot-vector（独热向量），选择概率最大的作为我们的预测标签。然而在实际过程中，对于损失函数，我们需要用预测概率去拟合真实概率，而拟合one-hot的真实概率函数会带来两个问题：</p><ul><li><strong>无法保证模型的泛化能力，容易造成过拟合；</strong></li><li><strong>全概率和0概率鼓励所属类别和其他类别之间的差距尽可能加大，而由梯度有界可知，这种情况很难适应。会造成模型过于相信预测的类别</strong></li></ul><p>交叉熵损失函数的实际是在最小化预测概率与真实标签概率的交叉熵：</p><script type="math/tex; mode=display">L=-\sum_iy_ilogp_i</script><p>如果分类准确，交叉熵损失函数的结果是0（即上式中p和y一致的情况），否则交叉熵为无穷大。也就是说交叉熵对分类正确给的是最大激励。换句话说，对于标注数据来说，这个时候我们认为其标注结果是准确的(对于真实标签概率的取值要么是1，要么是0，表征我们已知样本属于某一类别的概率是为1的确定事件,属于其他类别的概率则均为0)。但是，对于一个由多人标注的数据集，不同人标注的准则可能不同，每个人的标注也可能会有一些错误。模型对标签的过分信任，可能会造成过拟合。那么这时候，使用交叉熵损失函数作为目标函数并不一定是最优的。</p><p><strong>label smoothing的原理就是为损失函数增强其他标签的损失函数值。</strong>类似于其为非标签的标签增加了一定概率的可选择性。假设标签光滑化处理标签真实概率的表达式为：</p><script type="math/tex; mode=display">p = (1-\epsilon) y + \frac {\epsilon}{K}</script><p>原理：对于以Dirac函数分布的真实标签，我们将它变成分为两部分获得（替换）。</p><ol><li>第一部分：将原本Dirac分布的标签变量替换为(1 - ϵ)的Dirac函数；</li><li>第二部分：以概率 ϵ ，将其替换为在u(k) 分布中的随机变量（u（k）是类别分之一即$\frac {1}{K}$）</li></ol><p>当$y$为真实标签的时候，概率取值为$p = (1-\epsilon) + \frac {\epsilon}{K}$，否则取概率 $\frac {\epsilon}{K}$ 。</p><blockquote><p>如：假设存在标签[0, 1, 2]</p><p>取$\epsilon=0.1$，当某个样本真实标签为$y = 0$的时候，损失函数值计算如下：</p><script type="math/tex; mode=display">L=-0.93logp_0 - 0.03 logp_1 - 0.03 logp_2</script><p>其中$p_0, p_1, p_1$ 表示该模型将该标签分别预测为这三个标签的概率，$p_0+p_1+p_1=1$</p><p>不进行标签光滑化处理的话，该样本损失函数值计算如下：</p><script type="math/tex; mode=display">L=-logp_0</script></blockquote><p>标签在某种程度上软化了，增加了模型的<strong>泛化</strong>能力，一定程度上防止<strong>过拟合</strong>。</p>]]></content>
      
      
      <categories>
          
          <category> NLP Trick </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer详解</title>
      <link href="/2021/09/13/Transformer%E8%AF%A6%E8%A7%A3/"/>
      <url>/2021/09/13/Transformer%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="0-Transformer直观认识"><a href="#0-Transformer直观认识" class="headerlink" title="0. Transformer直观认识"></a>0. Transformer直观认识</h1><p>Transformer 和 LSTM 的最大区别，就是 LSTM 的训练是迭代的、串行的，必须要等当前字处理完，才可以处理下一个字。而 Transformer 的训练时<strong>并行</strong>的，即所有<strong>字</strong>是同时训练的，这样就大大增加了计算效率。Transformer 使用了<strong>位置嵌入</strong> (Positional Encoding) 来理解语言的顺序，使用自注意力机制（Self Attention Mechanism）和全连接层进行计算。</p><p>Transformer 模型主要分为两大部分，分别是 <strong>Encoder</strong> 和 <strong>Decoder</strong>。<strong>Encoder</strong> 负责把输入（语言序列）隐射成<strong>隐藏层</strong>，然后解码器再把隐藏层映射为自然语言序列。</p><p><img src="https://s1.ax1x.com/2020/04/25/JyCLlQ.png#shadow" alt="encoder"></p><p>上图为 Transformer Encoder Block 结构图，注意：下面的内容标题编号分别对应着图中 1,2,3,4 个方框的序号</p><h1 id="1-Positional-Encoding"><a href="#1-Positional-Encoding" class="headerlink" title="1. Positional Encoding"></a>1. Positional Encoding</h1><p>如下图所示，Transformer模型对每个输入的词向量都加上了一个位置向量。这些向量有助于确定每个单词的位置特征，或者句子中不同单词之间的距离特征。词向量加上位置向量背后的直觉是：将这些表示位置的向量添加到词向量中，得到的新向量，可以为模型提供更多有意义的信息，比如词的位置，词之间的距离等。</p><p><img src="https://datawhalechina.github.io/learn-nlp-with-transformers/%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/pictures/2-position.png" alt="位置向量"></p><p>现在定义一个<strong>位置嵌入</strong>的概念，也就是 Positional Encoding，位置嵌入的维度为 <code>[max_sequence_length, embedding_dimension]</code>, 位置嵌入的维度与词向量的维度是相同的，都是 <code>embedding_dimension</code>。<code>max_sequence_length</code> 属于超参数，指的是限定每个句子最长由多少个词构成。</p><p>我们一般以<strong>字</strong>为单位训练 Transformer 模型。首先初始化字编码的大小为 <code>[vocab_size, embedding_dimension]</code>，<code>vocab_size</code> 为字库中所有字的数量，<code>embedding_dimension</code> 为字向量的维度，对应到 PyTorch 中，其实就是 <code>nn.Embedding(vocab_size, embedding_dimension)</code>。</p><p>原始论文中给出的位置编码信息的向量的设计表达式为：</p><script type="math/tex; mode=display">PE(pos,2i)=sin(pos/10000^{2i/d_{model}})\\PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})</script><p>上面表达式中的$pos$代表词的位置,取值范围是[0,max_sequence_length]，$d_{model}$代表位置向量的维度，i 代表位置$d_{model}$维位置向量第i维,指字向量的维度序号，取值范围[0,embedding_dimension/2]。</p><p>上面有 sin 和 cos 一组公式，也就是对应着 embedding_dimension 维度的一组奇数和偶数的序号的维度，分别用上面的 sin 和 cos 函数做处理，从而产生不同的周期性变化，而位置嵌入在 embedding_dimension维度上随着维度序号增大，周期变化会越来越慢，最终产生一种包含位置信息的纹理，就像论文原文中第六页讲的，位置嵌入函数的周期从 2π 到 10000∗2π 变化，而每一个位置在 embedding_dimension维度上都会得到不同周期的 sin 和 cos 函数的取值组合，从而产生独一的纹理位置信息，最终使得模型学到位置之间的依赖关系和自然语言的时序特性。</p><p>在下图中，我们画出了一种位置向量在第4、5、6、7维度、不同位置的的数值大小。横坐标表示位置下标，纵坐标表示数值大小。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/434e6332bf04e94c.png" alt="位置编码"></p><p>如果不理解这里为何这么设计，可以看这篇文章 <a href="https://wmathor.com/index.php/archives/1453/">Transformer 中的 Positional Encoding</a></p><h1 id="2-Self-Attention-Mechanism"><a href="#2-Self-Attention-Mechanism" class="headerlink" title="2. Self Attention Mechanism"></a>2. Self Attention Mechanism</h1><p>对于输入的句子$ X $，通过 WordEmbedding 得到该句子中每个字的字向量，同时通过 Positional Encoding 得到所有字的位置向量，将其相加（维度相同，可以直接相加），得到该字真正的向量表示。第 $t $个字的向量记作$ x_t $。</p><p>先通过一个简单的例子来理解一下：什么是“self-attention自注意力机制”？假设一句话包含两个单词：Thinking Machines。自注意力的一种理解是：Thinking-Thinking，Thinking-Machines，Machines-Thinking，Machines-Machines，共$2^2$种两两attention。那么具体如何计算呢？假设Thinking、Machines这两个单词经过词向量算法得到向量是$X_1$,$ X_2$：</p><p>下面，我们将self-attention计算的6个步骤进行可视化。</p><p>第1步：对输入编码器的词向量进行线性变换得到：Query向量: $q_1,q_2$，Key向量: $k_1, k_2$，Value向量: $v_1, v_2$。这3个向量是词向量分别和3个参数矩阵相乘得到的，而这个矩阵也是是模型要学习的参数。attention计算的逻辑常常可以描述为：query和key计算相关或者叫attention得分，然后根据attention得分对value进行加权求和。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/d46ccb2c497c2b0e.png" alt="qkv"></p><p>第2步：计算Attention Score（注意力分数）。假设我们现在计算第一个词<em>Thinking</em> 的Attention Score（注意力分数），需要根据<em>Thinking</em> 对应的词向量，对句子中的其他词向量都计算一个分数。这些分数决定了我们在编码<em>Thinking</em>这个词时，需要对句子中其他位置的词向量的权重。</p><p>Attention score是根据”<em>Thinking</em>“ 对应的 Query 向量和其他位置的每个词的 Key 向量进行点积得到的。Thinking的第一个Attention Score就是$q_1$和$k_1$的内积，第二个分数就是$q_1$和$k_2$的点积。这个计算过程在下图中进行了展示，下图里的具体得分数据是为了表达方便而自定义的。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/9b1c5a3395ec4f37.png" alt="think"></p><p>第3步：把每个分数除以 $\sqrt{d_k}，d_{k}$是Key向量的维度。你也可以除以其他数，除以一个数是为了在反向传播时，求梯度时<strong>更加稳定</strong>。</p><p>第4步：接着把这些分数经过一个Softmax函数，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于1， 如下图所示。 这些分数决定了Thinking词向量，对其他所有位置的词向量分别有多少的注意力。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/4937826683ee6f3c.png" alt="think2"></p><p>第5步：得到每个词向量的分数后，将分数分别与对应的Value向量相乘。这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的。</p><p>第6步：把第5步得到的Value向量相加，就得到了Self Attention在当前位置（这里的例子是第1个位置）对应的输出。</p><p>最后，在下图展示了 对第一个位置词向量计算Self Attention 的全过程。最终得到的当前位置（这里的例子是第一个位置）词向量会继续输入到前馈神经网络。注意：上面的6个步骤每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention的计算过程是使用矩阵快速计算的，一次就得到所有位置的输出向量。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/7884339f34ef4ba6.png" alt="sum"></p><h2 id="self-attention矩阵运算"><a href="#self-attention矩阵运算" class="headerlink" title="self-attention矩阵运算"></a>self-attention矩阵运算</h2><p>将self-attention计算6个步骤中的向量放一起，比如X=[x_1;x_2]X=[x1;x2]，便可以进行矩阵计算啦。下面，依旧按步骤展示self-attention的矩阵计算方法。</p><script type="math/tex; mode=display">X=[X1;X2]\\Q=XW^Q,K=XW^K,V=XW^V\\Z=softmax(\frac {QK^T}{\sqrt(d_k)})V</script><p>第1步：计算 Query，Key，Value 的矩阵。首先，我们把所有词向量放到一个矩阵X中，然后分别和3个权重矩阵$W^Q,W^K,W^V$相乘，得到 Q，K，V 矩阵。矩阵X中的每一行，表示句子中的每一个词的词向量。Q，K，V 矩阵中的每一行表示 Query向量，Key向量，Value 向量，向量维度是$d_k$。</p><p><img src="https://z3.ax1x.com/2021/04/20/c7wF9x.png#shadow" alt=""></p><p>第2步：由于我们使用了矩阵来计算，我们可以把上面的第 2 步到第 6 步压缩为一步，直接得到 Self Attention 的输出。</p><p><img src="https://z3.ax1x.com/2021/04/20/c7wk36.png#shadow" alt=""></p><h2 id="Multi-Head-Attention（多头注意力机制）"><a href="#Multi-Head-Attention（多头注意力机制）" class="headerlink" title="Multi-Head Attention（多头注意力机制）"></a>Multi-Head Attention（多头注意力机制）</h2><p>Transformer 的论文通过增加多头注意力机制（一组注意力称为一个 attention head），进一步完善了Self-Attention。这种机制从如下两个方面增强了attention层的能力：</p><ul><li><strong>它扩展了模型关注不同位置的能力。</strong>在上面的例子中，第一个位置的输出z_1z1包含了句子中其他每个位置的很小一部分信息，但z_1z1仅仅是单个向量，所以可能仅由第1个位置的信息主导了。而当我们翻译句子：<code>The animal didn’t cross the street because it was too tired</code>时，我们不仅希望模型关注到”it”本身，还希望模型关注到”The”和“animal”，甚至关注到”tired”。这时，多头注意力机制会有帮助。</li><li><strong>多头注意力机制赋予attention层多个“子表示空间”。</strong>下面我们会看到，多头注意力机制会有多组$W^Q, W^K,W^V$ 的权重矩阵（在 Transformer 的论文中，使用了 8 组注意力),因此可以将$X$变换到更多种子空间进行表示。接下来我们也使用8组注意力头（attention heads））。每一组注意力的权重矩阵都是随机初始化的，但经过训练之后，每一组注意力的权重$W^Q,W^K,W^V$ 可以把输入的向量映射到一个对应的”子表示空间“。</li></ul><p><img src="https://z3.ax1x.com/2021/04/20/c7wEjO.png#shadow" alt="多头注意力机制"></p><p>在多头注意力机制中，我们为每组注意力设定单独的 WQ, WK, WV 参数矩阵。将输入X和每组注意力的WQ, WK, WV 相乘，得到8组 Q, K, V 矩阵。</p><p>接着，我们把每组 K, Q, V 计算得到每组的 Z 矩阵，就得到8个Z矩阵。</p><p><img src="https://z3.ax1x.com/2021/04/20/c7weDe.png#shadow" alt="8个"></p><p>由于前馈神经网络层接收的是 1 个矩阵（其中每行的向量表示一个词），而不是 8 个矩阵，所以我们直接把8个子矩阵拼接起来得到一个大的矩阵，然后和另一个权重矩阵$W^O$相乘做一次变换，映射到前馈神经网络层所需要的维度。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/fa037e4a516dd613.webp" alt=""></p><p>以上就是多头注意力的全部内容。最后将所有内容放到一张图中：</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/a9e5fec9fa653f24.webp" alt="总的多头"></p><h2 id="Attention代码"><a href="#Attention代码" class="headerlink" title="Attention代码"></a>Attention代码</h2><p>下面的代码实现中，张量的第1维是 batch size，第 2 维是句子长度。代码中进行了详细注释和说明。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MultiheadAttention</span>(<span class="params">nn.Module</span>):</span></span><br><span class="line">    <span class="comment"># n_heads：多头注意力的数量</span></span><br><span class="line">    <span class="comment"># hid_dim：每个词输出的向量维度</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self, hid_dim, n_heads, dropout</span>):</span></span><br><span class="line">        <span class="built_in">super</span>(MultiheadAttention, self).__init__()</span><br><span class="line">        self.hid_dim = hid_dim</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 强制 hid_dim 必须整除 h</span></span><br><span class="line">        <span class="keyword">assert</span> hid_dim % n_heads == <span class="number">0</span></span><br><span class="line">        <span class="comment"># 定义 W_q 矩阵</span></span><br><span class="line">        self.w_q = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        <span class="comment"># 定义 W_k 矩阵</span></span><br><span class="line">        self.w_k = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        <span class="comment"># 定义 W_v 矩阵</span></span><br><span class="line">        self.w_v = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.fc = nn.Linear(hid_dim, hid_dim)</span><br><span class="line">        self.do = nn.Dropout(dropout)</span><br><span class="line">        <span class="comment"># 缩放</span></span><br><span class="line">        self.scale = torch.sqrt(torch.FloatTensor([hid_dim // n_heads]))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span>(<span class="params">self, query, key, value, mask=<span class="literal">None</span></span>):</span></span><br><span class="line">        <span class="comment"># 注意 Q，K，V的在句子长度这一个维度的数值可以一样，可以不一样。</span></span><br><span class="line">        <span class="comment"># K: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        <span class="comment"># V: [64,10,300], 假设batch_size 为 64，有 10 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        <span class="comment"># Q: [64,12,300], 假设batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">        bsz = query.shape[<span class="number">0</span>]</span><br><span class="line">        Q = self.w_q(query)</span><br><span class="line">        K = self.w_k(key)</span><br><span class="line">        V = self.w_v(value)</span><br><span class="line">        <span class="comment"># 这里把 K Q V 矩阵拆分为多组注意力</span></span><br><span class="line">        <span class="comment"># 最后一维就是是用 self.hid_dim // self.n_heads 来得到的，表示每组注意力的向量长度, 每个 head 的向量长度是：300/6=50</span></span><br><span class="line">        <span class="comment"># 64 表示 batch size，6 表示有 6组注意力，10 表示有 10 词，50 表示每组注意力的词的向量长度</span></span><br><span class="line">        <span class="comment"># K: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span></span><br><span class="line">        <span class="comment"># V: [64,10,300] 拆分多组注意力 -&gt; [64,10,6,50] 转置得到 -&gt; [64,6,10,50]</span></span><br><span class="line">        <span class="comment"># Q: [64,12,300] 拆分多组注意力 -&gt; [64,12,6,50] 转置得到 -&gt; [64,6,12,50]</span></span><br><span class="line">        <span class="comment"># 转置是为了把注意力的数量 6 放到前面，把 10 和 50 放到后面，方便下面计算</span></span><br><span class="line">        Q = Q.view(bsz, -<span class="number">1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        K = K.view(bsz, -<span class="number">1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">        V = V.view(bsz, -<span class="number">1</span>, self.n_heads, self.hid_dim //</span><br><span class="line">                   self.n_heads).permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第 1 步：Q 乘以 K的转置，除以scale</span></span><br><span class="line">        <span class="comment"># [64,6,12,50] * [64,6,50,10] = [64,6,12,10]</span></span><br><span class="line">        <span class="comment"># attention：[64,6,12,10]</span></span><br><span class="line">        attention = torch.matmul(Q, K.permute(<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>)) / self.scale</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果 mask 不为空，那么就把 mask 为 0 的位置的 attention 分数设置为 -1e10，这里用“0”来指示哪些位置的词向量不能被attention到，比如padding位置，当然也可以用“1”或者其他数字来指示，主要设计下面2行代码的改动。</span></span><br><span class="line">        <span class="keyword">if</span> mask <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            attention = attention.masked_fill(mask == <span class="number">0</span>, -<span class="number">1e10</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第 2 步：计算上一步结果的 softmax，再经过 dropout，得到 attention。</span></span><br><span class="line">        <span class="comment"># 注意，这里是对最后一维做 softmax，也就是在输入序列的维度做 softmax</span></span><br><span class="line">        <span class="comment"># attention: [64,6,12,10]</span></span><br><span class="line">        attention = self.do(torch.softmax(attention, dim=-<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 第三步，attention结果与V相乘，得到多头注意力的结果</span></span><br><span class="line">        <span class="comment"># [64,6,12,10] * [64,6,10,50] = [64,6,12,50]</span></span><br><span class="line">        <span class="comment"># x: [64,6,12,50]</span></span><br><span class="line">        x = torch.matmul(attention, V)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 因为 query 有 12 个词，所以把 12 放到前面，把 50 和 6 放到后面，方便下面拼接多组的结果</span></span><br><span class="line">        <span class="comment"># x: [64,6,12,50] 转置-&gt; [64,12,6,50]</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>).contiguous()</span><br><span class="line">        <span class="comment"># 这里的矩阵转换就是：把多组注意力的结果拼接起来</span></span><br><span class="line">        <span class="comment"># 最终结果就是 [64,12,300]</span></span><br><span class="line">        <span class="comment"># x: [64,12,6,50] -&gt; [64,12,300]</span></span><br><span class="line">        x = x.view(bsz, -<span class="number">1</span>, self.n_heads * (self.hid_dim // self.n_heads))</span><br><span class="line">        x = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># batch_size 为 64，有 12 个词，每个词的 Query 向量是 300 维</span></span><br><span class="line">query = torch.rand(<span class="number">64</span>, <span class="number">12</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># batch_size 为 64，有 12 个词，每个词的 Key 向量是 300 维</span></span><br><span class="line">key = torch.rand(<span class="number">64</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line"><span class="comment"># batch_size 为 64，有 10 个词，每个词的 Value 向量是 300 维</span></span><br><span class="line">value = torch.rand(<span class="number">64</span>, <span class="number">10</span>, <span class="number">300</span>)</span><br><span class="line">attention = MultiheadAttention(hid_dim=<span class="number">300</span>, n_heads=<span class="number">6</span>, dropout=<span class="number">0.1</span>)</span><br><span class="line">output = attention(query, key, value)</span><br><span class="line"><span class="comment">## output: torch.Size([64, 12, 300])</span></span><br><span class="line"><span class="built_in">print</span>(output.shape)</span><br></pre></td></tr></table></figure><h2 id="Padding-Mask"><a href="#Padding-Mask" class="headerlink" title="Padding Mask"></a>Padding Mask</h2><p><img src="https://s1.ax1x.com/2020/04/25/Jy5rt0.png#shadow" alt="pad"></p><p>上面 Self Attention 的计算过程中，我们通常使用 <strong>mini-batch</strong> 来计算，也就是一次计算多句话，即 $X$ 的维度是 <code>[batch_size, sequence_length]</code>，sequence_length是句长，而一个 mini-batch 是由多个不等长的句子组成的，我们需要按照这个 mini-batch 中最大的句长对剩余的句子进行补齐，一般用 0 进行填充，这个过程叫做 padding。</p><p>但这时在进行 softmax 就会产生问题。回顾 softmax 函数 $σ(z_i)=\frac {e^{z_i}}{∑_{j=1}^Ke^z_j}$，$e^0$ 是 1，是有值的，这样的话 softmax 中被 padding 的部分就参与了运算，相当于让无效的部分参与了运算，这可能会产生很大的隐患。因此需要做一个 mask 操作，让这些无效的区域不参与运算，一般是给无效区域加一个很大的负数偏置，即</p><script type="math/tex; mode=display">Z_{illegal}=Z_{illegal}+bias_{illegal}\\bias_{illegal}→−∞</script><h1 id="3-残差连接和Layer-Normalization"><a href="#3-残差连接和Layer-Normalization" class="headerlink" title="3.残差连接和Layer Normalization"></a>3.残差连接和Layer Normalization</h1><p>到目前为止，我们计算得到了self-attention的输出向量。而单层encoder里后续还有两个重要的操作：残差连接、标准化。</p><p>编码器的每个子层（Self Attention 层和 FFNN）都有一个残差连接和层标准化（layer-normalization），如下图所示。</p><p><img src="https://tva1.sinaimg.cn/large/005Nrl8dly8gueta3ike3j60ii0dfwfn02.jpg" alt="resnet"></p><h2 id="残差连接"><a href="#残差连接" class="headerlink" title="残差连接"></a>残差连接</h2><p>我们在上一步得到了经过 self-attention 加权之后输出，也就是 Self-Attention(Q, K, V)，然后把他们加起来做残差连接。</p><script type="math/tex; mode=display">X_{embedding}+Self-Attention(Q, K, V)</script><p>关于残差连接详细的解释可以看这篇文章<a href="https://zhuanlan.zhihu.com/p/42833949">【模型解读】resnet中的残差连接，你确定真的看懂了？ - 知乎 (zhihu.com)</a></p><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p>Layer Normalization 的作用是把神经网络中隐藏层归一为标准正态分布，也就是 $i.i.d$ 独立同分布，以起到加快训练速度，加速收敛的作用。</p><script type="math/tex; mode=display">μ_j=\frac 1m∑_{i=1}^mx_{ij}</script><p>上式以矩阵的列（column）为单位求均值；</p><script type="math/tex; mode=display">σ_j^2=\frac 1m∑_{i=1}^m(x_{ij}−μ_j)^2</script><p>上式以矩阵的列（column）为单位求方差</p><script type="math/tex; mode=display">LayerNorm(x)=\frac {x_{ij}−μ_j}{\sqrt{σ_j^2+ϵ}}</script><p>然后用<strong>每一列</strong>的<strong>每一个元素</strong>减去<strong>这列的均值</strong>，再除以<strong>这列的标准差</strong>，从而得到归一化后的数值，加 ϵ 是为了防止分母为 0。</p><p><img src="https://z3.ax1x.com/2021/04/20/c7wtbQ.png#shadow" alt="层归一化"></p><p>将 Self-Attention 层的层标准化（layer-normalization）和涉及的向量计算细节都进行可视化，如下所示：</p><p><img src="https://z3.ax1x.com/2021/04/20/c7wD2V.png#shadow" alt="标准化"></p><p>编码器和和解码器的子层里面都有层标准化（layer-normalization）。假设一个 Transformer 是由 2 层编码器和两层解码器组成的，将全部内部细节展示起来如下图所示。</p><p><img src="https://tva2.sinaimg.cn/large/005Nrl8dly8guetrkfh1cj60u00h2ad402.jpg" alt="transformer示意图"></p><h1 id="4-Transformer-Encoder整体架构"><a href="#4-Transformer-Encoder整体架构" class="headerlink" title="4.Transformer Encoder整体架构"></a>4.Transformer Encoder整体架构</h1><p>经过上面 3 个步骤，我们已经基本了解了 <strong>Encoder</strong> 的主要构成部分，下面我们用公式把一个 Encoder block 的计算过程整理一下：</p><p>1). 字向量与位置编码</p><script type="math/tex; mode=display">X=Embedding-Lookup(X)+Positional-Encoding</script><p>2). 自注意力机制</p><script type="math/tex; mode=display">Q=Linear_q(X)=XW_Q\\K=Lineark(X)=XW_K\\V=Linearv(X)=XW_V\\X_{attention}=Self-Attention(Q,K,V)</script><p>3). self-attention 残差连接与 Layer Normalization</p><script type="math/tex; mode=display">X_{attention}=X+X_{attention}\\X_{attention}=LayerNorm(X_{attention})</script><p>4). 下面进行 Encoder block 结构图中的<strong>第 4 部分</strong>，也就是 FeedForward，其实就是两层线性映射并用激活函数激活，比如说 ReLU</p><script type="math/tex; mode=display">X_{hidden}=Linear(ReLU(Linear(X_{attention})))</script><p>5). FeedForward 残差连接与 Layer Normalization</p><script type="math/tex; mode=display">X_{hidden}=X_{attention}+X_{hidden}\\X_{hidden}=LayerNorm(X_{hidden})</script><p>其中</p><script type="math/tex; mode=display">X_{hidden}∈\mathbb{R}^{batch_size ∗ seq_len. ∗ embed_dim}</script><h1 id="5-Transformer-Decoder整体架构"><a href="#5-Transformer-Decoder整体架构" class="headerlink" title="5.Transformer Decoder整体架构"></a>5.Transformer Decoder整体架构</h1><p>我们先从 HighLevel 的角度观察一下 Decoder 结构，从下到上依次是：</p><ul><li>Masked Multi-Head Self-Attention</li><li>Multi-Head Encoder-Decoder Attention</li><li>FeedForward Network</li></ul><p>和 Encoder 一样，上面三个部分的每一个部分，都有一个残差连接，后接一个 <strong>Layer Normalization</strong>。Decoder 的中间部件并不复杂，大部分在前面 Encoder 里我们已经介绍过了，但是 Decoder 由于其特殊的功能，因此在训练时会涉及到一些细节。</p><p><img src="https://z3.ax1x.com/2021/04/20/c7wyKU.png#shadow" alt="decoder"></p><h2 id="Masked-Self-Attention"><a href="#Masked-Self-Attention" class="headerlink" title="Masked Self-Attention"></a>Masked Self-Attention</h2><p>传统 Seq2Seq 中 Decoder 使用的是 RNN 模型，因此在训练过程中输入 $t$ 时刻的词，模型无论如何也看不到未来时刻的词，因为循环神经网络是时间驱动的，只有当 $t$ 时刻运算结束了，才能看到 t+1 时刻的词。而 Transformer Decoder 抛弃了 RNN，改为 Self-Attention，由此就产生了一个问题，在训练过程中，整个 ground truth 都暴露在 Decoder 中，这显然是不对的，我们需要对 Decoder 的输入进行一些处理，该处理被称为 <strong>Mask</strong></p><p>举个例子，Decoder 的 ground truth 为 “<start> I am fine”，我们将这个句子输入到 Decoder 中，经过 WordEmbedding 和 Positional Encoding 之后，将得到的矩阵做三次线性变换（$W_Q,W_K,W_V$）。然后进行 self-attention 操作，首先通过 $\frac {Q×K^T}{\sqrt{d_k}}$ 得到 Scaled Scores，接下来非常关键，我们要对 Scaled Scores 进行 Mask，举个例子，当我们输入 “I” 时，模型目前仅知道包括 “I” 在内之前所有字的信息，即 “<start>“ 和 “I” 的信息，不应该让其知道 “I” 之后词的信息。道理很简单，我们做预测的时候是按照顺序一个字一个字的预测，怎么能这个字都没预测完，就已经知道后面字的信息了呢？Mask 非常简单，首先生成一个下三角全 0，上三角全为负无穷的矩阵，然后将其与 Scaled Scores 相加即可。</p><p><img src="https://z3.ax1x.com/2021/04/20/c7w48x.png#shadow" alt=""></p><p>之后再做 softmax，就能将 - inf 变为 0，得到的这个矩阵即为每个字之间的权重</p><p><img src="https://s1.ax1x.com/2020/07/12/U3FCQ0.png#shadow" alt=""></p><h2 id="Masked-Encoder-Decoder-Attention"><a href="#Masked-Encoder-Decoder-Attention" class="headerlink" title="Masked Encoder-Decoder Attention"></a>Masked Encoder-Decoder Attention</h2><p>其实这一部分的计算流程和前面 Masked Self-Attention 很相似，结构也一摸一样，唯一不同的是这里的 $K,V$ 为 Encoder 的输出，$Q$ 为 Decoder 中 Masked Self-Attention 的输出</p><p><img src="https://s1.ax1x.com/2020/07/12/U3EwOx.png#shadow" alt=""></p><h2 id="编码器和解码器协同工作"><a href="#编码器和解码器协同工作" class="headerlink" title="编码器和解码器协同工作"></a>编码器和解码器协同工作</h2><p>编码器一般有多层，第一个编码器的输入是一个序列文本，最后一个编码器输出是一组序列向量，这组序列向量会作为解码器的$K、V$输入，其中<strong>K=V=解码器输出的序列向量表示</strong>。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中到输入序列的合适位置，如下图所示。</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/ec5913355eb08291.gif" alt=""></p><p>解码（decoding ）阶段的每一个时间步都输出一个翻译后的单词（这里的例子是英语翻译），解码器当前时间步的输出又重新作为输入Q和编码器的输出K、V共同作为下一个时间步解码器的输入。然后重复这个过程，直到输出一个结束符。如下图所示：</p><p><img src="https://tva2.sinaimg.cn/large/005Nrl8dly8gueug0etu9g60hr09rtws02.gif" alt="decoder"></p><p>解码器中的 Self Attention 层，和编码器中的 Self Attention 层的区别：</p><ol><li>在解码器里，Self Attention 层只允许关注到输出序列中早于当前位置之前的单词。具体做法是：在 Self Attention 分数经过 Softmax 层之前，屏蔽当前位置之后的那些位置（将attention score设置成-inf）。</li><li>解码器 Attention层是使用前一层的输出来构造Query 矩阵，而Key矩阵和 Value矩阵来自于编码器最终的输出。</li></ol><p>对于形形色色的NLP任务，OpenAI 的论文列出了一些列输入变换方法，可以处理不同任务类型的输入。下面这张图片来源于论文，展示了处理不同任务的模型结构和对应输入变换。</p><p><img src="https://tva1.sinaimg.cn/large/005Nrl8dly8guf9qsyhlgj60i208cq3k02.jpg" alt=""></p><h1 id="6-相关问题"><a href="#6-相关问题" class="headerlink" title="6.相关问题"></a>6.相关问题</h1><h2 id="Transformer-为什么需要进行-Multi-head-Attention？"><a href="#Transformer-为什么需要进行-Multi-head-Attention？" class="headerlink" title="Transformer 为什么需要进行 Multi-head Attention？"></a>Transformer 为什么需要进行 Multi-head Attention？</h2><p>原论文中说到进行 Multi-head Attention 的原因是将模型分为多个头，形成多个子空间，可以让模型去关注不同方面的信息，最后再将各个方面的信息综合起来。其实直观上也可以想到，如果自己设计这样的一个模型，必然也不会只做一次 attention，多次 attention 综合的结果至少能够起到增强模型的作用，也可以类比 CNN 中同时使用<strong>多个卷积核</strong>的作用，直观上讲，多头的注意力<strong>有助于网络捕捉到更丰富的特征 / 信息</strong>。</p><h2 id="Transformer-相比于-RNN-LSTM，有什么优势？为什么？"><a href="#Transformer-相比于-RNN-LSTM，有什么优势？为什么？" class="headerlink" title="Transformer 相比于 RNN/LSTM，有什么优势？为什么？"></a>Transformer 相比于 RNN/LSTM，有什么优势？为什么？</h2><ol><li>RNN 系列的模型，无法并行计算，因为 T 时刻的计算依赖 T-1 时刻的隐层计算结果，而 T-1 时刻的计算依赖 T-2 时刻的隐层计算结果</li><li>Transformer 的特征抽取能力比 RNN 系列的模型要好</li></ol><h2 id="为什么说-Transformer-可以代替-seq2seq？"><a href="#为什么说-Transformer-可以代替-seq2seq？" class="headerlink" title="为什么说 Transformer 可以代替 seq2seq？"></a>为什么说 Transformer 可以代替 seq2seq？</h2><p>这里用代替这个词略显不妥当，seq2seq 虽已老，但始终还是有其用武之地，seq2seq 最大的问题在于<strong>将 Encoder 端的所有信息压缩到一个固定长度的向量中</strong>，并将其作为 Decoder 端首个隐藏状态的输入，来预测 Decoder 端第一个单词 (token) 的隐藏状态。在输入序列比较长的时候，这样做显然会损失 Encoder 端的很多信息，而且这样一股脑的把该固定向量送入 Decoder 端，<strong>Decoder 端不能够关注到其想要关注的信息</strong>。Transformer 不但对 seq2seq 模型这两点缺点有了实质性的改进 (多头交互式 attention 模块)，而且还引入了 self-attention 模块，让源序列和目标序列首先 “自关联” 起来，这样的话，源序列和目标序列自身的 embedding 表示所蕴含的信息更加丰富，而且后续的 FFN 层也增强了模型的表达能力，并且 Transformer 并行计算的能力远远超过了 seq2seq 系列模型</p><h2 id="Transformer中的softmax计算为什么需要除以-d-k"><a href="#Transformer中的softmax计算为什么需要除以-d-k" class="headerlink" title="Transformer中的softmax计算为什么需要除以$d_k$?"></a>Transformer中的softmax计算为什么需要除以$d_k$?</h2><p>论文中解释是：向量的点积结果会很大，将softmax函数push到梯度很小的区域，scaled会缓解这种现象。</p><h3 id="1-为什么比较大的输入会使得softmax的梯度变得很小"><a href="#1-为什么比较大的输入会使得softmax的梯度变得很小" class="headerlink" title="1.为什么比较大的输入会使得softmax的梯度变得很小"></a>1.为什么比较大的输入会使得softmax的梯度变得很小</h3><p>对于一个输入向量$X\subseteq \mathbb{R}^d$ ，softmax函数将其映射/归一化到一个分布 $\hat y\subseteq \mathbb{R}^d$ 。在这个过程中，softmax先用一个自然底数 $e$ 将输入中的<strong>元素间差距先“拉大”</strong>，然后归一化为一个分布。假设某个输入$X$中最大的的元素下标是$k$ ，<strong>如果输入的数量级变大（每个元素都很大），那么</strong>$\hat y_k$<strong>会非常接近1。</strong></p><p>我们可以用一个小例子来看看$X$的数量级对输入最大元素对应的预测概率的$\hat y_k$影响。假定输入$X=[a,a,2a]^T$  ），我们来看不同量级的 $a$产生的$\hat y_3$有什么区别。</p><ul><li>a=1时，$\hat y_3$=0.5761168847658291</li><li>a=10时，$\hat y_3$=0.999909208384341</li><li>a=100时，$\hat y_3 \approx$1.0 (计算机精度限制)</li></ul><p>我们不妨把 a 在不同取值下，$\hat y_3$对应的的全部绘制出来。代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> exp</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line">f = <span class="keyword">lambda</span> x: exp(x * <span class="number">2</span>) / (exp(x) + exp(x) + exp(x * <span class="number">2</span>))</span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">100</span>, <span class="number">100</span>)</span><br><span class="line">y_3 = [f(x_i) <span class="keyword">for</span> x_i <span class="keyword">in</span> x]</span><br><span class="line">plt.plot(x, y_3)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure><p>得到的图如下所示：</p><p><img src="https://tva4.sinaimg.cn/large/005Nrl8dly8gufb8j5t7fj60ve0nywex02.jpg" alt=""></p><p>可以看到，数量级对softmax得到的分布影响非常大。<strong>在数量级较大时，softmax将几乎全部的概率分布都分配给了最大值对应的标签</strong>。</p><p>然后我们来看softmax的梯度。不妨简记softmax函数为 $g(·)$，softmax得到的分布向量$\hat y=g(X)$对输入X的梯度为：</p><script type="math/tex; mode=display">\frac {\part g(x)}{\part x}=diag(\hat y)-\hat y \hat y^T \in \mathbb{R}^{d \times d}</script><p>把这个矩阵展开：</p><p><img src="https://tva2.sinaimg.cn/large/005Nrl8dly8gufbi2hxrcj30n706aq39.jpg" alt=""></p><p>根据前面的讨论，当输入$X$的元素均较大时，softmax会把大部分概率分布分配给最大的元素，假设我们的输入数量级很大，最大的元素是$x_1$，那么就将产生一个接近one-hot的向量 $\hat y \approx [1,0,···,0]^T$,此时上面的矩阵变为如下形式：</p><p><img src="https://tva4.sinaimg.cn/large/005Nrl8dly8gufbko4ed9j60l406u0sw02.jpg" alt=""></p><p>也就是说，在输入的数量级很大时，<strong>梯度消失为0，造成参数更新困难</strong>。</p><h3 id="2-维度与点积大小的关系是怎样的，为什么使用维度的根号来放缩？"><a href="#2-维度与点积大小的关系是怎样的，为什么使用维度的根号来放缩？" class="headerlink" title="2.维度与点积大小的关系是怎样的，为什么使用维度的根号来放缩？"></a>2.维度与点积大小的关系是怎样的，为什么使用维度的根号来放缩？</h3><p>针对为什么维度会影响点积的大小，在论文的脚注中其实给出了一点解释：</p><p><img src="https://tva4.sinaimg.cn/large/005Nrl8dly8gufbl7dnrnj60wm02imxl02.jpg" alt=""></p><p>假设向量$q$和$k$的各个分量是互相独立的随机变量，均值是0，方差是1，那么点积$q \cdot k$均值是0，方差是 $d_k$。</p><p><img src="https://tva3.sinaimg.cn/large/005Nrl8dly8gufbopiqplj60xt0mzdi302.jpg" alt=""></p><p><img src="https://tva4.sinaimg.cn/large/005Nrl8dly8gufboshkdgj60yi0bjab302.jpg" alt=""></p><p><strong>将方差控制为1，也就有效地控制了前面提到的梯度消失的问题</strong>。</p><h3 id="3-为什么在普通形式的-attention-时，使用非-scaled-的-softmax？"><a href="#3-为什么在普通形式的-attention-时，使用非-scaled-的-softmax？" class="headerlink" title="3.为什么在普通形式的 attention 时，使用非 scaled 的 softmax？"></a>3.为什么在普通形式的 attention 时，使用非 scaled 的 softmax？</h3><p>最基础的 attention 有两种形式， 一种是 Add [1]，一种是 Mul [2]，写成公式的话是这样：</p><p><img src="https://tva4.sinaimg.cn/large/005Nrl8dly8gufbqd7xdlj60ji03974d02.jpg" alt=""></p><p>&lt;&gt; 代表矩阵点积。至于为什么要用 Mul 来 完成 Self-attention，作者的说法是为了<strong>计算更快</strong>。因为虽然矩阵加法的计算更简单，但是 Add 形式套着$tanh$和$v$，相当于一个完整的隐层。在整体计算复杂度上两者接近，但是矩阵乘法已经有了非常成熟的加速实现。在 $d_k$ （即 attention-dim）较小的时候，两者的效果接近。但是随着$d_k$ 增大，Add 开始显著超越 Mul。</p><p><img src="https://tva2.sinaimg.cn/large/005Nrl8dly8gufbrvbgsuj60b807wdgk02.jpg" alt="img"></p><p>作者分析 Mul 性能不佳的原因，认为是<strong>极大的点积值将整个 softmax 推向梯度平缓区，使得收敛困难</strong>。也就是出现了“梯度消失”。这才有了 scaled。所以，Add 是天然地不需要 scaled，Mul在d_k 较大的时候必须要做 scaled。个人认为，<strong>Add 中的矩阵乘法，和 Mul 中的矩阵乘法不同</strong>。前者中只有随机变量 X 和参数矩阵 W 相乘，但是后者中包含随机变量 X 和 随机变量 X 之间的乘法。</p><p>那么，极大的点积值是从哪里来的呢？对于 Mul 来说，如果 $s$ 和 $h$ 都分布在 [0,1]，在相乘时引入一次对所有位置的 $\sum$ 求和，整体的分布就会扩大到  $[0,d_k]$ 。反过来看 Add，右侧是被 $tanh()$ 钳位后的值，分布在 [-1,1]。整体分布和 $d_k$  没有关系。</p><h3 id="4-为什么在分类层（最后一层），使用非-scaled-的-softmax？"><a href="#4-为什么在分类层（最后一层），使用非-scaled-的-softmax？" class="headerlink" title="4.为什么在分类层（最后一层），使用非 scaled 的 softmax？"></a>4.为什么在分类层（最后一层），使用非 scaled 的 softmax？</h3><p>同上面一部分，分类层的 softmax 也没有两个随机变量相乘的情况。此外，这一层的 softmax 通常和交叉熵联合求导，在某个目标类别 $i$ 上的整体梯度变为$y_i^{‘}-y_i$ ，即预测值和真值的差。当出现某个极大的元素值，softmax 的输出概率会集中在该类别上。<strong>如果是预测正确，整体梯度接近于 0，抑制参数更新；如果是错误类别，则整体梯度接近于1，给出最大程度的负反馈。</strong></p><p>也就是说，这个时候的梯度形式改变，不会出现极大值导致梯度消失的情况了。</p><p><a href="https://www.zhihu.com/question/339723385">该问题详细回答可以看这篇知乎问题</a></p><h1 id="7-参考文章"><a href="#7-参考文章" class="headerlink" title="7.参考文章"></a>7.参考文章</h1><ul><li><a href="http://mantchs.com/2019/09/26/NLP/Transformer/">Transformer (mantchs.com)</a></li><li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer (jalammar.github.io)</a></li><li><a href="https://wmathor.com/index.php/archives/1438/">transformer详解</a></li><li><a href="[https://datawhalechina.github.io/learn-nlp-with-transformers/#/./%E7%AF%87%E7%AB%A02-Transformer%E7%9B%B8%E5%85%B3%E5%8E%9F%E7%90%86/2.2-%E5%9B%BE%E8%A7%A3transformer](https://datawhalechina.github.io/learn-nlp-with-transformers/#/./篇章2-Transformer相关原理/2.2-图解transformer">图解transformer</a>)</li></ul>]]></content>
      
      
      <categories>
          
          <category> 论文阅读 </category>
          
          <category> NLP经典模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LSTM</title>
      <link href="/2021/09/06/LSTM/"/>
      <url>/2021/09/06/LSTM/</url>
      
        <content type="html"><![CDATA[<p>[TOC]</p><h1 id="1-LSTM网络"><a href="#1-LSTM网络" class="headerlink" title="1.LSTM网络"></a>1.LSTM网络</h1><p>​        <strong>长短期记忆网络</strong>（ Long Short-Term Memory Network ， LSTM ） [Gers et al.,2000; Hochreiter et al., 1997] 是循环神经网络的一个变体，长短期记忆是指长的“短期记忆”，可以有效地解决<strong>简单循环神经网络</strong>的梯度爆炸或消失问题．</p><p><img src="https://s3.bmp.ovh/imgs/2021/09/759ebcc2eb20626c.jpg" alt="LSTM 网络的循环单元结构"></p><p>​                                                                        LSTM 网络的循环单元结构</p><h1 id="2-LSTM核心思想"><a href="#2-LSTM核心思想" class="headerlink" title="2.LSTM核心思想"></a>2.LSTM核心思想</h1><p>​        LSTM 的关键是 cell 状态，即记忆单元c。cell 状态的传输就像一条传送带，向量从整个 cell 中穿过，只是做了少量的线性操作，这种结构能很轻松地实现信息从整个 cell 中穿过而不做改变（这样就可以实现长时期地记忆保留）。在 LSTM 网络中，记忆单元c可以在某个时刻捕捉到某个关键信息，并有能力将此关键信息保存一定的时间间隔．记忆单元c中保存信息的生命周期要长于短期记忆，但又远远短于长期记忆，因此称为长短期记忆（ Long Short-Term Memory ）。</p><p>​        LSTM 也有能力向 cell 状态中添加或删除信息，这是由称为<strong>门（gates）</strong>的结构仔细控制的。<strong>门</strong>可以选择性的让信息通过，它们由 sigmoid 神经网络层和逐点相乘实现。每个 LSTM 有三个这样的<strong>门</strong>结构来实现控制信息（分别是 forget gate <strong>遗忘门</strong>；input gate <strong>输入门</strong>；output gate <strong>输出门</strong>）</p><h1 id="3-逐步理解LSTM"><a href="#3-逐步理解LSTM" class="headerlink" title="3.逐步理解LSTM"></a>3.逐步理解LSTM</h1><h2 id="3-1-遗忘门"><a href="#3-1-遗忘门" class="headerlink" title="3.1 遗忘门"></a>3.1 遗忘门</h2><p>遗忘门<strong>$f_t$</strong>控制上一个时刻的内部状态<strong>$c_{t-1}$</strong>需要遗忘多少信息。它的输入是$h_{t-1}$和 $x_t$，输出是一个数值都在 0~1 之间的向量（向量长度和$c_{t-1}$一样），表示让$c_{t-1}$的各部分信息通过的比重，0 表示不让任何信息通过，1 表示让所有信息通过。</p><p>假设一个语言模型试图基于前面所有的词预测下一个单词，在这种情况下，每个 cell 状态都应该包含了当前主语的性别（<strong>保留信息</strong>），这样接下来我们才能正确使用代词。但是当我们又开始描述一个新的主语时，就应该把旧主语的性别给忘了才对（<strong>忘记信息</strong>）。</p><p><img src="https://s2.ax1x.com/2020/02/05/1sgHsJ.png#shadow" alt=""></p><h2 id="3-2-输入门"><a href="#3-2-输入门" class="headerlink" title="3.2 输入门"></a>3.2 输入门</h2><p>输入门$i_t$控制当前时刻的候选状态$\widetilde{c}$有多少信息需要保存。实现这个需要包括两个步骤：首先，一个叫做 <code>input gate layer</code> 的 sigmoid 层决定哪些信息需要更新。另一个$tanh$层创建一个新的候选状态向量 $\widetilde{c}$。最后，我们把这两个部分联合起来对 cell 状态进行更新。</p><p><img src="https://s2.ax1x.com/2020/02/05/1sRbU1.png#shadow" alt=""></p><p>在我们的语言模型的例子中，我们想把新的主语性别信息添加到 cell 状态中，替换掉老的状态信息。有了上述的结构，我们就能够更新 cell 状态了，即把$c_{t-1}$更新为$c_t$。从结构图中应该能一目了然，首先我们把旧的状态$c_{t-1}$和$f_t$相乘，把一些不想保留的信息忘掉，然后加上$i_t*\widetilde{c}_t$。这部分信息就是我们要添加的新内容。</p><p><img src="https://s2.ax1x.com/2020/02/05/1sfKyD.png#shadow" alt=""></p><h2 id="3-3-输出门"><a href="#3-3-输出门" class="headerlink" title="3.3 输出门"></a>3.3 输出门</h2><p>输出门$o_t$控制当前时刻的内部状态$c_t$有多少信息需要输出给外部状态$h_t$。这个输出主要是依赖于 cell 状态 $C_t$，但是是经过筛选的版本。首先，经过一个 sigmoid 层，它决定$ C_t $中的哪些部分将会被输出。接着，我们把 $C_t $通过一个$ tanh $层（把数值归一化到 - 1 和 1 之间），然后把$ tanh $层的输出和 simoid 层计算出来的权重相乘，这样就得到了最后的输出结果。</p><p>在语言模型例子中，假设我们的模型刚刚接触了一个代词，接下来可能要输出一个动词，这个输出可能就和代词的信息有关了。比如说，这个动词应该采用单数形式还是复数形式，那么我们就得把刚学到的和代词相关的信息都加入到 cell 状态中来，才能够进行正确的预测。</p><p><img src="https://s2.ax1x.com/2020/02/05/1s5dOO.png#shadow" alt=""></p><hr><p>📒<strong>tips</strong>:一般在深度网络参数学习时，参数初始化的值一般都比较小．但是在训练 LSTM 网络时，过小的值会使得遗忘门的值比较小．这意味着前一时刻的信息大部分都丢失了，这样网络很难捕捉到长距离的依赖信息．并且相邻时间间隔的梯度会非常小，这会导致梯度弥散问题．因此遗忘的参数初始值一般都设得比较大，其偏置向量 𝒄 𝑔 设为 1 或 2 。</p><hr><p>推荐阅读：<a href="https://nndl.github.io/">神经网络与深度学习</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP经典模型 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP模型 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>贪心搜索、维特比算法、集束搜索、梯度裁剪</title>
      <link href="/2021/06/11/tan-xin-sou-suo-wei-te-bi-suan-fa-ji-shu-sou-suo-ti-du-cai-jian/"/>
      <url>/2021/06/11/tan-xin-sou-suo-wei-te-bi-suan-fa-ji-shu-sou-suo-ti-du-cai-jian/</url>
      
        <content type="html"><![CDATA[<h1 id="贪心搜索greedy-search🍓"><a href="#贪心搜索greedy-search🍓" class="headerlink" title="贪心搜索greedy search🍓"></a>贪心搜索greedy search🍓</h1><p>每一步选择每个输出的最大概率，直到出现终结符或最大句子长度。</p><p><img src="https://ftp.bmp.ovh/imgs/2021/06/a43e8f93f0d82bdf.png" alt=""></p><h1 id="维特比算法Viterbi-algorithm🍈"><a href="#维特比算法Viterbi-algorithm🍈" class="headerlink" title="维特比算法Viterbi algorithm🍈"></a>维特比算法Viterbi algorithm🍈</h1><p>维特比算法是一种动态规划算法。它用于寻找最有可能产生观测事件序列的维特比路径——隐含状态序列，特别是在马尔可夫信息源上下文和隐马尔可夫模型中。viterbi算法是每次记录到当前时刻，每个观察标签的最优序列，每次只需要保存到当前位置最优路径，之后循环向后走。到结束时，从最后一个时刻的最优值回溯到开始位置，回溯完成后，这个从开始到结束的路径就是最优的。</p><h1 id="集束搜索beam-search🍒"><a href="#集束搜索beam-search🍒" class="headerlink" title="集束搜索beam search🍒"></a>集束搜索beam search🍒</h1><p>集束搜索可以认为是维特比算法的贪心形式，在维特比所有中由于利用动态规划导致当字典较大时效率低，而集束搜索使用beam<br>size参数来限制在每一步保留下来的可能性词的数量。集束搜索是在测试阶段为了获得更好准确性而采取的一种策略，在训练阶段无需使用。</p><p>假设字典为[a,b,c]，beam size选择2，则如下图有：</p><p><img src="https://ftp.bmp.ovh/imgs/2021/06/84dbb09cc2406643.png" alt=""></p><h1 id="梯度裁剪Gradient-Clipping🍑"><a href="#梯度裁剪Gradient-Clipping🍑" class="headerlink" title="梯度裁剪Gradient Clipping🍑"></a>梯度裁剪Gradient Clipping🍑</h1><p>梯度裁剪是解决梯度爆炸的一种技术，其出发点是非常简明的：如果梯度变得非常大，那么我们就调节它使其保持较小的状态。精确的说，∥g∥≥<em>c,则</em></p><p>g←<em>c</em>⋅g/∥g∥</p><p>此处的c指超参数，g指梯度，||g||为梯度的范数，<strong>g</strong>/∥<strong>g</strong>∥必然是一个单位矢量，因此调节后的梯度范数等于c。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 算法笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>transformer中文小结</title>
      <link href="/2021/06/05/transformer-zhong-wen-xiao-jie/"/>
      <url>/2021/06/05/transformer-zhong-wen-xiao-jie/</url>
      
        <content type="html"><![CDATA[<h2 id="1-背景🤡"><a href="#1-背景🤡" class="headerlink" title="1. 背景🤡"></a>1. 背景🤡</h2><p>Attention机制最早在视觉领域提出，2014年Google Mind发表了《Recurrent Models of Visual Attention》，使Attention机制流行起来，这篇论文采用了RNN模型，并加入了Attention机制来进行图像的分类。</p><p>2015年，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，将attention机制首次应用在nlp领域，其采用Seq2Seq+Attention模型来进行机器翻译，并且得到了效果的提升。</p><p>2017 年，Google 机器翻译团队发表的《Attention is All You Need》中，完全抛弃了RNN和CNN等网络结构，而仅仅采用Attention机制来进行机器翻译任务，并且取得了很好的效果，注意力机制也成为了大家近期的研究热点。</p><h2 id="2-Transformer🤑"><a href="#2-Transformer🤑" class="headerlink" title="2. Transformer🤑"></a>2. Transformer🤑</h2><p>大部分序列处理模型都采用encoder-decoder结构，其中encoder将输入序列($  x_1,x_2,…,x_n $)映射到连续表示$ \overrightarrow{z}=(z_1,z_2,…,z_n) $,然后decoder生成一个输出序列($ y_1,y_2,…,y_m $),每个时刻输出一个结果。本文提出的Transformer模型整体架构如下。</p><p><img src="https://pic1.zhimg.com/v2-0af88beb0cc280317e06b24c2582eb60_r.jpg" alt="Transformer"></p><h3 id="2-1-Encoder😇"><a href="#2-1-Encoder😇" class="headerlink" title="2.1 Encoder😇"></a>2.1 Encoder😇</h3><p>Encoder模块包含一个自注意力层和一个前馈神经网络层。自注意力层采用多头自注意力机制，前馈神经网络是一个标准的两层神经网络，第一层使用Relu激活函数，第二层无激活。此外，注意力层和前馈神经网络都采用残差连接，并进行批标准化（batchnormalization），进一步增强网络的泛化能力。</p><p>Encoder有N=6层，每层包括两个sub-layers：</p><ol><li>第一个sub-layers是multi-head self-attention mechanical，用来计算输入的self-attention。</li><li>第二个sub-layers是简单的全连接网络。</li></ol><p>在每个sub-layer都模拟了残差网络，每个sub-layer的输出都是</p><script type="math/tex; mode=display">LayerNorm(x+Sublayer(x))</script><p>其中Sublayer(x) 表示Sub-layer对输入x做的映射，为了确保连接，所有的sub-layers和embedding layer输出的维数都相同$ d_{model}=512 $</p><h3 id="2-2-Decoder😜"><a href="#2-2-Decoder😜" class="headerlink" title="2.2 Decoder😜"></a>2.2 Decoder😜</h3><p>Decoder模块包含两个自注意力层和一个前馈神经网络，实现译文句子的解码。该模块的第一个注意力层为带屏蔽的多头自注意力层（maskedmuti-headattention），目的是在推导过程中屏蔽未来输入。该模块的第二个注意力层和前馈神经网络与Encoder模块中的结构几乎完全一样，唯一的区别是，Decoder的输入除了包含前一层的输出之外，还增加了Encoder的输出。多个Decoder模块可以叠加，最后一个Decoder的输出经过一个线性变换，并利用softmax函数，得到输出词语的预测概率。</p><p>Decoder也是N=6层，每层包括3个sub-layers：</p><ol><li>第一个是Masked multi-head self-attention，也是计算输入的self-attention，但是因为是生成过程，因此在时刻 $ i $ 的时候，大于 $ i $的时刻都没有结果，只有小于$ i $的时刻有结果，因此需要做Mask</li><li>第二个sub-layer是全连接网络，与Encoder相同</li><li>第三个sub-layer是对encoder的输入进行attention计算。</li></ol><p>同时Decoder中的self-attention层需要进行修改，因为只能获取到当前时刻之前的输入，因此只对时刻 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 之前的时刻输入进行attention计算，这也称为Mask操作。</p><h3 id="2-3-Attention机制🏈"><a href="#2-3-Attention机制🏈" class="headerlink" title="2.3 Attention机制🏈"></a>2.3 Attention机制🏈</h3><p>在Transformer中使用的Attention是Scaled Dot-Product Attention, 是归一化的点乘Attention，假设输入的query $ Q $ 、key维度为$ d_k $,value维度为$ d_v $ , 计算query和每个key的点乘操作，然后为了防止其结果过大除以$ \sqrt{d_k} $ ，然后应用Softmax函数计算权重。</p><script type="math/tex; mode=display">Attention(Q,K,V)=softmax(\frac{Q^TK}{\sqrt{d_k}})V</script><h3 id="2-4-Positional-Embedding🏐"><a href="#2-4-Positional-Embedding🏐" class="headerlink" title="2.4 Positional Embedding🏐"></a>2.4 Positional Embedding🏐</h3><p>Transformer本质上是一个自编码器，不能利用词语之间的顺序信息，所以引入位置嵌入向量（positional Embedding）来表示词语的位置。它是为了解释输入序列中单词顺序而存在的，维度和embedding的维度一致。这个向量决定了当前词的位置，或者说是在一个句子中不同的词之间的距离。其中position embedding计算公式如下</p><script type="math/tex; mode=display">PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}) \\PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})</script><p>其中pos表示位置index，i表示每个值的index。</p><h2 id="3-NLP领域应用🎾"><a href="#3-NLP领域应用🎾" class="headerlink" title="3. NLP领域应用🎾"></a>3. NLP领域应用🎾</h2><p>在Transformer出现之前，添加了注意力的递归神经网络（如GRU和LSTM）赋予了大多数最先进的语言模型。然而，在RNNs中，信息流需要从上一个隐藏状态依次处理到下一个状态，这就导致训练过程难以加速和并行化，从而阻碍了RNNs处理较长序列或构建较大模型的潜力。2017年，Vaswani等人提出了Transformer，这是一种完全建立在多头自关注机制和前馈神经网络上的新型编码器-解码器架构，旨在解决seq-to-seq自然语言任务（如机器翻译），并轻松获取全局依赖性。Transformer的成功表明，单单利用注意力机制就可以达到与注意力RNN相当的性能。此外，Transformer的架构有利于大规模并行计算，可以在更大的数据集上进行训练，从而导致自然语言处理的大型预训练模型（PTM）的爆发。</p><p>生物NLP领域，基于Transformer的模型已经优于许多传统的生物医学方法。BioBERT使用Transformer架构进行生物医学文本挖掘任务；SciBERT通过对114M科学文章进行Transformer训练开发，覆盖生物医学和计算机科学领域，旨在更精确地执行与科学领域相关的NLP任务；Huang等提出ClinicalBERT利用Transformer开发和评估临床笔记的连续表示，他们的研究还有一个副产物，ClinicalBERT的注意力图可以用来解释预测结果，从而发现不同医疗内容之间的高质量联系。</p><p>多模态任务中，由于Transformer在基于文本的NLP任务上的成功，许多研究致力于开发Transformer处理多模态任务（如视频-文本、图像-文本和音频-文本）的潜力。VideoBERT使用基于CNN的模块对视频进行预处理以获得表征标记，在此基础上训练Transformer编码器学习视频-文本表征，用于下游任务，如视频字幕。VisualBERT和VL-BERT提出了单流统一的Transformer来捕捉视觉元素和图像-文本关系，用于下游任务，如视觉问题回答（VQA）和视觉常识推理（VCR）。</p><p>此外，一些研究如SpeechBERT探讨了用Transformer编码器对音频和文本对进行编码，以处理语音答题（SQA）等自动文本任务的可能性。</p><h2 id="4-优点🏉"><a href="#4-优点🏉" class="headerlink" title="4. 优点🏉"></a>4. 优点🏉</h2><ol><li>相较于RNN必须按时间顺序进行计算，Transformer<strong>并行处理机制的显著好处便在于更高的计算效率</strong>，可以通过并行计算来大大<strong>加快训练速度</strong>，从而能在更大的数据集上进行训练。</li><li>Transformer模型具有<strong>良好的可扩展性和伸缩性</strong>。</li><li>Transformer的<strong>特征抽取能力</strong>比RNN系列的模型要好。</li><li>Transformer的设计最大的带来性能提升的关键是将任意两个单词的距离变成1，这对<strong>解决NLP中棘手的长期依赖问题</strong>是非常有效的。</li></ol><h2 id="5-缺点🎱"><a href="#5-缺点🎱" class="headerlink" title="5. 缺点🎱"></a>5. 缺点🎱</h2><ol><li>Transformer模型<strong>缺乏归纳偏置能力</strong>，例如并不具备CNN那样的平移不变性和局部性，因此在数据不足时不能很好的泛化到该任务上。</li><li>抛弃RNN和CNN使模型<strong>丧失了捕捉局部特征的能力</strong>，RNN + CNN + Transformer的结合可能会带来更好的效果。</li><li>Transformer<strong>失去的位置信息</strong>其实在NLP中非常重要，而论文中在特征向量中加入Position Embedding也只是一个权宜之计，并没有改变Transformer结构上的固有缺陷。</li></ol>]]></content>
      
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Get To The Point: Summarization with Pointer-Generator Networks</title>
      <link href="/2021/05/26/get-to-the-point-summarization-with-pointer-generator-networks/"/>
      <url>/2021/05/26/get-to-the-point-summarization-with-pointer-generator-networks/</url>
      
        <content type="html"><![CDATA[<h2 id="一-摘要🐶"><a href="#一-摘要🐶" class="headerlink" title="一 摘要🐶"></a>一 摘要🐶</h2><p>传统的Seq2Seq+Attention模型存在三个缺陷：</p><ol><li><p>难以准确复述原文细节。</p></li><li><p>无法处理原文中的<strong>未登录词(OOV)</strong>。</p></li><li><p>在生成的摘要中存在一些重复的部分</p></li></ol><p>此文提出一种以两种正交的方式增强了增强标准的Seq2Seq+Attention模型</p><ol><li>使用<strong>指针生成器网络(pointer-generator network)</strong> ,通过指针从源文件中拷贝词，同时保留通过生成器生成新单词的能力。</li><li>使用<strong>覆盖率(coverage)</strong> 机制，追踪哪些信息已经在摘要中，避免生成具有重复片段的摘要。</li></ol><h2 id="二-模型🐱"><a href="#二-模型🐱" class="headerlink" title="二 模型🐱"></a>二 模型🐱</h2><ol><li><strong>baseline：sequence-to-sequence</strong> 模型</li><li><strong>指针生成器网络(pointer-generation network)</strong></li><li><strong>覆盖率机制(coverage mechanism)</strong>，可以被加在上述两种模型架构上</li></ol><h3 id="2-1-seq2seq-Attention模型"><a href="#2-1-seq2seq-Attention模型" class="headerlink" title="2.1 seq2seq + Attention模型"></a>2.1 seq2seq + Attention模型</h3><p><img src="https://pic1.zhimg.com/v2-9d85ac1a8d8cb7986163c39072fa0000_r.jpg" alt="baseline"></p><p><strong>encoder</strong>采用<strong>单层双向LSTM</strong>，训练数据中的文档被一个一个地喂入encoder中，产生encoder的隐藏层状态$h_i$的序列。</p><p><strong>decoder</strong>部分采用一个<strong>单层单向LSTM</strong>，每一步的输入是前一步预测的词的词向量，同时输出一个解码的状态序列$s_t$，用于当前步的预测。</p><p><strong>attention</strong>是针对原文的概率分布，告诉模型哪些词更重要。具体计算公式为</p><script type="math/tex; mode=display">e_i^t=v^t\tanh(W_hh_i+W_ss_t+b_{attn}) \\a^t=softmax(e^t)</script><p>计算出当前步的attention分布后，对encoder输出的隐层做加权平均，获得原文的动态表示，称为语境向量</p><script type="math/tex; mode=display">h_t^*= \Sigma_ia_i^th_i</script><p>依靠语境向量和decoder输出的隐层向量，共同决定当前步预测在词表上的概率分布</p><script type="math/tex; mode=display">P_{vocab}=softmax(V’(V[s_t,h_t^*]+b)+b')</script><p>损失函数采用交叉熵</p><script type="math/tex; mode=display">loss=\frac{1}{T}\Sigma_{t=0}^T-logP(w_t^*)</script><h3 id="2-2-Pointer-generator-network"><a href="#2-2-Pointer-generator-network" class="headerlink" title="2.2 Pointer-generator network"></a>2.2 Pointer-generator network</h3><p><img src="https://pic3.zhimg.com/v2-a8b65b1870be5671ccd46e7d6a6e0fc6_r.jpg" alt="指针生成器网络"></p><p>pointer-generator network是seq2seq模型和pointer network的混合模型，一方面通过seq2seq模型保持生成的能力，另一方面通过pointer network直接从原文中取词，提高摘要的准确度和缓解OOV问题。在预测的每一步，通过动态计算一个生成概率$P_{gen}$作为一个软开关，用于选择是通过$P{vocab}$从词汇表中生成一个词，或者从输入序列的注意力分布$a_t$中复制一个词。</p><script type="math/tex; mode=display">P_{gen}=\sigma(w_{h^*}^Th_t^*+w_s^Ts_t+w_x^Tx_t+b_{ptr})</script><p>对于每一篇文档，用<strong>扩展后的词表(extended vocabulary)</strong> 来表示整个词汇表和原文档中的词的并集，得到在扩展词表上建立的概率分布：</p><script type="math/tex; mode=display">P(w)=p_{gen}P_{vocab}(w)+(1-p_{gen}\sum_{i:w_i=w}a_i^t)</script><p>如果w是一个OOV单词，$P_{vocab}(w)$为0；如果w没有出现在源文档中，但在词表中出现，那么$\sum_{i:w_i=w}a_i^t$就为0。</p><p>生成OOV单词的能力是pointer-generator网络的一个主要优势。</p><h3 id="2-3-覆盖机制-coverage-mechanism"><a href="#2-3-覆盖机制-coverage-mechanism" class="headerlink" title="2.3 覆盖机制(coverage mechanism)"></a>2.3 覆盖机制(coverage mechanism)</h3><p>重复问题是seq2seq模型的常见问题，此文采用覆盖模型来解决重复问题，在此覆盖模型中，保留了一个<strong>覆盖率向量(coverage vector)</strong>$e^t$,它是过去所有decoder步骤计算的attention分布的累加和，记录模型已经关注过原文的哪些词。</p><script type="math/tex; mode=display">e^t=\sum_{t'=0}^{t-1}a^{t'}</script><p>覆盖率向量也被用来作为注意力机制的额外输入,$c^0$是一个0向量，表示在第一个时间步上，源文档还没有被覆盖。</p><script type="math/tex; mode=display">e_i^t=v^t\tanh(W_hh_i+W_ss_t+w_cc_i^t+b_{attn})</script><p>定义了一个<strong>覆盖率损失(coverage loss)</strong> 来惩罚attention重复放在同一区域的行为。</p><script type="math/tex; mode=display">covloss_t=\sum_imin(a_i^t,c_i^t)</script><p>最终的模型整体损失函数为：</p><script type="math/tex; mode=display">loss_t=-logP(w_t^*)+\lambda\sum_imin(a_i^t,c_i^t)</script>]]></content>
      
      
      
        <tags>
            
            <tag> 论文阅读 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多模态机器学习</title>
      <link href="/2021/05/17/duomotai1/"/>
      <url>/2021/05/17/duomotai1/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是多模态机器学习🍌"><a href="#什么是多模态机器学习🍌" class="headerlink" title="什么是多模态机器学习🍌"></a>什么是多模态机器学习🍌</h2><p>每一种信息的来源或者形式，都可以称为一种模态。例如，人有触觉，听觉，视觉，嗅觉；信息的媒介有语音、视频、文字等；多种多样的传感器，如雷达、红外、加速度计等。</p><p>同时，模态也可以有非常广泛的定义，比如我们可以把两种不同的语言当做是两种模态，甚至在两种不同情况下采集到的数据集，亦可认为是两种模态。</p><p>因此，多模态机器学习，英文全称 MultiModal Machine Learning (MMML)，旨在通过机器学习的方法实现处理和理解多源模态信息的能力。目前比较热门的研究方向是图像、视频、音频、语义之间的多模态学习。</p><h2 id="多模态学习的分类"><a href="#多模态学习的分类" class="headerlink" title="多模态学习的分类"></a>多模态学习的分类</h2><p>多模态学习可以划分为以下五个研究方向：</p><p><strong>1.多模态表示学习 Multimodal Representation</strong><br><strong>2.模态转化 Translation</strong><br><strong>3.对齐 Alignment</strong><br><strong>4.多模态融合 Multimodal Fusion</strong><br><strong>5.协同学习 Co-learning</strong></p><h2 id="多模态表示学习🍏"><a href="#多模态表示学习🍏" class="headerlink" title="多模态表示学习🍏"></a>多模态表示学习🍏</h2><p>单模态的表示学习负责将信息表示为计算机可以处理的数值向量或者进一步抽象为更高层的特征向量，而多模态表示学习是指通过利用多模态之间的互补性，剔除模态间的冗余性，从而学习到更好的特征表示。主要包括两大研究方向：<strong>联合表示（Joint Representations）</strong>和<strong>协同表示(Coordinated Representations)</strong>。</p><p>1.联合表示：将多个模态的信息一起映射到一个统一的多模态向量空间；</p><p>2.协同表示：将多模态中的每个模态分别映射到各自的表示空间，但映射后的向量之间满足一定的相关性约束（例如线性相关）。<br><img src="https://ftp.bmp.ovh/imgs/2021/05/bf273786533ade93.jpg" alt="表示学习"></p><h2 id="模态转化🍎"><a href="#模态转化🍎" class="headerlink" title="模态转化🍎"></a>模态转化🍎</h2><p>转化也成为映射，负责将一个模态的信息转换位另一个模态的信息。常用的应用包括：<strong>机器翻译</strong>，<strong>唇语</strong>和<strong>语音翻译</strong>；<strong>图片描述</strong>，<strong>视频描述</strong>；<strong>语音合成</strong>等。</p><p>模态间的转换有两个难点：<br>1.open-ended,即未知结束位，例如在实时翻译中，在未获得句尾的情况下，必须实时的对句子进行翻译。</p><p>2.subjective，即主观评判性，是指很多模态转换问题的效果没有一个比较客观的评判标准，也就是说目标函数的确定是非常主观的。例如，在图片描述中，形成怎样的一段话才算是对图片好的诠释？也许一千个人心中有一千个哈姆雷特。</p><h2 id="对齐-Alignment🍐"><a href="#对齐-Alignment🍐" class="headerlink" title="对齐 Alignment🍐"></a>对齐 Alignment🍐</h2><p>多模态的对齐负责对来自同一个实例的不同模态信息的子分支/元素寻找对应关系。比如电影画面-语音-字幕的自动对齐。</p><p>对齐又可以是空间维度的，比如图片语义分割 （Image Semantic Segmentation）：尝试将图片中的每个像素对应到某一种类型标签，实现视觉-词汇对齐。</p><h2 id="多模态融合-Multimodal-Fusion🍊"><a href="#多模态融合-Multimodal-Fusion🍊" class="headerlink" title="多模态融合 Multimodal Fusion🍊"></a>多模态融合 Multimodal Fusion🍊</h2><p>多模态融合（Multimodal Fusion ）负责联合多个模态的信息，进行目标预测（分类或者回归）。<br>按照融合的层次，可以将多模态融合分为 pixel level，feature level 和 decision level 三类，分别对应对原始数据进行融合、对抽象的特征进行融合和对决策结果进行融合。而 feature level 又可以分为 early 和 late 两个大类，代表了融合发生在特征抽取的早期和晚期。当然还有将多种融合层次混合的 hybrid 方法。<br><img src="https://ftp.bmp.ovh/imgs/2021/05/b0811aadfebe106d.png" alt="Pixel-level-feature-level-and-decision-level-fusion"></p><p><strong>视觉-音频识别（Visual-Audio Recognition）：</strong> 综合源自同一个实例的视频信息和音频信息，进行识别工作。</p><p><strong>多模态情感分析（Multimodal sentiment analysis）：</strong> 综合利用多个模态的数据（例如下图中的文字、面部表情、声音），通过互补，消除歧义和不确定性，得到更加准确的情感类型判断结果。<br>多模态融合研究的难点主要包括如何判断每个模态的置信水平、如何判断模态间的相关性、如何对多模态的特征信息进行降维以及如何对非同步采集的多模态数据进行配准等。</p><h2 id="协同学习-Co-learning🍋"><a href="#协同学习-Co-learning🍋" class="headerlink" title="协同学习 Co-learning🍋"></a>协同学习 Co-learning🍋</h2><p>协同学习是指使用一个资源丰富的模态信息来辅助另一个资源相对贫瘠的模态进行学习。比如<strong>迁移学习</strong></p><p>迁移学习比较常探讨的方面目前集中在<strong>领域适应性（Domain Adaptation）</strong>问题上，即如何将train domain上学习到的模型应用到 application domain。</p><p>迁移学习领域著名的还有<strong>零样本学习（Zero-Shot Learning）</strong>和<strong>一样本学习（One-Shot Learning）</strong>，很多相关的方法也会用到领域适应性的相关知识。</p><p>Co-learning 中还有一类工作叫做<strong>协同训练（Co-training ）</strong>，它负责研究如何在多模态数据中将少量的标注进行扩充，得到更多的标注信息。</p>]]></content>
      
      
      
        <tags>
            
            <tag> 多模态 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多文本分类|（1）数据集预处理</title>
      <link href="/2021/04/29/1/"/>
      <url>/2021/04/29/1/</url>
      
        <content type="html"><![CDATA[<h2 id="1-数据集简介🛴"><a href="#1-数据集简介🛴" class="headerlink" title="1.数据集简介🛴"></a>1.数据集简介🛴</h2><p>​        THUCNews是根据新浪新闻RSS订阅频道2005~2011年间的历史数据筛选过滤生成，包含74万篇新闻文档（2.19 GB），均为UTF-8纯文本格式。我们在原始新浪新闻分类体系的基础上，重新整合划分出14个候选分类类别：财经、彩票、房产、股票、家居、教育、科技、社会、时尚、时政、体育、星座、游戏、娱乐。</p><p>​        <a href="https://pan.baidu.com/s/1NgQP6Vu2eIj9AGrzpk5lnQ#b5c6">完整数据集压缩包下载</a></p><p>&nbsp;</p><h2 id="2-数据预处理🚲"><a href="#2-数据预处理🚲" class="headerlink" title="2.数据预处理🚲"></a>2.数据预处理🚲</h2><p>​        在进行特征提取之前，需要对原始文本数据进行预处理，这对于特征提取来说至关重要，一个好的预处理过程会显著的提高特征提取的质量以及分类算法的性能。 <strong>中文文本</strong>预处理一般包括以下步骤:</p><p>​        （1）分词：把文本切分成词或者字。</p><p>​        （2）去停用词：文本中大量出现但对分类没有多大作用的词。</p><p>​        （3）噪声移除：去除文本中的特殊符号，如特殊标点符号等。这些符号对分类没有太大意义。</p><p>​        （4）词频统计语过滤：对文本训练集预处理后，统计剩余单词的词频，并过滤低频词，由剩余的单词构建词典。  </p><p>&nbsp;</p><h2 id="3-Code🛵"><a href="#3-Code🛵" class="headerlink" title="3.Code🛵"></a>3.Code🛵</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext.vocab <span class="keyword">as</span> Vocab</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"><span class="keyword">import</span> collections</span><br><span class="line"><span class="keyword">from</span> config <span class="keyword">import</span> opt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_cnews</span>():</span></span><br><span class="line">    data = []</span><br><span class="line">    <span class="comment"># 所有的主题标签</span></span><br><span class="line">    labels = [label <span class="keyword">for</span> label <span class="keyword">in</span> os.listdir(opt.data_root) <span class="keyword">if</span> label != <span class="string">&#x27;.DS_Store&#x27;</span>]</span><br><span class="line">    <span class="built_in">print</span>(labels)</span><br><span class="line">    <span class="comment"># 标签到整数索引的映射</span></span><br><span class="line">    labels2index = <span class="built_in">dict</span>(<span class="built_in">zip</span>(labels, <span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(labels)))))</span><br><span class="line">    <span class="comment"># 整数索引到标签的映射</span></span><br><span class="line">    index2labels = <span class="built_in">dict</span>(<span class="built_in">zip</span>(<span class="built_in">list</span>(<span class="built_in">range</span>(<span class="built_in">len</span>(labels))), labels))</span><br><span class="line">    <span class="built_in">print</span>(labels2index)</span><br><span class="line">    <span class="built_in">print</span>(index2labels)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 存储整数索引到标签的映射，以便预测时使用</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;index2labels.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(index2labels, f)</span><br><span class="line">    <span class="comment"># 存储类别标签</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;labels.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        json.dump(labels, f)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> label <span class="keyword">in</span> labels:</span><br><span class="line">        folder_name = os.path.join(opt.data_root, label)</span><br><span class="line">        datasub = []  <span class="comment"># 存储某一类的数据[[string，index],.....]</span></span><br><span class="line">        <span class="keyword">for</span> file <span class="keyword">in</span> tqdm(os.listdir(folder_name)):</span><br><span class="line">            <span class="keyword">with</span> <span class="built_in">open</span>(os.path.join(folder_name, file), <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="comment"># 去除文本中空白行</span></span><br><span class="line">                review = f.read().decode(<span class="string">&#x27;utf-8&#x27;</span>).replace(<span class="string">&#x27;\n&#x27;</span>, <span class="string">&#x27;&#x27;</span>).replace(<span class="string">&#x27;\r&#x27;</span>, <span class="string">&#x27;&#x27;</span>).replace(<span class="string">&#x27;\t&#x27;</span>, <span class="string">&#x27;&#x27;</span>)</span><br><span class="line">                datasub.append([review, labels2index[label]])</span><br><span class="line">        data.append(datasub)  <span class="comment"># 存储所有类的数据[[[string,label],...],[[string,label],...],...]</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split_data</span>(<span class="params">data</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    切分数据集为训练集，验证集，测试集</span></span><br><span class="line"><span class="string">    :param data: 数据集</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    train_data, val_data, test_data = [], [], []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对每一类数据进行打乱</span></span><br><span class="line">    <span class="comment"># 设置验证集和测试集中每一类样本数据均为200（样本均衡）</span></span><br><span class="line">    <span class="keyword">for</span> data1 <span class="keyword">in</span> data:  <span class="comment"># 遍历每一类数据</span></span><br><span class="line">        np.random.shuffle(data1)  <span class="comment"># 打乱数据</span></span><br><span class="line">        val_data += data1[:<span class="number">200</span>]</span><br><span class="line">        test_data += data1[<span class="number">200</span>:<span class="number">400</span>]</span><br><span class="line">        train_data += data1[<span class="number">400</span>:]</span><br><span class="line"></span><br><span class="line">    np.random.shuffle(train_data)  <span class="comment"># 打乱数据集</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(train_data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(val_data))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(test_data))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> train_data, val_data, test_data</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 基于训练集构建词典</span></span><br><span class="line"><span class="comment"># 获取停用词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stopwords</span>(<span class="params">file_path</span>):</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(file_path, <span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        stopword = [line.strip() <span class="keyword">for</span> line <span class="keyword">in</span> f]</span><br><span class="line">    <span class="built_in">print</span>(stopword[:<span class="number">5</span>])</span><br><span class="line">    <span class="keyword">return</span> stopword</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 分词 去除停用词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_tokenized</span>(<span class="params">data, stopword</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    :param data:list of [string,label]</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">tokenizer</span>(<span class="params">text</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [tok <span class="keyword">for</span> tok <span class="keyword">in</span> jieba.lcut(text) <span class="keyword">if</span> tok <span class="keyword">not</span> <span class="keyword">in</span> stopword]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> [tokenizer(review) <span class="keyword">for</span> review, _ <span class="keyword">in</span> data]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建词典</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_vocab</span>(<span class="params">data, stopword</span>):</span></span><br><span class="line">    tokenized_data = get_tokenized(data, stopword)</span><br><span class="line">    counter = collections.Counter([tk <span class="keyword">for</span> st <span class="keyword">in</span> tokenized_data <span class="keyword">for</span> tk <span class="keyword">in</span> st])  <span class="comment"># 统计词频</span></span><br><span class="line">    <span class="keyword">return</span> Vocab.Vocab(counter, min_freq=<span class="number">5</span>,</span><br><span class="line">                       specials=[<span class="string">&#x27;&lt;pad&gt;&#x27;</span>, <span class="string">&#x27;&lt;unk&gt;&#x27;</span>])  <span class="comment"># 保留词频大于5的词 &lt;pad&gt;对应填充项（词典中第0个词） &lt;unk&gt;对应低频词和停止词等未知词（词典中第1个词）</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">preprocess_imdb</span>(<span class="params">data, vocab, stopword</span>):</span></span><br><span class="line">    <span class="comment"># 将训练集、验证集、测试集中的单词转换为词典中对应的索引</span></span><br><span class="line">    max_len = <span class="number">500</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 每条新闻通过截断或者补0，使得长度变成500（所有数据统一成一个长度，方便用矩阵并行计算（其实也可以每个 batch填充成一个长度，batch间可以不一样））</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">pad</span>(<span class="params">x</span>):</span></span><br><span class="line">        <span class="keyword">return</span> x[:max_len] <span class="keyword">if</span> <span class="built_in">len</span>(x) &gt; max_len <span class="keyword">else</span> x + [<span class="number">0</span>] * (max_len - <span class="built_in">len</span>(x))</span><br><span class="line"></span><br><span class="line">    tokenized_data = get_tokenized(data, stopword)</span><br><span class="line">    features = torch.tensor(</span><br><span class="line">        [pad([vocab.stoi[word] <span class="keyword">for</span> word <span class="keyword">in</span> words]) <span class="keyword">for</span> words <span class="keyword">in</span> tokenized_data])  <span class="comment"># 把单词转换为词典中对应的索引，并填充成固定长度，封装为tensor</span></span><br><span class="line">    labels = torch.tensor([score <span class="keyword">for</span> _, score <span class="keyword">in</span> data])</span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    data = read_cnews()</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(data))</span><br><span class="line">    train_data, val_data, test_data = split_data(data)</span><br><span class="line">    stopword = stopwords(<span class="string">&#x27;./cn_stopwords.txt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    vocab = get_vocab(train_data, stopword)</span><br><span class="line">    <span class="built_in">print</span>(vocab.itos[:<span class="number">5</span>])</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;word2index.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:  <span class="comment"># 保存词到索引的映射，预测和后续加载预训练词向量时会用到</span></span><br><span class="line">        json.dump(vocab.stoi, f)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;---------------------&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab.itos))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab.stoi))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;---------------------&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    X_train, y_train = preprocess_imdb(train_data, vocab, stopword)</span><br><span class="line">    X_val, y_val = preprocess_imdb(val_data, vocab, stopword)</span><br><span class="line">    X_test, y_test = preprocess_imdb(test_data, vocab, stopword)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;---------------------&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab.itos))</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(vocab.stoi))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;---------------------&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;vocabsize.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> f:  <span class="comment"># 保存词典的大小（因为我们基于词频阈值过滤低频词，词典大小不确定，需要保存，后续模型中会用到）</span></span><br><span class="line">        json.dump(<span class="built_in">len</span>(vocab), f)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 保存预处理好的训练集、验证集和测试集 以便后续训练时使用</span></span><br><span class="line">    torch.save(X_train, <span class="string">&#x27;X_train.pt&#x27;</span>)</span><br><span class="line">    torch.save(y_train, <span class="string">&#x27;y_train.pt&#x27;</span>)</span><br><span class="line">    torch.save(X_val, <span class="string">&#x27;X_val.pt&#x27;</span>)</span><br><span class="line">    torch.save(y_val, <span class="string">&#x27;y_val.pt&#x27;</span>)</span><br><span class="line">    torch.save(X_test, <span class="string">&#x27;X_test.pt&#x27;</span>)</span><br><span class="line">    torch.save(y_test, <span class="string">&#x27;y_test.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(X_train.shape, X_train[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(y_train.shape)</span><br><span class="line">    <span class="built_in">print</span>(X_val.shape)</span><br><span class="line">    <span class="built_in">print</span>(y_val.shape)</span><br><span class="line">    <span class="built_in">print</span>(X_test.shape)</span><br><span class="line">    <span class="built_in">print</span>(y_test.shape)</span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      
        <tags>
            
            <tag> 文本分类 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
